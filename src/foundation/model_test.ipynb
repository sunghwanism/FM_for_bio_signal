{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data8/jungmin/anaconda3/envs/patchtst/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "import args\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from models.AdversarialModel import AdversarialModel\n",
    "from models.FOCALModules import FOCAL\n",
    "from models.loss import FOCALLoss\n",
    "from data.EfficientDataset import MESAPairDataset\n",
    "from data.Augmentaion import init_augmenter\n",
    "\n",
    "import datetime\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_config: \n",
      " {'train_data_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/dataset/pair_small', 'valid_data_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/dataset/pair_small_valid', 'test_data_dir': '/NFS/Users/moonsh/data/mesa/preproc/pair_test', 'modalities': ['ecg', 'hr'], 'label_key': 'stage', 'subject_key': 'subject_idx', 'train_num_subjects': 100, 'test_num_subjects': 50, 'device': device(type='cpu'), 'log_save_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/logs'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"base_config: \\n {args.base_config}\")\n",
    "# print(f\"focal_config: \\n {args.focal_config} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MESAPairDataset(file_path=args.base_config['train_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            batch_size=14,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "0\n",
      "torch.Size([14, 7680])\n",
      "torch.Size([14, 30])\n",
      "torch.Size([14])\n",
      "torch.Size([14])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__())\n",
    "\n",
    "for i , (raw_modal_1, raw_modal_2, subj, sleep) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(raw_modal_1.shape)\n",
    "    print(raw_modal_2.shape)\n",
    "    print(subj.shape)\n",
    "    print(sleep.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_config: \n",
      " {'modalities': ['ecg', 'hr'], 'label_key': 'stage', 'augmentation': ['GaussianNoise', 'AmplitudeScale'], 'augmenter_config': {'GaussianNoise': {'max_noise_std': 0.1}, 'AmplitudeScale': {'amplitude_scale': 0.5}}, 'num_classes': None} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"data_config: \\n {args.data_config} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GaussianNoise augmenter...\n",
      "Loading AmplitudeScale augmenter...\n"
     ]
    }
   ],
   "source": [
    "aug_1_name = args.data_config['augmentation'][0]\n",
    "aug_1_config = args.data_config['augmenter_config'].get(aug_1_name, {})\n",
    "aug_2_name = args.data_config['augmentation'][1]\n",
    "aug_2_config = args.data_config['augmenter_config'].get(aug_2_name, {})\n",
    "    \n",
    "augmenter_1 = init_augmenter(aug_1_name, aug_1_config)\n",
    "augmenter_2 = init_augmenter(aug_2_name, aug_2_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 7680])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_modal_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 7680])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It changes the shape of input: (B, seq) -> (B\n",
    "augmenter_1(raw_modal_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 7680])\n",
      "torch.Size([14, 30])\n",
      "torch.Size([14, 7680])\n",
      "torch.Size([14, 30])\n",
      "torch.Size([2, 7680])\n",
      "torch.Size([2, 30])\n",
      "torch.Size([2, 7680])\n",
      "torch.Size([2, 30])\n"
     ]
    }
   ],
   "source": [
    "for i , (raw_modal_1, raw_modal_2, subj, sleep) in enumerate(train_loader):\n",
    "    aug_1_modal_1 = augmenter_1(raw_modal_1)\n",
    "    aug_1_modal_2 = augmenter_1(raw_modal_2)\n",
    "    aug_2_modal_1 = augmenter_2(raw_modal_1)\n",
    "    aug_2_modal_2 = augmenter_2(raw_modal_2)\n",
    "    print(aug_1_modal_1.shape)\n",
    "    print(aug_1_modal_2.shape)\n",
    "    print(aug_2_modal_1.shape)\n",
    "    print(aug_2_modal_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'args' from '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/args.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from models.Backbone import DeepSense\n",
    "importlib.reload(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    }
   ],
   "source": [
    "backbone_model = DeepSense(args)\n",
    "# dims = [1, 16, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7680])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_1_modal_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_mod_features_1 = backbone_model(aug_1_modal_1, aug_1_modal_2)\n",
    "enc_mod_features_2 = backbone_model(aug_2_modal_1, aug_2_modal_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ecg': tensor([[-0.1205,  0.0510, -0.0154, -0.0995,  0.0389, -0.0790,  0.1167, -0.1134,\n",
       "           0.0437, -0.0232,  0.0305, -0.0069,  0.0858, -0.0982,  0.0513,  0.0054,\n",
       "          -0.0261,  0.0346, -0.1462, -0.0010,  0.0674,  0.1134,  0.1225, -0.0386,\n",
       "           0.0015, -0.0038,  0.1149, -0.0395, -0.1354,  0.0545, -0.0453, -0.1230,\n",
       "          -0.0513,  0.0319, -0.1029,  0.0298,  0.0984,  0.0499, -0.0849,  0.1402,\n",
       "          -0.0122,  0.0769, -0.0491,  0.0675, -0.0633,  0.0470, -0.0396,  0.0556,\n",
       "          -0.0443,  0.0676, -0.0007,  0.1153,  0.0786,  0.0881,  0.0467, -0.1072,\n",
       "           0.1378,  0.1019,  0.0977,  0.0358, -0.0373,  0.0451,  0.0697,  0.0915],\n",
       "         [-0.1066,  0.0448, -0.0077, -0.1018,  0.0312, -0.0617,  0.1292, -0.1213,\n",
       "           0.0102, -0.0425,  0.0401, -0.0127,  0.1092, -0.0653,  0.0602,  0.0049,\n",
       "          -0.0078,  0.0246, -0.1199, -0.0069,  0.0352,  0.1160,  0.1241, -0.0177,\n",
       "           0.0180,  0.0077,  0.1146, -0.0325, -0.1199,  0.0474, -0.0434, -0.1152,\n",
       "          -0.0596,  0.0436, -0.1273,  0.0419,  0.1097,  0.0573, -0.0915,  0.1214,\n",
       "          -0.0165,  0.0673, -0.0712,  0.0699, -0.0405,  0.0571, -0.0197,  0.0450,\n",
       "          -0.0555,  0.0675,  0.0247,  0.1030,  0.0699,  0.0913,  0.0761, -0.1091,\n",
       "           0.1198,  0.1029,  0.0983,  0.0396, -0.0446,  0.0567,  0.0737,  0.0835]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'hr': tensor([[ 0.0393, -0.0797, -0.0303,  0.0311, -0.0984,  0.0835,  0.1211, -0.0244,\n",
       "           0.0277,  0.0782, -0.0690,  0.0799,  0.0882,  0.0762, -0.0446,  0.0174,\n",
       "          -0.1349,  0.0968,  0.0929, -0.0689, -0.0679,  0.0508, -0.0129,  0.0031,\n",
       "           0.0671, -0.1042,  0.0220,  0.0772, -0.0560, -0.1253,  0.0640,  0.0370,\n",
       "          -0.0170,  0.0588,  0.1693,  0.0788, -0.0827,  0.0798,  0.0052, -0.1106,\n",
       "           0.0697, -0.0777, -0.0409,  0.1070, -0.1030, -0.0133, -0.0486, -0.0779,\n",
       "          -0.0690, -0.0191,  0.0026, -0.0897,  0.1027, -0.1525, -0.0005, -0.0669,\n",
       "           0.0132,  0.1076, -0.0743,  0.0188, -0.1160,  0.0108,  0.0267,  0.0822],\n",
       "         [ 0.0792, -0.0795, -0.0309,  0.0663, -0.1250,  0.0397,  0.1328, -0.0174,\n",
       "          -0.0076,  0.0654, -0.0947,  0.0516,  0.0695,  0.0276, -0.0710, -0.0072,\n",
       "          -0.1093,  0.0953,  0.0963, -0.0795, -0.0954,  0.0367,  0.0134, -0.0376,\n",
       "           0.1021, -0.1062,  0.0460,  0.0784, -0.0599, -0.1319,  0.0594,  0.0226,\n",
       "          -0.0655,  0.0482,  0.1254,  0.1138, -0.0451,  0.0523, -0.0045, -0.1242,\n",
       "           0.0954, -0.0441, -0.0281,  0.1136, -0.1427, -0.0127, -0.0759, -0.0724,\n",
       "          -0.1077, -0.0773,  0.0137, -0.0300,  0.0588, -0.1242, -0.0082, -0.0442,\n",
       "           0.0514,  0.0952, -0.0683,  0.0506, -0.1564, -0.0504,  0.0198,  0.0784]],\n",
       "        grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_mod_features_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "print(enc_mod_features_1['ecg'].shape)\n",
    "print(enc_mod_features_1['hr'].shape)\n",
    "print(enc_mod_features_2['ecg'].shape)\n",
    "print(enc_mod_features_2['hr'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal Model and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.FOCALModules import FOCAL\n",
    "from models.loss import FOCALLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    }
   ],
   "source": [
    "backbone_model = DeepSense(args)\n",
    "focal_model = FOCAL(args, backbone_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_mod_features_1 = backbone_model(aug_1_modal_1, aug_1_modal_2)\n",
    "enc_mod_features_2 = backbone_model(aug_2_modal_1, aug_2_modal_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_2_modal_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proj_head = True\n",
    "mod_features_1, mod_features_2 = focal_model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mod_features_1)\n",
    "# print(mod_features_1['ecg'].shape)\n",
    "# print(mod_features_1['hr'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MESAPairDataset(file_path=args.base_config['train_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            batch_size=16,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)\n",
    "\n",
    "valid_dataset = MESAPairDataset(file_path=args.base_config['valid_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                                            batch_size=16,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data8/jungmin/anaconda3/envs/patchtst/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "if str(list(args.focal_config[\"backbone\"].keys())[0]) == \"DeepSense\":\n",
    "    backbone = DeepSense(args).to(args.focal_config[\"device\"])\n",
    "\n",
    "model = FOCAL(args, backbone)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.focal_config[\"lr\"])\n",
    "focal_loss_fn = FOCALLoss(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "advs_model = AdversarialModel(args).to(args.subj_invariant_config[\"device\"])\n",
    "advs_optimizer = torch.optim.Adam(advs_model.parameters(), lr=args.subj_invariant_config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GaussianNoise augmenter...\n",
      "Loading AmplitudeScale augmenter...\n",
      "Epoch 0 - Adversarial Loss: 36.54109191894531,             Focal Loss: 0.08751561492681503\n",
      "--------------------------------------------------\n",
      "(Validation) Epoch0 - Adversarial Loss: 37.21275329589844,                 Focal Loss: 0.20008498430252075\n",
      "************* Model Saved *************\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer_config = args.trainer_config\n",
    "\n",
    "aug_1_name = args.data_config['augmentation'][0]\n",
    "aug_1_config = args.data_config['augmenter_config'].get(aug_1_name, {})\n",
    "aug_2_name = args.data_config['augmentation'][1]\n",
    "aug_2_config = args.data_config['augmenter_config'].get(aug_2_name, {})\n",
    "\n",
    "aug_1 = init_augmenter(aug_1_name, aug_1_config)\n",
    "aug_2 = init_augmenter(aug_2_name, aug_2_config)\n",
    "\n",
    "model.train()\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for ep in range(trainer_config['epochs']):\n",
    "    running_advs_train_loss = 0\n",
    "    focal_train_loss = 0\n",
    "    \n",
    "    for raw_modal_1, raw_modal_2, subj_label, sleep_label in train_loader:\n",
    "        raw_modal_1, raw_modal_2, subj_label, sleep_label = raw_modal_1.to(device), raw_modal_2.to(device), subj_label.to(device), sleep_label.to(device) # [B, 30], [B, 30*256], [B, 1]\n",
    "        \n",
    "        aug_1_modal_1 = aug_1(raw_modal_1)\n",
    "        aug_2_modal_1 = aug_2(raw_modal_1)\n",
    "        \n",
    "        aug_1_modal_2 = aug_1(raw_modal_2)\n",
    "        aug_2_modal_2 = aug_2(raw_modal_2)\n",
    "        \n",
    "        # For updating the only advs_model (classifier)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in advs_model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        advs_optimizer.zero_grad()\n",
    "        \n",
    "        # Using Encoder for classify the subject\n",
    "        enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
    "        # enc_feature1 -> dict // (example) enc_feature1['ecg'] & enc_feature1['hr'] from Augmentation 1\n",
    "        # enc_feature2 -> dict // (example) enc_feature2['ecg'] & enc_feature2['hr'] from Augmentation 2\n",
    "        \n",
    "        \n",
    "        # Predict the subject\n",
    "        subj_pred = advs_model(enc_feature_1, enc_feature_2)     \n",
    "        advs_loss = advs_model.forward_adversarial_loss(subj_pred, subj_label)\n",
    "        \n",
    "        # To-do for calculating the accuracy\n",
    "        # num_adversary_correct_train_preds += adversarial_loss_fn.get_number_of_correct_preds(x_t1_initial_subject_preds, y)\n",
    "        # total_num_adversary_train_preds += len(x_t1_initial_subject_preds)\n",
    "        \n",
    "        advs_loss.backward()\n",
    "        advs_optimizer.step()\n",
    "        \n",
    "        running_advs_train_loss += advs_loss.item()\n",
    "        \n",
    "        # For efficient memory management\n",
    "        del enc_feature_1, enc_feature_2, subj_pred, advs_loss\n",
    "        \n",
    "        # For updating the only Focal model (SSL model)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in advs_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
    "        \n",
    "        subj_pred = advs_model(enc_feature_1, enc_feature_2) \n",
    "        subj_invariant_loss = advs_model.forward_subject_invariance_loss(subj_pred, subj_label, args.subj_invariant_config['adversarial_weighting_factor']) # DONE -> add subject_invariant function loss\n",
    "        \n",
    "        focal_loss = focal_loss_fn(enc_feature_1, enc_feature_2, subj_invariant_loss) # To-Do -> add regularization term about subject invariant\n",
    "        focal_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        focal_train_loss += focal_loss.item()\n",
    "        \n",
    "        # For efficient memory management\n",
    "        del enc_feature_1, enc_feature_2, subj_pred, focal_loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    if ep % trainer_config['log_interval'] == 0:\n",
    "        print(f\"Epoch {ep} - Adversarial Loss: {running_advs_train_loss/ len(train_loader)}, \\\n",
    "            Focal Loss: {focal_train_loss/ len(train_loader)}\")\n",
    "        \n",
    "        if ep % trainer_config['val_interval'] == 0:\n",
    "            model.eval()\n",
    "            advs_model.eval()\n",
    "            \n",
    "            advs_val_loss = 0\n",
    "            focal_val_loss = 0\n",
    "            \n",
    "            for raw_modal_1, raw_modal_2, subj_label, sleep_label in valid_loader:\n",
    "                raw_modal_1, raw_modal_2, subj_label, sleep_label = raw_modal_1.to(device), raw_modal_2.to(device), subj_label.to(device), sleep_label.to(device)\n",
    "                \n",
    "                aug_1_modal_1, aug_2_modal_1  = aug_1(raw_modal_1), aug_2(raw_modal_1)\n",
    "                aug_1_modal_2, aug_2_modal_2  = aug_1(raw_modal_2), aug_2(raw_modal_2)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # x1_represent, x2_represent = model(raw_modal_1, raw_modal_2)\n",
    "                    enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
    "                    subj_pred = advs_model(enc_feature_1, enc_feature_2)\n",
    "                    \n",
    "                    advs_loss = advs_model.forward_adversarial_loss(subj_pred, subj_label)\n",
    "                    focal_loss = focal_loss_fn(enc_feature_1, enc_feature_2, subj_invariant_loss) # To-Do -> add regularization term about subject invariant\n",
    "                    \n",
    "                    advs_val_loss += advs_loss.item()\n",
    "                    focal_val_loss += focal_loss.item()\n",
    "                    \n",
    "                    # For efficient memory management\n",
    "                    del enc_feature_1, enc_feature_2, subj_pred, focal_loss, advs_loss\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            print(\"-----\"*10)\n",
    "            print(f\"(Validation) Epoch{ep} - Adversarial Loss: {advs_val_loss/ len(valid_loader)}, \\\n",
    "                Focal Loss: {focal_val_loss/ len(valid_loader)}\")                    \n",
    "                            \n",
    "            if focal_val_loss < best_val_loss:\n",
    "                best_val_loss = focal_val_loss\n",
    "                \n",
    "                # To-do -> fix the save model format\n",
    "                # torch.save(model.state_dict(), os.path.join(args.save_dir, 'focal_model.pth'))\n",
    "                # torch.save(advs_model.state_dict(), os.path.join(args.save_dir, 'advs_model.pth'))\n",
    "                print(\"************* Model Saved *************\")\n",
    "            print(\"-----\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 64-bit ('patchtst')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e0a0ed8c9d253a0f21f5456fde53cd73d7f33b56362cce5f62479a7d0aeeb66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
