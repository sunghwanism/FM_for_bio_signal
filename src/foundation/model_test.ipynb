{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data8/jungmin/anaconda3/envs/patchtst/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "import args\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from models.AdversarialModel import AdversarialModel\n",
    "from models.FOCALModules import FOCAL\n",
    "from models.loss import FOCALLoss\n",
    "from data.EfficientDataset import MESAPairDataset\n",
    "from data.Augmentaion import init_augmenter\n",
    "\n",
    "import datetime\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_config: \n",
      " {'train_data_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/dataset/pair_small', 'val_data_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/dataset/pair', 'test_data_dir': '/NFS/Users/moonsh/data/mesa/preproc/pair_test', 'modalities': ['ecg', 'hr'], 'label_key': 'stage', 'subject_key': 'subject_idx', 'train_num_subjects': 100, 'test_num_subjects': 50, 'device': device(type='cpu'), 'log_save_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/logs'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"base_config: \\n {args.base_config}\")\n",
    "# print(f\"focal_config: \\n {args.focal_config} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MESAPairDataset(file_path=args.base_config['train_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            batch_size=4,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "0\n",
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__())\n",
    "\n",
    "for i , (raw_modal_1, raw_modal_2, subj, sleep) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(raw_modal_1.shape)\n",
    "    print(raw_modal_2.shape)\n",
    "    print(subj.shape)\n",
    "    print(sleep.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_config: \n",
      " {'modalities': ['ecg', 'hr'], 'label_key': 'stage', 'augmentation': ['GaussianNoise', 'AmplitudeScale'], 'augmenter_config': {'GaussianNoise': {'max_noise_std': 0.1}, 'AmplitudeScale': {'amplitude_scale': 0.5}}, 'num_classes': None} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"data_config: \\n {args.data_config} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GaussianNoise augmenter...\n",
      "Loading AmplitudeScale augmenter...\n"
     ]
    }
   ],
   "source": [
    "aug_1_name = args.data_config['augmentation'][0]\n",
    "aug_1_config = args.data_config['augmenter_config'].get(aug_1_name, {})\n",
    "aug_2_name = args.data_config['augmentation'][1]\n",
    "aug_2_config = args.data_config['augmenter_config'].get(aug_2_name, {})\n",
    "    \n",
    "augmenter_1 = init_augmenter(aug_1_name, aug_1_config)\n",
    "augmenter_2 = init_augmenter(aug_2_name, aug_2_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7680])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_modal_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7680])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It changes the shape of input: (B, seq) -> (B\n",
    "augmenter_1(raw_modal_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([2, 7680])\n",
      "torch.Size([2, 30])\n",
      "torch.Size([2, 7680])\n",
      "torch.Size([2, 30])\n"
     ]
    }
   ],
   "source": [
    "for i , (raw_modal_1, raw_modal_2, subj, sleep) in enumerate(train_loader):\n",
    "    aug_1_modal_1 = augmenter_1(raw_modal_1)\n",
    "    aug_1_modal_2 = augmenter_1(raw_modal_2)\n",
    "    aug_2_modal_1 = augmenter_2(raw_modal_1)\n",
    "    aug_2_modal_2 = augmenter_2(raw_modal_2)\n",
    "    print(aug_1_modal_1.shape)\n",
    "    print(aug_1_modal_2.shape)\n",
    "    print(aug_2_modal_1.shape)\n",
    "    print(aug_2_modal_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'args' from '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/args.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from models.Backbone import DeepSense\n",
    "importlib.reload(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 16, 32]\n",
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    }
   ],
   "source": [
    "backbone_model = DeepSense(args)\n",
    "# dims = [1, 16, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7680])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_1_modal_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod1 cnn feature shape: torch.Size([2, 64, 280]) mod2 cnn feature shape: torch.Size([2, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([2, 17920]) mod2 rnn feature shape: torch.Size([2, 1920])\n",
      "mod1 cnn feature shape: torch.Size([2, 64, 280]) mod2 cnn feature shape: torch.Size([2, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([2, 17920]) mod2 rnn feature shape: torch.Size([2, 1920])\n"
     ]
    }
   ],
   "source": [
    "enc_mod_features_1 = backbone_model(aug_1_modal_1, aug_1_modal_2)\n",
    "enc_mod_features_2 = backbone_model(aug_2_modal_1, aug_2_modal_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ecg': tensor([[-0.0469,  0.0594, -0.0035, -0.1884,  0.0118,  0.1436, -0.0171,  0.0004,\n",
       "          -0.0408,  0.1022,  0.0754, -0.0747, -0.0902,  0.0702, -0.1361, -0.0884,\n",
       "          -0.0608,  0.0980,  0.0162, -0.0623,  0.0227,  0.0675,  0.2004, -0.0830,\n",
       "          -0.0643,  0.0542,  0.0541, -0.0054,  0.0563, -0.0466, -0.1225, -0.0362,\n",
       "          -0.0038, -0.0609, -0.0492, -0.0611, -0.0693, -0.0300,  0.0859, -0.0529,\n",
       "           0.0483, -0.1190, -0.0650, -0.0939,  0.0668,  0.1367, -0.1430,  0.0392,\n",
       "          -0.0744,  0.0925,  0.0746,  0.0222, -0.0917, -0.0927, -0.0793, -0.0003,\n",
       "           0.0704, -0.0661,  0.0338,  0.0701,  0.0193, -0.0116, -0.1011,  0.0049],\n",
       "         [-0.0101,  0.0699,  0.0118, -0.1676,  0.0237,  0.1079, -0.0151,  0.0360,\n",
       "          -0.0587,  0.0968,  0.0721, -0.0561, -0.0806,  0.0266, -0.1426, -0.0640,\n",
       "          -0.0928,  0.1159, -0.0355, -0.0153,  0.0098,  0.0931,  0.1730, -0.0837,\n",
       "          -0.0983,  0.0318,  0.0434, -0.0042,  0.0514, -0.0955, -0.1056, -0.0503,\n",
       "           0.0258, -0.0471, -0.0764, -0.0665, -0.0821, -0.0169,  0.1060,  0.0041,\n",
       "           0.0332, -0.1151, -0.1011, -0.0887,  0.0948,  0.1092, -0.1423,  0.0496,\n",
       "          -0.1023,  0.1060,  0.0652,  0.0508, -0.1144, -0.0858, -0.0648, -0.0046,\n",
       "           0.0469, -0.0702,  0.0363,  0.0219,  0.0489, -0.0388, -0.0953,  0.0179]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'hr': tensor([[-0.0186,  0.0409,  0.0569,  0.0664,  0.0575, -0.0316, -0.0554,  0.0577,\n",
       "           0.1260,  0.1257, -0.0130,  0.0998, -0.0677, -0.1200, -0.0458,  0.0579,\n",
       "           0.1138, -0.0932,  0.1306,  0.0461,  0.0352, -0.0505,  0.1380,  0.0275,\n",
       "           0.0801, -0.1126, -0.1139, -0.1084, -0.0766, -0.0501,  0.0032,  0.0262,\n",
       "          -0.1246, -0.1529,  0.1175, -0.0720, -0.0247,  0.0014,  0.1583,  0.0101,\n",
       "           0.0643,  0.1834,  0.0278,  0.1150,  0.0727,  0.0801, -0.0706, -0.1075,\n",
       "           0.0825,  0.0007, -0.0659, -0.0890,  0.0259,  0.1089, -0.0616, -0.0259,\n",
       "           0.1121, -0.0377,  0.0595, -0.0638, -0.0893, -0.0380, -0.0684,  0.0574],\n",
       "         [-0.0247,  0.0204,  0.0498,  0.0680,  0.1002, -0.0210, -0.0489,  0.0782,\n",
       "           0.1141,  0.1542,  0.0266,  0.0946, -0.0869, -0.1479, -0.0709,  0.0461,\n",
       "           0.1355, -0.1113,  0.1222,  0.0696,  0.0062, -0.0348,  0.1380,  0.0390,\n",
       "           0.0718, -0.1388, -0.1283, -0.0711, -0.1002, -0.0431, -0.0107,  0.0497,\n",
       "          -0.1162, -0.1868,  0.1384, -0.0593, -0.0019,  0.0188,  0.1549, -0.0011,\n",
       "           0.0959,  0.2108,  0.0088,  0.1177,  0.0496,  0.1005, -0.0814, -0.0836,\n",
       "           0.0492,  0.0111, -0.0675, -0.1021,  0.0414,  0.1206, -0.0426, -0.0097,\n",
       "           0.1048, -0.0475,  0.0569, -0.0733, -0.0917, -0.0447, -0.0929,  0.0739]],\n",
       "        grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_mod_features_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "print(enc_mod_features_1['ecg'].shape)\n",
    "print(enc_mod_features_1['hr'].shape)\n",
    "print(enc_mod_features_2['ecg'].shape)\n",
    "print(enc_mod_features_2['hr'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal Model and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.FOCALModules import FOCAL\n",
    "from models.loss import FOCALLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 16, 32]\n",
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    }
   ],
   "source": [
    "backbone_model = DeepSense(args)\n",
    "focal_model = FOCAL(args, backbone_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod1 cnn feature shape: torch.Size([2, 64, 280]) mod2 cnn feature shape: torch.Size([2, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([2, 17920]) mod2 rnn feature shape: torch.Size([2, 1920])\n",
      "mod1 cnn feature shape: torch.Size([2, 64, 280]) mod2 cnn feature shape: torch.Size([2, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([2, 17920]) mod2 rnn feature shape: torch.Size([2, 1920])\n"
     ]
    }
   ],
   "source": [
    "enc_mod_features_1 = backbone_model(aug_1_modal_1, aug_1_modal_2)\n",
    "enc_mod_features_2 = backbone_model(aug_2_modal_1, aug_2_modal_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_2_modal_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod1 cnn feature shape: torch.Size([2, 64, 280]) mod2 cnn feature shape: torch.Size([2, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([2, 17920]) mod2 rnn feature shape: torch.Size([2, 1920])\n",
      "mod1 cnn feature shape: torch.Size([2, 64, 280]) mod2 cnn feature shape: torch.Size([2, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([2, 17920]) mod2 rnn feature shape: torch.Size([2, 1920])\n"
     ]
    }
   ],
   "source": [
    "# proj_head = True\n",
    "mod_features_1, mod_features_2 = focal_model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ecg': tensor([[ 4.4685e-02, -4.9168e-02,  4.5327e-02,  4.2640e-02,  6.6214e-03,\n",
      "         -1.1088e-01,  7.6225e-03,  5.8352e-02,  5.9951e-02,  9.9122e-02,\n",
      "         -9.1643e-02,  1.1393e-01,  2.2038e-02,  2.1702e-02,  1.2288e-01,\n",
      "          2.4977e-03, -3.9403e-02, -3.9489e-03, -6.5871e-02,  3.4900e-02,\n",
      "         -5.4816e-02, -6.7001e-02,  3.2699e-02, -8.2026e-02, -1.0802e-01,\n",
      "          4.6436e-03, -5.1613e-02, -9.6486e-02, -9.5172e-02, -3.7564e-02,\n",
      "          1.7867e-02, -1.1037e-01, -1.7181e-02, -5.5518e-02, -2.6604e-02,\n",
      "          2.8248e-02, -9.5827e-02, -7.7724e-02, -9.0723e-02,  1.5823e-02,\n",
      "         -1.3332e-01, -1.5923e-01,  8.7660e-02, -2.0874e-02,  8.7037e-02,\n",
      "          4.7225e-02, -5.0642e-02,  3.4934e-02,  5.0135e-02, -1.9298e-01,\n",
      "         -9.1765e-02, -1.1545e-01, -1.7079e-02,  1.1389e-01,  3.7390e-02,\n",
      "          1.2760e-01, -4.8292e-02,  6.0565e-02,  4.1838e-03,  3.4358e-02,\n",
      "          8.7230e-02, -1.0784e-01, -4.1390e-02,  1.2189e-01],\n",
      "        [ 5.1116e-02, -5.3966e-02,  2.2625e-02,  4.0501e-02, -9.2763e-03,\n",
      "         -1.0026e-01, -1.4025e-02,  2.8344e-02,  7.1869e-02,  1.3544e-01,\n",
      "         -1.0190e-01,  7.8017e-02,  2.9199e-02,  6.8186e-02,  1.1727e-01,\n",
      "          6.3829e-03, -3.2490e-02, -2.6546e-05, -2.4289e-02,  2.2479e-02,\n",
      "         -9.1672e-02, -7.8186e-02,  4.5854e-02, -9.7906e-02, -7.3400e-02,\n",
      "          3.4186e-02, -4.4422e-02, -1.2107e-01, -1.0792e-01, -6.1660e-02,\n",
      "          1.9854e-02, -1.0431e-01, -4.2982e-02, -4.5979e-02, -5.4110e-02,\n",
      "          6.3332e-02, -1.1101e-01, -9.4823e-02, -9.7729e-02, -1.2777e-02,\n",
      "         -1.0782e-01, -1.1855e-01,  9.8689e-02, -4.8574e-02,  7.6170e-02,\n",
      "          3.3316e-02, -6.6011e-02,  1.0239e-02,  2.3267e-02, -1.7754e-01,\n",
      "         -8.5685e-02, -9.4397e-02, -2.3949e-02,  8.5451e-02,  6.6896e-02,\n",
      "          1.1939e-01, -1.6179e-02,  8.9182e-02, -2.4646e-02,  2.9939e-02,\n",
      "          5.7418e-02, -8.5618e-02, -2.7069e-02,  1.2144e-01]],\n",
      "       grad_fn=<AddmmBackward0>), 'hr': tensor([[-0.0774, -0.1114,  0.0745,  0.0469, -0.0311,  0.0086, -0.1147,  0.0309,\n",
      "         -0.1270,  0.1096,  0.1134, -0.1272, -0.0024, -0.0418, -0.1007,  0.0253,\n",
      "          0.0832,  0.0627, -0.0536,  0.0813,  0.1163,  0.0985, -0.0741,  0.0699,\n",
      "         -0.0433,  0.0476,  0.0625,  0.0161, -0.0382, -0.0775,  0.0609, -0.0909,\n",
      "         -0.1078, -0.0903, -0.0200, -0.0180,  0.1186, -0.0742, -0.1309, -0.0358,\n",
      "         -0.0844, -0.0993,  0.0828, -0.0046,  0.0680, -0.0882, -0.0994, -0.0531,\n",
      "         -0.0718, -0.0478, -0.0790, -0.0973,  0.1265, -0.0797, -0.1399, -0.1162,\n",
      "         -0.1054, -0.0099, -0.0780,  0.0563, -0.1275,  0.0095, -0.0506, -0.0757],\n",
      "        [-0.0923, -0.0752,  0.0809,  0.0431, -0.0584, -0.0123, -0.1308,  0.0280,\n",
      "         -0.1513,  0.0827,  0.0955, -0.1276,  0.0442, -0.0259, -0.0786,  0.0404,\n",
      "          0.0739,  0.0651, -0.0279,  0.0700,  0.0518,  0.1070, -0.0675,  0.0448,\n",
      "         -0.0349,  0.0536,  0.0522,  0.0238, -0.0032, -0.0652,  0.0785, -0.0947,\n",
      "         -0.1071, -0.0461, -0.0105, -0.0030,  0.1130, -0.0628, -0.1214, -0.0196,\n",
      "         -0.0591, -0.1151,  0.0936,  0.0015,  0.0411, -0.0473, -0.0790, -0.0635,\n",
      "         -0.0353, -0.0619, -0.0789, -0.0979,  0.0839, -0.0748, -0.1335, -0.1199,\n",
      "         -0.1184, -0.0374, -0.0854,  0.0222, -0.1075, -0.0051, -0.0283, -0.0937]],\n",
      "       grad_fn=<AddmmBackward0>)}\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "print(mod_features_1)\n",
    "print(mod_features_1['ecg'].shape)\n",
    "print(mod_features_1['hr'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ecg': tensor([[ 0.0371, -0.0468,  0.0518,  0.0490,  0.0114, -0.1230,  0.0031,  0.0635,\n",
      "          0.0655,  0.1005, -0.0900,  0.1190,  0.0243,  0.0245,  0.1211,  0.0090,\n",
      "         -0.0433, -0.0005, -0.0747,  0.0430, -0.0481, -0.0592,  0.0367, -0.0914,\n",
      "         -0.1147,  0.0067, -0.0567, -0.0978, -0.0905, -0.0431,  0.0310, -0.1173,\n",
      "         -0.0158, -0.0544, -0.0356,  0.0230, -0.0930, -0.0780, -0.0846,  0.0162,\n",
      "         -0.1362, -0.1443,  0.0912, -0.0176,  0.0944,  0.0440, -0.0380,  0.0381,\n",
      "          0.0506, -0.1917, -0.0929, -0.1129, -0.0240,  0.1162,  0.0391,  0.1196,\n",
      "         -0.0566,  0.0551, -0.0010,  0.0361,  0.0852, -0.1069, -0.0434,  0.1198],\n",
      "        [ 0.0484, -0.0547,  0.0299,  0.0495,  0.0006, -0.1061, -0.0130,  0.0322,\n",
      "          0.0750,  0.1430, -0.0762,  0.0740,  0.0318,  0.0643,  0.1068,  0.0134,\n",
      "         -0.0256, -0.0038, -0.0278,  0.0136, -0.0891, -0.0785,  0.0376, -0.0965,\n",
      "         -0.0642,  0.0319, -0.0429, -0.1224, -0.1066, -0.0585,  0.0136, -0.1050,\n",
      "         -0.0438, -0.0451, -0.0494,  0.0640, -0.1136, -0.0929, -0.1058, -0.0234,\n",
      "         -0.1061, -0.1117,  0.1016, -0.0525,  0.0618,  0.0377, -0.0742,  0.0121,\n",
      "          0.0262, -0.1831, -0.0809, -0.0817, -0.0262,  0.0830,  0.0749,  0.1146,\n",
      "         -0.0093,  0.0962, -0.0245,  0.0290,  0.0554, -0.0773, -0.0215,  0.1156]],\n",
      "       grad_fn=<AddmmBackward0>), 'hr': tensor([[-0.0923, -0.0980,  0.0970,  0.0452, -0.0707,  0.0067, -0.0997,  0.0452,\n",
      "         -0.1209,  0.0378,  0.0785, -0.1295,  0.0366, -0.0090, -0.0808,  0.0303,\n",
      "          0.0971,  0.0402, -0.0717,  0.0798,  0.0961,  0.1028, -0.0276,  0.0283,\n",
      "         -0.0540,  0.0426,  0.0587, -0.0011, -0.0416, -0.0446,  0.0433, -0.1226,\n",
      "         -0.1039, -0.0203,  0.0090, -0.0495,  0.1298, -0.0870, -0.1124, -0.0132,\n",
      "         -0.0827, -0.1514,  0.0701, -0.0189,  0.0776, -0.0624, -0.0711, -0.0615,\n",
      "         -0.0419, -0.0868, -0.1009, -0.0901,  0.0760, -0.1028, -0.1680, -0.1024,\n",
      "         -0.1163, -0.0297, -0.0853,  0.0389, -0.0790,  0.0166, -0.0743, -0.0720],\n",
      "        [-0.0729, -0.0745,  0.0707,  0.0445, -0.0481,  0.0100, -0.1434,  0.0550,\n",
      "         -0.1468,  0.1360,  0.1060, -0.1063, -0.0060, -0.0468, -0.0883,  0.0405,\n",
      "          0.0864,  0.0356, -0.0750,  0.0561,  0.1142,  0.0776, -0.0851,  0.0820,\n",
      "         -0.0886,  0.0589,  0.0897,  0.0439, -0.0283, -0.0597,  0.0426, -0.1074,\n",
      "         -0.1211, -0.1190, -0.0264, -0.0237,  0.1441, -0.0688, -0.0961, -0.0299,\n",
      "         -0.0772, -0.0792,  0.0783, -0.0213,  0.0357, -0.0533, -0.0970, -0.0212,\n",
      "         -0.0827, -0.0606, -0.0475, -0.0710,  0.1268, -0.0847, -0.1094, -0.1484,\n",
      "         -0.0973, -0.0047, -0.0762,  0.0617, -0.1363,  0.0147, -0.0261, -0.1018]],\n",
      "       grad_fn=<AddmmBackward0>)}\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "print(mod_features_2)\n",
    "print(mod_features_2['ecg'].shape)\n",
    "print(mod_features_2['hr'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FOCALLoss(\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (similarity_f): CosineSimilarity()\n",
       "  (orthonal_loss_f): CosineEmbeddingLoss()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss_fn = FOCALLoss(args)\n",
    "focal_loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MESAPairDataset(file_path=args.base_config['train_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            batch_size=4,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 16, 32]\n",
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data8/jungmin/anaconda3/envs/patchtst/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "if str(list(args.focal_config[\"backbone\"].keys())[0]) == \"DeepSense\":\n",
    "    backbone = DeepSense(args).to(args.focal_config[\"device\"])\n",
    "\n",
    "model = FOCAL(args, backbone)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.focal_config[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "advs_model = AdversarialModel(args).to(args.subj_invariant_config[\"device\"])\n",
    "advs_optimizer = torch.optim.Adam(advs_model.parameters(), lr=args.subj_invariant_config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GaussianNoise augmenter...\n",
      "Loading AmplitudeScale augmenter...\n",
      "mod1 cnn feature shape: torch.Size([4, 64, 280]) mod2 cnn feature shape: torch.Size([4, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([4, 17920]) mod2 rnn feature shape: torch.Size([4, 1920])\n",
      "mod1 cnn feature shape: torch.Size([4, 64, 280]) mod2 cnn feature shape: torch.Size([4, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([4, 17920]) mod2 rnn feature shape: torch.Size([4, 1920])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 4])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(subj_pred\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(subj_label\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 47\u001b[0m advs_loss \u001b[38;5;241m=\u001b[39m \u001b[43madvs_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_adversarial_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubj_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubj_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# To-do for calculating the accuracy\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# num_adversary_correct_train_preds += adversarial_loss_fn.get_number_of_correct_preds(x_t1_initial_subject_preds, y)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# total_num_adversary_train_preds += len(x_t1_initial_subject_preds)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m advs_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/models/AdversarialModel.py:48\u001b[0m, in \u001b[0;36mAdversarialModel.forward_adversarial_loss\u001b[0;34m(self, subject_preds, subject_labels)\u001b[0m\n\u001b[1;32m     45\u001b[0m curr_batch_size \u001b[39m=\u001b[39m subject_outs\u001b[39m.\u001b[39msize(BATCH_DIM)\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(curr_batch_size):\n\u001b[0;32m---> 48\u001b[0m     j \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(subject_labels[i, :])\n\u001b[1;32m     49\u001b[0m     adversarial_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1.\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mlog(log_noise \u001b[39m+\u001b[39m subject_outs[i, j])\n\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m adversarial_loss\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "trainer_config = args.trainer_config\n",
    "\n",
    "aug_1_name = args.data_config['augmentation'][0]\n",
    "aug_1_config = args.data_config['augmenter_config'].get(aug_1_name, {})\n",
    "aug_2_name = args.data_config['augmentation'][1]\n",
    "aug_2_config = args.data_config['augmenter_config'].get(aug_2_name, {})\n",
    "\n",
    "aug_1 = init_augmenter(aug_1_name, aug_1_config)\n",
    "aug_2 = init_augmenter(aug_2_name, aug_2_config)\n",
    "\n",
    "model.train()\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for ep in range(trainer_config['epochs']):\n",
    "    running_advs_train_loss = 0\n",
    "    focal_train_loss = 0\n",
    "    \n",
    "    for raw_modal_1, raw_modal_2, subj_label, sleep_label in train_loader:\n",
    "        raw_modal_1, raw_modal_2, subj_label, sleep_label = raw_modal_1.to(device), raw_modal_2.to(device), subj_label.to(device), sleep_label.to(device) # [B, 30], [B, 30*256], [B, 1]\n",
    "        \n",
    "        aug_1_modal_1 = aug_1(raw_modal_1)\n",
    "        aug_2_modal_1 = aug_2(raw_modal_1)\n",
    "        \n",
    "        aug_1_modal_2 = aug_1(raw_modal_2)\n",
    "        aug_2_modal_2 = aug_2(raw_modal_2)\n",
    "        \n",
    "        # For updating the only advs_model (classifier)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in advs_model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        advs_optimizer.zero_grad()\n",
    "        \n",
    "        # Using Encoder for classify the subject\n",
    "        enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
    "        # enc_feature1 -> dict // (example) enc_feature1['ecg'] & enc_feature1['hr'] from Augmentation 1\n",
    "        # enc_feature2 -> dict // (example) enc_feature2['ecg'] & enc_feature2['hr'] from Augmentation 2\n",
    "        \n",
    "        \n",
    "        # Predict the subject\n",
    "        subj_pred = advs_model(enc_feature_1, enc_feature_2) \n",
    "        # or subj_pred = advs_model(enc_feature_1['ecg'], enc_feature_1['hr], enc_feature_2['ecg'], enc_feature_2['hr'])\n",
    "        print(subj_pred.shape)\n",
    "        print(subj_label.shape)\n",
    "        \n",
    "        advs_loss = advs_model.forward_adversarial_loss(subj_pred, subj_label)\n",
    "        \n",
    "        # To-do for calculating the accuracy\n",
    "        # num_adversary_correct_train_preds += adversarial_loss_fn.get_number_of_correct_preds(x_t1_initial_subject_preds, y)\n",
    "        # total_num_adversary_train_preds += len(x_t1_initial_subject_preds)\n",
    "        \n",
    "        advs_loss.backward()\n",
    "        advs_optimizer.step()\n",
    "        \n",
    "        running_advs_train_loss += advs_loss.item()\n",
    "        \n",
    "        # For efficient memory management\n",
    "        del enc_feature_1, enc_feature_2, subj_pred, advs_loss\n",
    "        \n",
    "        # For updating the only Focal model (SSL model)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in advs_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x1_represent, x2_represent = model(raw_modal_1, raw_modal_2)\n",
    "        \n",
    "        x1_embd, x2_embd = model.encoder(raw_modal_1, raw_modal_2)\n",
    "        subj_pred = advs_model(x1_embd, x2_embd)\n",
    "        subj_invariant_loss = advs_model.forward_subject_invariance_loss(subj_pred, subj_label, args.subj_invariant_config['adversarial_weighting_factor']) # DONE -> add subject_invariant function loss\n",
    "        \n",
    "        focal_loss = focal_loss_fn(x1_represent, x2_represent, subj_invariant_loss) # To-Do -> add regularization term about subject invariant\n",
    "        focal_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        focal_train_loss += focal_loss.item()\n",
    "        \n",
    "        # For efficient memory management\n",
    "        del x1_represent, x2_represent, x1_embd, x2_embd, subj_pred, focal_loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    if ep % args.log_interval == 0:\n",
    "        print(f\"Epoch {ep} - Adversarial Loss: {running_advs_train_loss/ len(train_loader)}, \\\n",
    "            Focal Loss: {focal_train_loss/ len(train_loader)}\")\n",
    "        \n",
    "        if ep % args.val_interval == 0:\n",
    "            model.eval()\n",
    "            advs_model.eval()\n",
    "            \n",
    "            advs_val_loss = 0\n",
    "            focal_val_loss = 0\n",
    "            \n",
    "            for raw_modal_1, raw_modal_2, subj_label, sleep_label in val_loader:\n",
    "                raw_modal_1, raw_modal_2, subj_label, sleep_label = raw_modal_1.to(device), raw_modal_2.to(device), subj_label.to(device), sleep_label.to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    x1_represent, x2_represent = model(raw_modal_1, raw_modal_2)\n",
    "                    x1_embd, x2_embd = model.encoder(raw_modal_1), model.encoder(raw_modal_2)\n",
    "                    subj_pred = advs_model(x1_embd, x2_embd) # output -> sigmoid value\n",
    "                    \n",
    "                    advs_loss = advs_model.loss_fcn(subj_pred, subj_label)\n",
    "                    focal_loss = focal_loss_fn(x1_represent, x2_represent, subj_pred, subj_label)\n",
    "                    \n",
    "                    advs_val_loss += advs_loss.item()\n",
    "                    focal_val_loss += focal_loss.item()\n",
    "                    \n",
    "                    # For efficient memory management\n",
    "                    del x1_represent, x2_represent, x1_embd, x2_embd, subj_pred, focal_loss\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            print(\"-----\"*10)\n",
    "            print(f\"(Validation) Epoch{ep} - Adversarial Loss: {advs_val_loss/ len(val_loader)}, \\\n",
    "                Focal Loss: {focal_val_loss/ len(val_loader)}\")                    \n",
    "                            \n",
    "            if focal_val_loss < best_val_loss:\n",
    "                best_val_loss = focal_val_loss\n",
    "                \n",
    "                # To-do -> fix the save model format\n",
    "                # torch.save(model.state_dict(), os.path.join(args.save_dir, 'focal_model.pth'))\n",
    "                # torch.save(advs_model.state_dict(), os.path.join(args.save_dir, 'advs_model.pth'))\n",
    "                print(\"************* Model Saved *************\")\n",
    "            print(\"-----\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 64-bit ('patchtst')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e0a0ed8c9d253a0f21f5456fde53cd73d7f33b56362cce5f62479a7d0aeeb66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
