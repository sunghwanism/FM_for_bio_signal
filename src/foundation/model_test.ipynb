{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data8/jungmin/anaconda3/envs/patchtst/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "import args\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from models.AdversarialModel import AdversarialModel\n",
    "from models.FOCALModules import FOCAL\n",
    "from models.loss import FOCALLoss\n",
    "from data.EfficientDataset import MESAPairDataset\n",
    "from data.Augmentaion import init_augmenter\n",
    "\n",
    "import datetime\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_config: \n",
      " {'train_data_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/dataset/pair_small', 'valid_data_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/dataset/pair_small_valid', 'test_data_dir': '/NFS/Users/moonsh/data/mesa/preproc/pair_test', 'modalities': ['ecg', 'hr'], 'label_key': 'stage', 'subject_key': 'subject_idx', 'train_num_subjects': 100, 'test_num_subjects': 50, 'device': device(type='cpu'), 'log_save_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/logs'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"base_config: \\n {args.base_config}\")\n",
    "# print(f\"focal_config: \\n {args.focal_config} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MESAPairDataset(file_path=args.base_config['train_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            batch_size=14,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "0\n",
      "torch.Size([14, 7680])\n",
      "torch.Size([14, 30])\n",
      "torch.Size([14])\n",
      "torch.Size([14])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__())\n",
    "\n",
    "for i , (raw_modal_1, raw_modal_2, subj, sleep) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(raw_modal_1.shape)\n",
    "    print(raw_modal_2.shape)\n",
    "    print(subj.shape)\n",
    "    print(sleep.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_config: \n",
      " {'modalities': ['ecg', 'hr'], 'label_key': 'stage', 'augmentation': ['GaussianNoise', 'AmplitudeScale'], 'augmenter_config': {'GaussianNoise': {'max_noise_std': 0.1}, 'AmplitudeScale': {'amplitude_scale': 0.5}}, 'num_classes': None} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"data_config: \\n {args.data_config} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GaussianNoise augmenter...\n",
      "Loading AmplitudeScale augmenter...\n"
     ]
    }
   ],
   "source": [
    "aug_1_name = args.data_config['augmentation'][0]\n",
    "aug_1_config = args.data_config['augmenter_config'].get(aug_1_name, {})\n",
    "aug_2_name = args.data_config['augmentation'][1]\n",
    "aug_2_config = args.data_config['augmenter_config'].get(aug_2_name, {})\n",
    "    \n",
    "augmenter_1 = init_augmenter(aug_1_name, aug_1_config)\n",
    "augmenter_2 = init_augmenter(aug_2_name, aug_2_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 7680])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_modal_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 7680])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It changes the shape of input: (B, seq) -> (B\n",
    "augmenter_1(raw_modal_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 7680])\n",
      "torch.Size([14, 30])\n",
      "torch.Size([14, 7680])\n",
      "torch.Size([14, 30])\n",
      "torch.Size([2, 7680])\n",
      "torch.Size([2, 30])\n",
      "torch.Size([2, 7680])\n",
      "torch.Size([2, 30])\n"
     ]
    }
   ],
   "source": [
    "for i , (raw_modal_1, raw_modal_2, subj, sleep) in enumerate(train_loader):\n",
    "    aug_1_modal_1 = augmenter_1(raw_modal_1)\n",
    "    aug_1_modal_2 = augmenter_1(raw_modal_2)\n",
    "    aug_2_modal_1 = augmenter_2(raw_modal_1)\n",
    "    aug_2_modal_2 = augmenter_2(raw_modal_2)\n",
    "    print(aug_1_modal_1.shape)\n",
    "    print(aug_1_modal_2.shape)\n",
    "    print(aug_2_modal_1.shape)\n",
    "    print(aug_2_modal_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'args' from '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/args.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from models.Backbone import DeepSense\n",
    "importlib.reload(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    }
   ],
   "source": [
    "backbone_model = DeepSense(args)\n",
    "# dims = [1, 16, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7680])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_1_modal_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_mod_features_1 = backbone_model(aug_1_modal_1, aug_1_modal_2)\n",
    "enc_mod_features_2 = backbone_model(aug_2_modal_1, aug_2_modal_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ecg': tensor([[ 4.2017e-02, -1.9050e-03, -1.0544e-01, -2.3508e-02,  2.7437e-02,\n",
       "           1.0870e-01, -8.6688e-02, -1.3929e-02, -9.3054e-02,  7.1771e-02,\n",
       "          -2.2432e-02, -7.7172e-02, -6.7358e-02, -5.2277e-02,  1.1432e-02,\n",
       "           2.6332e-02,  7.7238e-02, -1.2176e-02,  1.0183e-01,  6.7707e-02,\n",
       "           5.6365e-03,  1.1129e-01,  4.2131e-02, -4.5158e-02, -2.7685e-02,\n",
       "          -6.4567e-02,  9.3014e-03, -4.6144e-02,  1.2319e-01,  6.4648e-02,\n",
       "          -4.3848e-02, -1.3296e-01, -2.0376e-02,  5.3034e-02, -7.3046e-02,\n",
       "          -1.0714e-01,  3.9749e-02, -6.5112e-02, -4.5738e-03,  8.8621e-02,\n",
       "           6.3777e-03,  4.8814e-02,  5.5052e-02,  5.4076e-02,  4.7503e-02,\n",
       "           1.4531e-02, -2.8623e-02, -1.2066e-01,  1.1547e-01,  5.1368e-02,\n",
       "           5.3672e-02, -3.4242e-03,  5.5235e-02,  9.0216e-02,  1.0376e-01,\n",
       "           4.2634e-02, -3.9785e-02, -9.0280e-02,  2.9262e-02,  1.4152e-01,\n",
       "          -1.0171e-01,  1.0605e-01, -2.3399e-02, -6.4366e-02],\n",
       "         [ 4.6072e-02, -1.1256e-02, -1.4098e-01, -2.3623e-02,  4.1231e-02,\n",
       "           8.3882e-02, -1.0245e-01,  2.0546e-02, -9.3676e-02,  3.5569e-02,\n",
       "          -2.4667e-02, -1.0718e-01, -7.1076e-02, -7.8353e-02,  9.7522e-03,\n",
       "          -9.2471e-03,  6.8381e-02, -1.0893e-02,  6.9559e-02,  4.6535e-02,\n",
       "           8.0059e-04,  7.8306e-02,  4.9897e-02, -5.0295e-02, -3.5916e-02,\n",
       "          -6.6252e-02,  1.3908e-02, -4.6297e-02,  8.8949e-02,  5.2069e-02,\n",
       "          -5.2870e-02, -1.4146e-01, -1.4672e-02,  4.6833e-02, -4.2738e-02,\n",
       "          -1.0290e-01,  3.5634e-02, -6.9202e-02,  2.0880e-02,  8.6330e-02,\n",
       "          -1.7252e-04,  2.4823e-02,  4.2687e-02,  6.0218e-02,  2.1795e-02,\n",
       "          -8.0958e-05, -3.3697e-02, -1.2858e-01,  8.3071e-02,  2.3939e-02,\n",
       "           1.9362e-02, -2.7072e-02,  4.6306e-02,  6.6938e-02,  8.6840e-02,\n",
       "          -8.9021e-03, -5.3517e-02, -8.8623e-02,  5.3687e-02,  1.1092e-01,\n",
       "          -1.2221e-01,  8.4235e-02, -1.0348e-02, -5.4329e-02]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'hr': tensor([[ 0.0410, -0.1117,  0.0810,  0.0699, -0.0436,  0.0382, -0.0030,  0.0443,\n",
       "           0.1219, -0.0739,  0.0140,  0.0609,  0.1044, -0.0431, -0.0393,  0.0112,\n",
       "          -0.1439, -0.1446, -0.1113, -0.0415,  0.0409,  0.0654,  0.0161, -0.0028,\n",
       "          -0.0230,  0.0104, -0.1180,  0.1283,  0.0425,  0.0863, -0.1218,  0.1524,\n",
       "          -0.1282, -0.1311, -0.0900,  0.0076, -0.1990, -0.0493, -0.1233, -0.0228,\n",
       "          -0.0630, -0.0075,  0.0538,  0.0740, -0.0845,  0.0450, -0.0727,  0.0999,\n",
       "          -0.0843,  0.1023,  0.0656,  0.0184, -0.0567,  0.1315,  0.1220, -0.0559,\n",
       "          -0.0384, -0.0176, -0.0325, -0.1113, -0.0189, -0.0730,  0.0420, -0.0100],\n",
       "         [ 0.0707, -0.0452,  0.1097,  0.0569,  0.0132,  0.0489, -0.0149,  0.0496,\n",
       "           0.0934, -0.0345, -0.0328, -0.0055,  0.0667, -0.0443,  0.0280, -0.0223,\n",
       "          -0.1539, -0.2153, -0.1196, -0.1030,  0.0092,  0.0719, -0.0225,  0.0068,\n",
       "          -0.0614,  0.0010, -0.1092,  0.0711,  0.0478,  0.0740, -0.1168,  0.0995,\n",
       "          -0.1029, -0.1435, -0.0581, -0.0379, -0.2079, -0.0524, -0.0885,  0.0525,\n",
       "          -0.1330,  0.0505,  0.0556,  0.0490, -0.0549,  0.0636, -0.1022,  0.1261,\n",
       "          -0.0656,  0.0797,  0.0472, -0.0058, -0.0923,  0.0808,  0.1543, -0.0588,\n",
       "          -0.0553, -0.0573, -0.0105, -0.0375,  0.0422, -0.0667,  0.0530, -0.0051]],\n",
       "        grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_mod_features_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "print(enc_mod_features_1['ecg'].shape)\n",
    "print(enc_mod_features_1['hr'].shape)\n",
    "print(enc_mod_features_2['ecg'].shape)\n",
    "print(enc_mod_features_2['hr'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal Model and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.FOCALModules import FOCAL\n",
    "from models.loss import FOCALLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    }
   ],
   "source": [
    "backbone_model = DeepSense(args)\n",
    "focal_model = FOCAL(args, backbone_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_mod_features_1 = backbone_model(aug_1_modal_1, aug_1_modal_2)\n",
    "enc_mod_features_2 = backbone_model(aug_2_modal_1, aug_2_modal_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_2_modal_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proj_head = True\n",
    "mod_features_1, mod_features_2 = focal_model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mod_features_1)\n",
    "# print(mod_features_1['ecg'].shape)\n",
    "# print(mod_features_1['hr'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MESAPairDataset(file_path=args.base_config['train_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            batch_size=16,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)\n",
    "\n",
    "valid_dataset = MESAPairDataset(file_path=args.base_config['valid_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                                            batch_size=16,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data8/jungmin/anaconda3/envs/patchtst/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "if str(list(args.focal_config[\"backbone\"].keys())[0]) == \"DeepSense\":\n",
    "    backbone = DeepSense(args).to(args.focal_config[\"device\"])\n",
    "\n",
    "model = FOCAL(args, backbone)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.focal_config[\"lr\"])\n",
    "focal_loss_fn = FOCALLoss(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "advs_model = AdversarialModel(args).to(args.subj_invariant_config[\"device\"])\n",
    "advs_optimizer = torch.optim.Adam(advs_model.parameters(), lr=args.subj_invariant_config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GaussianNoise augmenter...\n",
      "Loading AmplitudeScale augmenter...\n",
      "Epoch 0 - Adversarial Loss: 36.68265151977539,             Focal Loss: 0.15066389739513397\n",
      "--------------------------------------------------\n",
      "(Validation) Epoch0 - Adversarial Loss: 36.60572814941406,                 Focal Loss: 0.03700614720582962\n",
      "************* Model Saved *************\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer_config = args.trainer_config\n",
    "\n",
    "aug_1_name = args.data_config['augmentation'][0]\n",
    "aug_1_config = args.data_config['augmenter_config'].get(aug_1_name, {})\n",
    "aug_2_name = args.data_config['augmentation'][1]\n",
    "aug_2_config = args.data_config['augmenter_config'].get(aug_2_name, {})\n",
    "\n",
    "aug_1 = init_augmenter(aug_1_name, aug_1_config)\n",
    "aug_2 = init_augmenter(aug_2_name, aug_2_config)\n",
    "\n",
    "model.train()\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for ep in range(trainer_config['epochs']):\n",
    "    running_advs_train_loss = 0\n",
    "    focal_train_loss = 0\n",
    "    \n",
    "    for raw_modal_1, raw_modal_2, subj_label, sleep_label in train_loader:\n",
    "        raw_modal_1, raw_modal_2, subj_label, sleep_label = raw_modal_1.to(device), raw_modal_2.to(device), subj_label.to(device), sleep_label.to(device) # [B, 30], [B, 30*256], [B, 1]\n",
    "        \n",
    "        aug_1_modal_1 = aug_1(raw_modal_1)\n",
    "        aug_2_modal_1 = aug_2(raw_modal_1)\n",
    "        \n",
    "        aug_1_modal_2 = aug_1(raw_modal_2)\n",
    "        aug_2_modal_2 = aug_2(raw_modal_2)\n",
    "        \n",
    "        # For updating the only advs_model (classifier)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in advs_model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        advs_optimizer.zero_grad()\n",
    "        \n",
    "        # Using Encoder for classify the subject\n",
    "        enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
    "        # enc_feature1 -> dict // (example) enc_feature1['ecg'] & enc_feature1['hr'] from Augmentation 1\n",
    "        # enc_feature2 -> dict // (example) enc_feature2['ecg'] & enc_feature2['hr'] from Augmentation 2\n",
    "        \n",
    "        \n",
    "        # Predict the subject\n",
    "        subj_pred = advs_model(enc_feature_1, enc_feature_2)     \n",
    "        advs_loss = advs_model.forward_adversarial_loss(subj_pred, subj_label)\n",
    "        \n",
    "        # To-do for calculating the accuracy\n",
    "        # num_adversary_correct_train_preds += adversarial_loss_fn.get_number_of_correct_preds(x_t1_initial_subject_preds, y)\n",
    "        # total_num_adversary_train_preds += len(x_t1_initial_subject_preds)\n",
    "        \n",
    "        advs_loss.backward()\n",
    "        advs_optimizer.step()\n",
    "        \n",
    "        running_advs_train_loss += advs_loss.item()\n",
    "        \n",
    "        # For efficient memory management\n",
    "        del enc_feature_1, enc_feature_2, subj_pred, advs_loss\n",
    "        \n",
    "        # For updating the only Focal model (SSL model)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in advs_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
    "        \n",
    "        subj_pred = advs_model(enc_feature_1, enc_feature_2) \n",
    "        subj_invariant_loss = advs_model.forward_subject_invariance_loss(subj_pred, subj_label, args.subj_invariant_config['adversarial_weighting_factor']) # DONE -> add subject_invariant function loss\n",
    "        \n",
    "        focal_loss = focal_loss_fn(enc_feature_1, enc_feature_2, subj_invariant_loss) # To-Do -> add regularization term about subject invariant\n",
    "        focal_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        focal_train_loss += focal_loss.item()\n",
    "        \n",
    "        # For efficient memory management\n",
    "        del enc_feature_1, enc_feature_2, subj_pred, focal_loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    if ep % trainer_config['log_interval'] == 0:\n",
    "        print(f\"Epoch {ep} - Adversarial Loss: {running_advs_train_loss/ len(train_loader)}, \\\n",
    "            Focal Loss: {focal_train_loss/ len(train_loader)}\")\n",
    "        \n",
    "        if ep % trainer_config['val_interval'] == 0:\n",
    "            model.eval()\n",
    "            advs_model.eval()\n",
    "            \n",
    "            advs_val_loss = 0\n",
    "            focal_val_loss = 0\n",
    "            \n",
    "            for raw_modal_1, raw_modal_2, subj_label, sleep_label in valid_loader:\n",
    "                raw_modal_1, raw_modal_2, subj_label, sleep_label = raw_modal_1.to(device), raw_modal_2.to(device), subj_label.to(device), sleep_label.to(device)\n",
    "                \n",
    "                aug_1_modal_1, aug_2_modal_1  = aug_1(raw_modal_1), aug_2(raw_modal_1)\n",
    "                aug_1_modal_2, aug_2_modal_2  = aug_1(raw_modal_2), aug_2(raw_modal_2)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # x1_represent, x2_represent = model(raw_modal_1, raw_modal_2)\n",
    "                    enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
    "                    subj_pred = advs_model(enc_feature_1, enc_feature_2)\n",
    "                    \n",
    "                    advs_loss = advs_model.forward_adversarial_loss(subj_pred, subj_label)\n",
    "                    focal_loss = focal_loss_fn(enc_feature_1, enc_feature_2, subj_invariant_loss) # To-Do -> add regularization term about subject invariant\n",
    "                    \n",
    "                    advs_val_loss += advs_loss.item()\n",
    "                    focal_val_loss += focal_loss.item()\n",
    "                    \n",
    "                    # For efficient memory management\n",
    "                    del enc_feature_1, enc_feature_2, subj_pred, focal_loss, advs_loss\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            print(\"-----\"*10)\n",
    "            print(f\"(Validation) Epoch{ep} - Adversarial Loss: {advs_val_loss/ len(valid_loader)}, \\\n",
    "                Focal Loss: {focal_val_loss/ len(valid_loader)}\")                    \n",
    "                            \n",
    "            if focal_val_loss < best_val_loss:\n",
    "                best_val_loss = focal_val_loss\n",
    "                \n",
    "                # To-do -> fix the save model format\n",
    "                # torch.save(model.state_dict(), os.path.join(args.save_dir, 'focal_model.pth'))\n",
    "                # torch.save(advs_model.state_dict(), os.path.join(args.save_dir, 'advs_model.pth'))\n",
    "                print(\"************* Model Saved *************\")\n",
    "            print(\"-----\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 64-bit ('patchtst')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e0a0ed8c9d253a0f21f5456fde53cd73d7f33b56362cce5f62479a7d0aeeb66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
