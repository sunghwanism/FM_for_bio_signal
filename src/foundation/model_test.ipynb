{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data8/jungmin/anaconda3/envs/patchtst/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "import args\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from models.AdversarialModel import AdversarialModel\n",
    "from models.FOCALModules import FOCAL\n",
    "from models.loss import FOCALLoss\n",
    "from data.EfficientDataset import MESAPairDataset\n",
    "from data.Augmentaion import init_augmenter\n",
    "\n",
    "import datetime\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_config: \n",
      " {'train_data_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/dataset/pair_small', 'val_data_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/dataset/pair', 'test_data_dir': '/NFS/Users/moonsh/data/mesa/preproc/pair_test', 'modalities': ['ecg', 'hr'], 'label_key': 'stage', 'subject_key': 'subject_idx', 'train_num_subjects': 100, 'test_num_subjects': 50, 'device': device(type='cpu'), 'log_save_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/logs'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"base_config: \\n {args.base_config}\")\n",
    "# print(f\"focal_config: \\n {args.focal_config} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MESAPairDataset(file_path=args.base_config['train_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            batch_size=4,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "0\n",
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__())\n",
    "\n",
    "for i , (raw_modal_1, raw_modal_2, subj, sleep) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(raw_modal_1.shape)\n",
    "    print(raw_modal_2.shape)\n",
    "    print(subj.shape)\n",
    "    print(sleep.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_config: \n",
      " {'modalities': ['ecg', 'hr'], 'label_key': 'stage', 'augmentation': ['GaussianNoise', 'AmplitudeScale'], 'augmenter_config': {'GaussianNoise': {'max_noise_std': 0.1}, 'AmplitudeScale': {'amplitude_scale': 0.5}}, 'num_classes': None} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"data_config: \\n {args.data_config} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GaussianNoise augmenter...\n",
      "Loading AmplitudeScale augmenter...\n"
     ]
    }
   ],
   "source": [
    "aug_1_name = args.data_config['augmentation'][0]\n",
    "aug_1_config = args.data_config['augmenter_config'].get(aug_1_name, {})\n",
    "aug_2_name = args.data_config['augmentation'][1]\n",
    "aug_2_config = args.data_config['augmenter_config'].get(aug_2_name, {})\n",
    "    \n",
    "augmenter_1 = init_augmenter(aug_1_name, aug_1_config)\n",
    "augmenter_2 = init_augmenter(aug_2_name, aug_2_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7680])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_modal_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7680])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It changes the shape of input: (B, seq) -> (B\n",
    "augmenter_1(raw_modal_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4, 7680])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([2, 7680])\n",
      "torch.Size([2, 30])\n",
      "torch.Size([2, 7680])\n",
      "torch.Size([2, 30])\n"
     ]
    }
   ],
   "source": [
    "for i , (raw_modal_1, raw_modal_2, subj, sleep) in enumerate(train_loader):\n",
    "    aug_1_modal_1 = augmenter_1(raw_modal_1)\n",
    "    aug_1_modal_2 = augmenter_1(raw_modal_2)\n",
    "    aug_2_modal_1 = augmenter_2(raw_modal_1)\n",
    "    aug_2_modal_2 = augmenter_2(raw_modal_2)\n",
    "    print(aug_1_modal_1.shape)\n",
    "    print(aug_1_modal_2.shape)\n",
    "    print(aug_2_modal_1.shape)\n",
    "    print(aug_2_modal_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'args' from '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/args.py'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from models.Backbone import DeepSense\n",
    "importlib.reload(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 16, 32]\n",
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    }
   ],
   "source": [
    "backbone_model = DeepSense(args)\n",
    "# dims = [1, 16, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7680])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_1_modal_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod1 cnn feature shape: torch.Size([2, 64, 280]) mod2 cnn feature shape: torch.Size([2, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([2, 17920]) mod2 rnn feature shape: torch.Size([2, 1920])\n",
      "mod1 cnn feature shape: torch.Size([2, 64, 280]) mod2 cnn feature shape: torch.Size([2, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([2, 17920]) mod2 rnn feature shape: torch.Size([2, 1920])\n"
     ]
    }
   ],
   "source": [
    "enc_mod_features_1 = backbone_model(aug_1_modal_1, aug_1_modal_2)\n",
    "enc_mod_features_2 = backbone_model(aug_2_modal_1, aug_2_modal_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ecg': tensor([[-0.1241,  0.0844, -0.1616, -0.0966,  0.0134, -0.0635, -0.0689,  0.0298,\n",
       "          -0.0890, -0.0152,  0.0303,  0.0630,  0.0059,  0.0851,  0.0213, -0.0429,\n",
       "           0.0028, -0.0003, -0.0404, -0.0465, -0.0803, -0.0385,  0.0005, -0.0915,\n",
       "          -0.1217,  0.0007, -0.0927,  0.0268,  0.1581,  0.0587,  0.0383,  0.0710,\n",
       "           0.1060,  0.0111,  0.1551, -0.0105,  0.0744,  0.1144,  0.1411,  0.1285,\n",
       "          -0.0831, -0.0204,  0.1206,  0.0123, -0.1289, -0.0165, -0.0503,  0.0140,\n",
       "          -0.0649,  0.0073, -0.0044, -0.0010,  0.0933,  0.0865, -0.0045,  0.0095,\n",
       "          -0.0885,  0.1091,  0.0897, -0.0932,  0.1393, -0.0972,  0.0073,  0.0770],\n",
       "         [-0.1030,  0.0623, -0.1084, -0.1130,  0.0150, -0.0558, -0.0655, -0.0128,\n",
       "          -0.1164, -0.0196,  0.0149,  0.0769,  0.0198,  0.0897,  0.0047, -0.0564,\n",
       "           0.0142,  0.0122, -0.0319, -0.0310, -0.0421, -0.0547, -0.0215, -0.1030,\n",
       "          -0.1109, -0.0423, -0.0943,  0.0035,  0.1794,  0.0762,  0.0308,  0.0461,\n",
       "           0.1291,  0.0374,  0.1331,  0.0189,  0.0544,  0.0702,  0.1187,  0.1214,\n",
       "          -0.0589,  0.0099,  0.1276, -0.0043, -0.1250, -0.0100, -0.0457,  0.0311,\n",
       "          -0.0662, -0.0112, -0.0400, -0.0299,  0.0571,  0.0885, -0.0015,  0.0131,\n",
       "          -0.0838,  0.1217,  0.0973, -0.1125,  0.1191, -0.1120,  0.0287,  0.0735]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'hr': tensor([[-0.0672, -0.0318,  0.0750, -0.0807, -0.0853,  0.0562,  0.0343, -0.0175,\n",
       "           0.0665,  0.0951, -0.0230,  0.0368, -0.0224, -0.1605,  0.0571,  0.1468,\n",
       "           0.0390,  0.1147,  0.0569,  0.0616, -0.1859,  0.0665,  0.0550,  0.0374,\n",
       "           0.1700,  0.0586,  0.0841, -0.0080, -0.1388,  0.1141,  0.0926,  0.0953,\n",
       "          -0.0035, -0.0807,  0.0964, -0.0733, -0.0446,  0.0031, -0.0975,  0.0970,\n",
       "           0.0418,  0.1144, -0.1208,  0.1122,  0.1470, -0.0870, -0.0253, -0.0913,\n",
       "          -0.0123,  0.0791, -0.0965, -0.0106,  0.1142, -0.0399,  0.0467, -0.1373,\n",
       "           0.1243,  0.0287, -0.0455,  0.0343, -0.0405,  0.0268, -0.0140, -0.0289],\n",
       "         [-0.0861, -0.0178,  0.1114, -0.1159, -0.0898,  0.0435,  0.0265, -0.0482,\n",
       "           0.0862,  0.0960, -0.0514,  0.0593,  0.0059, -0.1612,  0.0769,  0.1376,\n",
       "           0.0675,  0.0995,  0.0350,  0.0568, -0.1844,  0.0762,  0.0648,  0.0590,\n",
       "           0.1530,  0.0971,  0.0540,  0.0081, -0.1410,  0.0603,  0.1221,  0.0668,\n",
       "          -0.0309, -0.0990,  0.1052, -0.0615, -0.0690,  0.0237, -0.0692,  0.1015,\n",
       "           0.0029,  0.1050, -0.1314,  0.1382,  0.1441, -0.1016,  0.0048, -0.1089,\n",
       "          -0.0026,  0.0298, -0.0586,  0.0081,  0.1007, -0.0009, -0.0022, -0.1006,\n",
       "           0.0902,  0.0515, -0.0465,  0.0192, -0.0108,  0.0491, -0.0149, -0.0202]],\n",
       "        grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_mod_features_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "print(enc_mod_features_1['ecg'].shape)\n",
    "print(enc_mod_features_1['hr'].shape)\n",
    "print(enc_mod_features_2['ecg'].shape)\n",
    "print(enc_mod_features_2['hr'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal Model and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.FOCALModules import FOCAL\n",
    "from models.loss import FOCALLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 16, 32]\n",
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    }
   ],
   "source": [
    "backbone_model = DeepSense(args)\n",
    "focal_model = FOCAL(args, backbone_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod1 cnn feature shape: torch.Size([2, 64, 280]) mod2 cnn feature shape: torch.Size([2, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([2, 17920]) mod2 rnn feature shape: torch.Size([2, 1920])\n",
      "mod1 cnn feature shape: torch.Size([2, 64, 280]) mod2 cnn feature shape: torch.Size([2, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([2, 17920]) mod2 rnn feature shape: torch.Size([2, 1920])\n"
     ]
    }
   ],
   "source": [
    "enc_mod_features_1 = backbone_model(aug_1_modal_1, aug_1_modal_2)\n",
    "enc_mod_features_2 = backbone_model(aug_2_modal_1, aug_2_modal_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_2_modal_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod1 cnn feature shape: torch.Size([2, 64, 280]) mod2 cnn feature shape: torch.Size([2, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([2, 17920]) mod2 rnn feature shape: torch.Size([2, 1920])\n",
      "mod1 cnn feature shape: torch.Size([2, 64, 280]) mod2 cnn feature shape: torch.Size([2, 64, 30])\n",
      "mod1 rnn feature shape: torch.Size([2, 17920]) mod2 rnn feature shape: torch.Size([2, 1920])\n"
     ]
    }
   ],
   "source": [
    "# proj_head = True\n",
    "mod_features_1, mod_features_2 = focal_model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ecg': tensor([[ 5.9375e-02,  9.3346e-02, -5.9988e-02, -8.4248e-03,  2.9164e-02,\n",
      "          1.5648e-01, -1.0696e-01, -1.0277e-02,  4.7275e-02,  7.8495e-02,\n",
      "         -1.2658e-01,  1.3161e-01, -1.1611e-01, -1.4131e-01,  3.2412e-02,\n",
      "         -1.6903e-02, -5.4150e-02, -8.1899e-02,  9.5554e-05, -7.0945e-02,\n",
      "         -3.4094e-02,  4.6955e-02,  1.3384e-01,  1.2069e-01, -1.9006e-02,\n",
      "          6.3173e-03, -2.9440e-02,  1.1398e-01, -1.3466e-01, -1.6825e-02,\n",
      "         -1.0564e-01, -3.7422e-02,  5.6960e-02,  1.2609e-01,  2.6290e-03,\n",
      "         -1.2372e-01, -8.2032e-02,  1.5359e-02,  4.9434e-02, -6.0125e-02,\n",
      "          1.2757e-01,  7.0556e-02, -4.1827e-02,  2.9723e-02,  3.5180e-02,\n",
      "         -8.6259e-02, -4.0477e-02,  1.2347e-01,  1.0928e-01, -9.8584e-02,\n",
      "          2.9936e-02, -7.4971e-02,  1.5314e-02, -2.3503e-02, -1.1261e-01,\n",
      "         -4.0138e-02, -5.3316e-02,  1.4977e-01, -5.0693e-02,  5.8202e-02,\n",
      "          3.2393e-02, -6.2542e-02, -7.2803e-02, -4.1505e-02],\n",
      "        [ 7.6348e-02,  1.0286e-01, -6.8874e-02,  6.2372e-04,  3.6464e-02,\n",
      "          1.2050e-01, -1.0514e-01,  2.0198e-02,  4.2315e-02,  6.7864e-02,\n",
      "         -1.1750e-01,  1.3343e-01, -9.5717e-02, -1.4966e-01,  4.4631e-02,\n",
      "          5.5837e-03, -4.6657e-02, -7.3872e-02,  5.0193e-03, -6.9677e-02,\n",
      "         -3.9124e-02,  8.0323e-02,  1.1436e-01,  1.3879e-01, -8.5539e-03,\n",
      "          3.2881e-02, -2.7439e-02,  1.0896e-01, -9.9846e-02, -9.9904e-03,\n",
      "         -8.2761e-02, -4.4989e-02,  7.0909e-02,  1.1622e-01, -4.1091e-02,\n",
      "         -1.0190e-01, -8.1712e-02,  5.8127e-03,  4.1208e-02, -4.1795e-02,\n",
      "          1.0878e-01,  8.5492e-02, -4.3958e-02,  1.9395e-02,  6.3450e-03,\n",
      "         -8.5937e-02, -4.8696e-02,  1.1675e-01,  1.1770e-01, -9.4807e-02,\n",
      "          4.7989e-02, -9.4284e-02,  1.0573e-03, -1.5302e-02, -1.2566e-01,\n",
      "         -4.1048e-02, -8.0961e-02,  1.4294e-01, -4.3991e-02,  6.1693e-02,\n",
      "          1.4995e-02, -3.7228e-02, -5.7005e-02, -5.7643e-02]],\n",
      "       grad_fn=<AddmmBackward0>), 'hr': tensor([[ 0.0367,  0.0184,  0.0379,  0.1157,  0.0833,  0.0201,  0.0465, -0.0906,\n",
      "          0.0832, -0.0454,  0.0362, -0.0508,  0.1060,  0.1010, -0.0532,  0.0663,\n",
      "         -0.0042,  0.1232,  0.0954,  0.0603,  0.0153, -0.0449, -0.0577,  0.0065,\n",
      "         -0.0285, -0.1049,  0.0303,  0.0885,  0.0077, -0.1281,  0.0036, -0.0029,\n",
      "          0.0298,  0.0395, -0.1272, -0.0750,  0.0336, -0.1267, -0.0660, -0.0227,\n",
      "          0.0497,  0.0019,  0.0723, -0.1635, -0.1407, -0.0244,  0.1292, -0.0632,\n",
      "          0.0075,  0.0398, -0.0248, -0.0694, -0.0863, -0.0399,  0.0919, -0.0835,\n",
      "          0.0958, -0.0999, -0.1323,  0.1678,  0.0178, -0.0339,  0.0085,  0.0831],\n",
      "        [ 0.0363,  0.0114,  0.0180,  0.0799,  0.0269,  0.0395,  0.0243, -0.1145,\n",
      "          0.0542, -0.0199, -0.0408,  0.0130,  0.1356,  0.0914, -0.0262,  0.0680,\n",
      "          0.0190,  0.1045,  0.1373,  0.0956,  0.0314, -0.0761, -0.0881, -0.0304,\n",
      "         -0.0501, -0.0971,  0.0341,  0.0333, -0.0051, -0.0653,  0.0330, -0.0117,\n",
      "          0.0256,  0.0097, -0.1898, -0.0290,  0.0882, -0.1178, -0.1339, -0.0145,\n",
      "          0.1461, -0.0230,  0.0801, -0.1515, -0.1010, -0.0665,  0.0914,  0.0100,\n",
      "         -0.0230,  0.0048, -0.0201, -0.0378,  0.0185, -0.0495,  0.0581, -0.0364,\n",
      "          0.0854, -0.0675, -0.0680,  0.1468, -0.0440, -0.0173,  0.0740,  0.0223]],\n",
      "       grad_fn=<AddmmBackward0>)}\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "print(mod_features_1)\n",
    "print(mod_features_1['ecg'].shape)\n",
    "print(mod_features_1['hr'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ecg': tensor([[ 0.0464,  0.0861, -0.0591, -0.0155,  0.0219,  0.1569, -0.0983, -0.0196,\n",
      "          0.0447,  0.0720, -0.1324,  0.1300, -0.1093, -0.1331,  0.0272, -0.0097,\n",
      "         -0.0601, -0.0931, -0.0134, -0.0793, -0.0246,  0.0520,  0.1455,  0.1034,\n",
      "         -0.0395,  0.0099, -0.0339,  0.1163, -0.1373, -0.0222, -0.1134, -0.0464,\n",
      "          0.0550,  0.1360,  0.0093, -0.1391, -0.0766,  0.0118,  0.0462, -0.0572,\n",
      "          0.1289,  0.0721, -0.0339,  0.0215,  0.0445, -0.0769, -0.0340,  0.1261,\n",
      "          0.1134, -0.1085,  0.0282, -0.0626,  0.0163, -0.0250, -0.1092, -0.0398,\n",
      "         -0.0568,  0.1510, -0.0642,  0.0530,  0.0482, -0.0744, -0.0790, -0.0436],\n",
      "        [ 0.0769,  0.0968, -0.0629,  0.0019,  0.0328,  0.1255, -0.0936,  0.0129,\n",
      "          0.0284,  0.0567, -0.1017,  0.1567, -0.0982, -0.1598,  0.0486,  0.0070,\n",
      "         -0.0288, -0.0875,  0.0107, -0.0941, -0.0397,  0.0823,  0.1168,  0.1139,\n",
      "         -0.0090,  0.0207, -0.0179,  0.1101, -0.0954, -0.0127, -0.0626, -0.0294,\n",
      "          0.0515,  0.1166, -0.0334, -0.1169, -0.0792,  0.0038,  0.0560, -0.0510,\n",
      "          0.1155,  0.0798, -0.0408,  0.0280,  0.0149, -0.0780, -0.0298,  0.1221,\n",
      "          0.1146, -0.0930,  0.0288, -0.1083, -0.0209, -0.0085, -0.1174, -0.0341,\n",
      "         -0.0847,  0.1573, -0.0402,  0.0530,  0.0136, -0.0611, -0.0674, -0.0696]],\n",
      "       grad_fn=<AddmmBackward0>), 'hr': tensor([[ 0.0455,  0.0429,  0.0246,  0.1219,  0.0561,  0.0407,  0.0513, -0.0781,\n",
      "          0.0905, -0.0428,  0.0098, -0.0410,  0.1216,  0.0677, -0.0260,  0.0669,\n",
      "         -0.0157,  0.1263,  0.0962,  0.1023,  0.0172, -0.0627, -0.0744,  0.0079,\n",
      "         -0.0281, -0.1108,  0.0431,  0.0493,  0.0030, -0.0956,  0.0073, -0.0374,\n",
      "          0.0151,  0.0324, -0.1456, -0.0455,  0.0406, -0.1032, -0.1198, -0.0233,\n",
      "          0.0897, -0.0052,  0.0796, -0.1542, -0.1362, -0.0562,  0.1231, -0.0209,\n",
      "         -0.0333,  0.0141, -0.0127, -0.0858, -0.0451, -0.0333,  0.0598, -0.0667,\n",
      "          0.0839, -0.0492, -0.1116,  0.1434, -0.0219, -0.0245,  0.0309,  0.0656],\n",
      "        [ 0.0123,  0.0149,  0.0349,  0.0403,  0.0387,  0.0285, -0.0082, -0.1283,\n",
      "          0.0517, -0.0157, -0.0038,  0.0284,  0.1277,  0.0798, -0.0495,  0.0505,\n",
      "          0.0284,  0.1416,  0.1518,  0.0972,  0.0342, -0.0610, -0.0373, -0.0239,\n",
      "         -0.0690, -0.1133,  0.0621,  0.0418, -0.0002, -0.0615, -0.0140,  0.0355,\n",
      "          0.0163,  0.0242, -0.1625, -0.0150,  0.0924, -0.1036, -0.1045, -0.0135,\n",
      "          0.1158,  0.0101,  0.0714, -0.1442, -0.1180, -0.0694,  0.0751, -0.0112,\n",
      "         -0.0210,  0.0404, -0.0025, -0.0413,  0.0041, -0.0208,  0.0713, -0.0434,\n",
      "          0.1029, -0.1097, -0.0720,  0.1508, -0.0484, -0.0359,  0.0560,  0.0767]],\n",
      "       grad_fn=<AddmmBackward0>)}\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "print(mod_features_2)\n",
    "print(mod_features_2['ecg'].shape)\n",
    "print(mod_features_2['hr'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FOCALLoss(\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (similarity_f): CosineSimilarity()\n",
       "  (orthonal_loss_f): CosineEmbeddingLoss()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss_fn = FOCALLoss(args)\n",
    "focal_loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MESAPairDataset(file_path=args.base_config['train_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            batch_size=4,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 16, 32]\n",
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data8/jungmin/anaconda3/envs/patchtst/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "if str(list(args.focal_config[\"backbone\"].keys())[0]) == \"DeepSense\":\n",
    "    backbone = DeepSense(args).to(args.focal_config[\"device\"])\n",
    "\n",
    "model = FOCAL(args, backbone)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.focal_config[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "advs_model = AdversarialModel(args).to(args.subj_invariant_config[\"device\"])\n",
    "advs_optimizer = torch.optim.Adam(advs_model.parameters(), lr=args.subj_invariant_config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SA_Focal(train_loader, val_loader, model, advs_model, \n",
    "                   optimizer, advs_optimizer, focal_loss_fn, device, args):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = args.trainer_config\n",
    "\n",
    "aug_1_name = args.data_config['augmentation'][0]\n",
    "aug_1_config = args.data_config['augmenter_config'].get(aug_1_name, {})\n",
    "aug_2_name = args.data_config['augmentation'][1]\n",
    "aug_2_config = args.data_config['augmenter_config'].get(aug_2_name, {})\n",
    "\n",
    "aug_1 = init_augmenter(aug_1_name, aug_1_config)\n",
    "aug_2 = init_augmenter(aug_2_name, aug_2_config)\n",
    "\n",
    "model.train()\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for ep in range(trainer_config['epochs']):\n",
    "    running_advs_train_loss = 0\n",
    "    focal_train_loss = 0\n",
    "    \n",
    "    for raw_modal_1, raw_modal_2, subj_label in train_loader:\n",
    "        raw_modal_1, raw_modal_2, subj_label = raw_modal_1.to(device), raw_modal_2.to(device), subj_label.to(device) # [B, 30], [B, 30*256], [B, 1]\n",
    "        \n",
    "        aug_1_modal_1 = aug_1(raw_modal_1)\n",
    "        aug_2_modal_1 = aug_2(raw_modal_1)\n",
    "        \n",
    "        aug_1_modal_2 = aug_1(raw_modal_2)\n",
    "        aug_2_modal_2 = aug_2(raw_modal_2)\n",
    "        \n",
    "        # For updating the only advs_model (classifier)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in advs_model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        advs_optimizer.zero_grad()\n",
    "        \n",
    "        # Using Encoder for classify the subject\n",
    "        enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
    "        # enc_feature1 -> dict // (example) enc_feature1['ecg'] & enc_feature1['hr'] from Augmentation 1\n",
    "        # enc_feature2 -> dict // (example) enc_feature2['ecg'] & enc_feature2['hr'] from Augmentation 2\n",
    "        \n",
    "        \n",
    "        # Predict the subject\n",
    "        subj_preds = advs_model(enc_modal_1, enc_modal_2) \n",
    "        # or subj_preds = advs_model(enc_feature_1['ecg'], enc_feature_1['hr], enc_feature_2['ecg'], enc_feature_2['hr'])\n",
    "        \n",
    "        advs_loss = advs_model.forward_adversarial_loss(subj_preds, subj_label)\n",
    "        \n",
    "        # To-do for calculating the accuracy\n",
    "        # num_adversary_correct_train_preds += adversarial_loss_fn.get_number_of_correct_preds(x_t1_initial_subject_preds, y)\n",
    "        # total_num_adversary_train_preds += len(x_t1_initial_subject_preds)\n",
    "        \n",
    "        advs_loss.backward()\n",
    "        advs_optimizer.step()\n",
    "        \n",
    "        running_advs_train_loss += advs_loss.item()\n",
    "        \n",
    "        # For efficient memory management\n",
    "        del enc_modal_1, enc_modal_2, subj_preds, advs_loss\n",
    "        \n",
    "        # For updating the only Focal model (SSL model)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in advs_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x1_represent, x2_represent = model(raw_modal_1, raw_modal_2)\n",
    "        \n",
    "        x1_embd, x2_embd = model.encoder(raw_modal_1, raw_modal_2)\n",
    "        subj_pred = advs_model(x1_embd, x2_embd)\n",
    "        subj_invariant_loss = advs_model.forward_subject_invariance_loss(subj_pred, subj_labels, args.adversarial_weighting_factor) # DONE -> add subject_invariant function loss\n",
    "        \n",
    "        focal_loss = focal_loss_fn(x1_represent, x2_represent, subj_invariant_loss) # To-Do -> add regularization term about subject invariant\n",
    "        focal_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        focal_train_loss += focal_loss.item()\n",
    "        \n",
    "        # For efficient memory management\n",
    "        del x1_represent, x2_represent, x1_embd, x2_embd, subj_pred, focal_loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    if ep % args.log_interval == 0:\n",
    "        print(f\"Epoch {ep} - Adversarial Loss: {running_advs_train_loss/ len(train_loader)}, \\\n",
    "            Focal Loss: {focal_train_loss/ len(train_loader)}\")\n",
    "        \n",
    "        if ep % args.val_interval == 0:\n",
    "            model.eval()\n",
    "            advs_model.eval()\n",
    "            \n",
    "            advs_val_loss = 0\n",
    "            focal_val_loss = 0\n",
    "            \n",
    "            for raw_modal_1, raw_modal_2, subj in val_loader:\n",
    "                raw_modal_1, raw_modal_2, subj = raw_modal_1.to(device), raw_modal_2.to(device), subj.to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    x1_represent, x2_represent = model(raw_modal_1, raw_modal_2)\n",
    "                    x1_embd, x2_embd = model.encoder(raw_modal_1), model.encoder(raw_modal_2)\n",
    "                    subj_pred = advs_model(x1_embd, x2_embd) # output -> sigmoid value\n",
    "                    \n",
    "                    advs_loss = advs_model.loss_fcn(subj_pred, subj)\n",
    "                    focal_loss = focal_loss_fn(x1_represent, x2_represent, subj_pred, subj)\n",
    "                    \n",
    "                    advs_val_loss += advs_loss.item()\n",
    "                    focal_val_loss += focal_loss.item()\n",
    "                    \n",
    "                    # For efficient memory management\n",
    "                    del x1_represent, x2_represent, x1_embd, x2_embd, subj_pred, focal_loss\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            print(\"-----\"*10)\n",
    "            print(f\"(Validation) Epoch{ep} - Adversarial Loss: {advs_val_loss/ len(val_loader)}, \\\n",
    "                Focal Loss: {focal_val_loss/ len(val_loader)}\")                    \n",
    "                            \n",
    "            if focal_val_loss < best_val_loss:\n",
    "                best_val_loss = focal_val_loss\n",
    "                \n",
    "                # To-do -> fix the save model format\n",
    "                # torch.save(model.state_dict(), os.path.join(args.save_dir, 'focal_model.pth'))\n",
    "                # torch.save(advs_model.state_dict(), os.path.join(args.save_dir, 'advs_model.pth'))\n",
    "                print(\"************* Model Saved *************\")\n",
    "            print(\"-----\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 64-bit ('patchtst')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e0a0ed8c9d253a0f21f5456fde53cd73d7f33b56362cce5f62479a7d0aeeb66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
