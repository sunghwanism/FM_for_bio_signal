{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data8/jungmin/anaconda3/envs/patchtst/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "import args\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from models.AdversarialModel import AdversarialModel\n",
    "from models.FOCALModules import FOCAL\n",
    "from models.loss import FOCALLoss\n",
    "from data.EfficientDataset import MESAPairDataset\n",
    "from data.Augmentaion import init_augmenter\n",
    "\n",
    "import datetime\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_config: \n",
      " {'train_data_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/dataset/pair_small', 'valid_data_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/dataset/pair_small_valid', 'test_data_dir': '/NFS/Users/moonsh/data/mesa/preproc/pair_test', 'modalities': ['ecg', 'hr'], 'label_key': 'stage', 'subject_key': 'subject_idx', 'train_num_subjects': 100, 'test_num_subjects': 50, 'device': device(type='cpu'), 'log_save_dir': '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/logs'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"base_config: \\n {args.base_config}\")\n",
    "# print(f\"focal_config: \\n {args.focal_config} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MESAPairDataset(file_path=args.base_config['train_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            batch_size=14,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "0\n",
      "torch.Size([14, 7680])\n",
      "torch.Size([14, 30])\n",
      "torch.Size([14])\n",
      "torch.Size([14])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__())\n",
    "\n",
    "for i , (raw_modal_1, raw_modal_2, subj, sleep) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(raw_modal_1.shape)\n",
    "    print(raw_modal_2.shape)\n",
    "    print(subj.shape)\n",
    "    print(sleep.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_config: \n",
      " {'modalities': ['ecg', 'hr'], 'label_key': 'stage', 'augmentation': ['GaussianNoise', 'AmplitudeScale'], 'augmenter_config': {'GaussianNoise': {'max_noise_std': 0.1}, 'AmplitudeScale': {'amplitude_scale': 0.5}}, 'num_classes': None} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"data_config: \\n {args.data_config} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GaussianNoise augmenter...\n",
      "Loading AmplitudeScale augmenter...\n"
     ]
    }
   ],
   "source": [
    "aug_1_name = args.data_config['augmentation'][0]\n",
    "aug_1_config = args.data_config['augmenter_config'].get(aug_1_name, {})\n",
    "aug_2_name = args.data_config['augmentation'][1]\n",
    "aug_2_config = args.data_config['augmenter_config'].get(aug_2_name, {})\n",
    "    \n",
    "augmenter_1 = init_augmenter(aug_1_name, aug_1_config)\n",
    "augmenter_2 = init_augmenter(aug_2_name, aug_2_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 7680])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_modal_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 7680])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It changes the shape of input: (B, seq) -> (B\n",
    "augmenter_1(raw_modal_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 7680])\n",
      "torch.Size([14, 30])\n",
      "torch.Size([14, 7680])\n",
      "torch.Size([14, 30])\n",
      "torch.Size([2, 7680])\n",
      "torch.Size([2, 30])\n",
      "torch.Size([2, 7680])\n",
      "torch.Size([2, 30])\n"
     ]
    }
   ],
   "source": [
    "for i , (raw_modal_1, raw_modal_2, subj, sleep) in enumerate(train_loader):\n",
    "    aug_1_modal_1 = augmenter_1(raw_modal_1)\n",
    "    aug_1_modal_2 = augmenter_1(raw_modal_2)\n",
    "    aug_2_modal_1 = augmenter_2(raw_modal_1)\n",
    "    aug_2_modal_2 = augmenter_2(raw_modal_2)\n",
    "    print(aug_1_modal_1.shape)\n",
    "    print(aug_1_modal_2.shape)\n",
    "    print(aug_2_modal_1.shape)\n",
    "    print(aug_2_modal_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'args' from '/data8/jungmin/uot_class/MIE1517_DL/FM_for_bio_signal/src/foundation/args.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from models.Backbone import DeepSense\n",
    "importlib.reload(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    }
   ],
   "source": [
    "backbone_model = DeepSense(args)\n",
    "# dims = [1, 16, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7680])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_1_modal_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_mod_features_1 = backbone_model(aug_1_modal_1, aug_1_modal_2)\n",
    "enc_mod_features_2 = backbone_model(aug_2_modal_1, aug_2_modal_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ecg': tensor([[-0.1035,  0.0884,  0.1163, -0.1078,  0.0254,  0.0194, -0.0934,  0.1192,\n",
       "          -0.1386, -0.0968,  0.0526,  0.0269, -0.0050, -0.0701, -0.0083,  0.0279,\n",
       "          -0.0439, -0.0279,  0.0330, -0.0225, -0.0716, -0.0077, -0.1226,  0.0619,\n",
       "          -0.0277,  0.0469,  0.1183,  0.1174, -0.0727, -0.0186, -0.0106, -0.0919,\n",
       "          -0.0322,  0.0380,  0.0356, -0.0224, -0.0362, -0.0271,  0.0232,  0.0389,\n",
       "           0.0825, -0.0928, -0.0551, -0.1667, -0.0355,  0.1242,  0.1216,  0.0337,\n",
       "           0.1263, -0.0619, -0.0187,  0.0838, -0.1013,  0.0774, -0.0534,  0.0082,\n",
       "          -0.0013, -0.0337,  0.0511,  0.0570,  0.1249, -0.0481,  0.1119, -0.0105],\n",
       "         [-0.1044,  0.0841,  0.1308, -0.1162,  0.0123, -0.0005, -0.0942,  0.0941,\n",
       "          -0.1394, -0.0521,  0.0703,  0.0375,  0.0112, -0.0447, -0.0526, -0.0071,\n",
       "          -0.0235, -0.0259,  0.0203, -0.0057, -0.0683, -0.0222, -0.1220,  0.0665,\n",
       "          -0.0193,  0.0479,  0.1070,  0.1173, -0.0480, -0.0235, -0.0058, -0.0822,\n",
       "          -0.0325,  0.0238,  0.0151, -0.0304, -0.0278, -0.0371,  0.0570,  0.0226,\n",
       "           0.0719, -0.0810, -0.0425, -0.1634, -0.0814,  0.1042,  0.1176,  0.0085,\n",
       "           0.1217, -0.0536, -0.0254,  0.0860, -0.1023,  0.0774, -0.0178,  0.0482,\n",
       "           0.0212, -0.0346,  0.0333,  0.0318,  0.1073, -0.0922,  0.0924, -0.0137]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'hr': tensor([[-0.0825, -0.0607, -0.0906, -0.0500,  0.0060, -0.1112, -0.0165,  0.0770,\n",
       "          -0.0088,  0.0613,  0.0308, -0.1016,  0.1406,  0.1289,  0.1189,  0.0081,\n",
       "           0.0430,  0.0175, -0.1673, -0.0466, -0.0036,  0.0671,  0.0392, -0.0325,\n",
       "          -0.1144,  0.1405,  0.0565,  0.0586,  0.0310,  0.0280, -0.0985, -0.0155,\n",
       "           0.0485,  0.0083,  0.0909, -0.0396, -0.0210,  0.0461, -0.0696,  0.0251,\n",
       "           0.0606,  0.0773,  0.1037,  0.0610, -0.0393,  0.0429, -0.0769, -0.0555,\n",
       "           0.0641, -0.1595, -0.1301, -0.0131, -0.0379, -0.0491,  0.0561,  0.0455,\n",
       "          -0.0406, -0.0047,  0.1640, -0.0346,  0.0037, -0.0151,  0.0902,  0.0806],\n",
       "         [-0.0843, -0.0725, -0.0435, -0.1207,  0.0618, -0.1823, -0.0438,  0.0267,\n",
       "          -0.0066,  0.0264,  0.0517, -0.0651,  0.1373,  0.0629,  0.1472, -0.0641,\n",
       "           0.0553,  0.0233, -0.1224, -0.0386,  0.0226,  0.0540,  0.0275, -0.0157,\n",
       "          -0.1219,  0.1024,  0.0503,  0.0777,  0.0025,  0.0091, -0.0933, -0.0421,\n",
       "           0.0173,  0.0070,  0.0926, -0.0487,  0.0223,  0.0049, -0.0737,  0.0535,\n",
       "           0.0627,  0.0944,  0.0890,  0.0866, -0.0631,  0.0771, -0.1064, -0.1030,\n",
       "           0.0871, -0.1597, -0.0840, -0.0275, -0.0440, -0.0744,  0.0457,  0.1272,\n",
       "          -0.0049, -0.0158,  0.1670, -0.0612, -0.0351, -0.0040,  0.0400,  0.0944]],\n",
       "        grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_mod_features_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "print(enc_mod_features_1['ecg'].shape)\n",
    "print(enc_mod_features_1['hr'].shape)\n",
    "print(enc_mod_features_2['ecg'].shape)\n",
    "print(enc_mod_features_2['hr'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal Model and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.FOCALModules import FOCAL\n",
    "from models.loss import FOCALLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    }
   ],
   "source": [
    "backbone_model = DeepSense(args)\n",
    "focal_model = FOCAL(args, backbone_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_mod_features_1 = backbone_model(aug_1_modal_1, aug_1_modal_2)\n",
    "enc_mod_features_2 = backbone_model(aug_2_modal_1, aug_2_modal_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_2_modal_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proj_head = True\n",
    "mod_features_1, mod_features_2 = focal_model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mod_features_1)\n",
    "# print(mod_features_1['ecg'].shape)\n",
    "# print(mod_features_1['hr'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MESAPairDataset(file_path=args.base_config['train_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            batch_size=16,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)\n",
    "\n",
    "valid_dataset = MESAPairDataset(file_path=args.base_config['valid_data_dir'],\n",
    "                                    modalities=args.base_config['modalities'],\n",
    "                                    subject_idx=args.base_config['subject_key'],\n",
    "                                    stage=args.base_config['label_key'])\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                                            batch_size=16,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg extractor is initialized.\n",
      "hr extractor is initialized.\n",
      "ecg recurrent layer is initialized.\n",
      "hr recurrent layer is initialized.\n",
      "** Finished Initializing DeepSense Backbone **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data8/jungmin/anaconda3/envs/patchtst/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "if str(list(args.focal_config[\"backbone\"].keys())[0]) == \"DeepSense\":\n",
    "    backbone = DeepSense(args).to(args.focal_config[\"device\"])\n",
    "\n",
    "model = FOCAL(args, backbone)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.focal_config[\"lr\"])\n",
    "focal_loss_fn = FOCALLoss(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "advs_model = AdversarialModel(args).to(args.subj_invariant_config[\"device\"])\n",
    "advs_optimizer = torch.optim.Adam(advs_model.parameters(), lr=args.subj_invariant_config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GaussianNoise augmenter...\n",
      "Loading AmplitudeScale augmenter...\n",
      "Epoch 0 - Adversarial Loss: 37.47340774536133,             Focal Loss: 0.09777083992958069\n",
      "--------------------------------------------------\n",
      "(Validation) Epoch0 - Adversarial Loss: 37.036746978759766,                 Focal Loss: 0.03758801147341728\n",
      "************* Model Saved *************\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer_config = args.trainer_config\n",
    "\n",
    "aug_1_name = args.data_config['augmentation'][0]\n",
    "aug_1_config = args.data_config['augmenter_config'].get(aug_1_name, {})\n",
    "aug_2_name = args.data_config['augmentation'][1]\n",
    "aug_2_config = args.data_config['augmenter_config'].get(aug_2_name, {})\n",
    "\n",
    "aug_1 = init_augmenter(aug_1_name, aug_1_config)\n",
    "aug_2 = init_augmenter(aug_2_name, aug_2_config)\n",
    "\n",
    "model.train()\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for ep in range(trainer_config['epochs']):\n",
    "    running_advs_train_loss = 0\n",
    "    focal_train_loss = 0\n",
    "    \n",
    "    for raw_modal_1, raw_modal_2, subj_label, sleep_label in train_loader:\n",
    "        raw_modal_1, raw_modal_2, subj_label, sleep_label = raw_modal_1.to(device), raw_modal_2.to(device), subj_label.to(device), sleep_label.to(device) # [B, 30], [B, 30*256], [B, 1]\n",
    "        \n",
    "        aug_1_modal_1 = aug_1(raw_modal_1)\n",
    "        aug_2_modal_1 = aug_2(raw_modal_1)\n",
    "        \n",
    "        aug_1_modal_2 = aug_1(raw_modal_2)\n",
    "        aug_2_modal_2 = aug_2(raw_modal_2)\n",
    "        \n",
    "        # For updating the only advs_model (classifier)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in advs_model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        advs_optimizer.zero_grad()\n",
    "        \n",
    "        # Using Encoder for classify the subject\n",
    "        enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
    "        # enc_feature1 -> dict // (example) enc_feature1['ecg'] & enc_feature1['hr'] from Augmentation 1\n",
    "        # enc_feature2 -> dict // (example) enc_feature2['ecg'] & enc_feature2['hr'] from Augmentation 2\n",
    "        \n",
    "        \n",
    "        # Predict the subject\n",
    "        subj_pred = advs_model(enc_feature_1, enc_feature_2)     \n",
    "        advs_loss = advs_model.forward_adversarial_loss(subj_pred, subj_label)\n",
    "        \n",
    "        # To-do for calculating the accuracy\n",
    "        # num_adversary_correct_train_preds += adversarial_loss_fn.get_number_of_correct_preds(x_t1_initial_subject_preds, y)\n",
    "        # total_num_adversary_train_preds += len(x_t1_initial_subject_preds)\n",
    "        \n",
    "        advs_loss.backward()\n",
    "        advs_optimizer.step()\n",
    "        \n",
    "        running_advs_train_loss += advs_loss.item()\n",
    "        \n",
    "        # For efficient memory management\n",
    "        del enc_feature_1, enc_feature_2, subj_pred, advs_loss\n",
    "        \n",
    "        # For updating the only Focal model (SSL model)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in advs_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
    "        \n",
    "        subj_pred = advs_model(enc_feature_1, enc_feature_2) \n",
    "        subj_invariant_loss = advs_model.forward_subject_invariance_loss(subj_pred, subj_label, args.subj_invariant_config['adversarial_weighting_factor']) # DONE -> add subject_invariant function loss\n",
    "        \n",
    "        focal_loss = focal_loss_fn(enc_feature_1, enc_feature_2, subj_invariant_loss) # To-Do -> add regularization term about subject invariant\n",
    "        focal_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        focal_train_loss += focal_loss.item()\n",
    "        \n",
    "        # For efficient memory management\n",
    "        del enc_feature_1, enc_feature_2, subj_pred, focal_loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    if ep % trainer_config['log_interval'] == 0:\n",
    "        print(f\"Epoch {ep} - Adversarial Loss: {running_advs_train_loss/ len(train_loader)}, \\\n",
    "            Focal Loss: {focal_train_loss/ len(train_loader)}\")\n",
    "        \n",
    "        if ep % trainer_config['val_interval'] == 0:\n",
    "            model.eval()\n",
    "            advs_model.eval()\n",
    "            \n",
    "            advs_val_loss = 0\n",
    "            focal_val_loss = 0\n",
    "            \n",
    "            for raw_modal_1, raw_modal_2, subj_label, sleep_label in valid_loader:\n",
    "                raw_modal_1, raw_modal_2, subj_label, sleep_label = raw_modal_1.to(device), raw_modal_2.to(device), subj_label.to(device), sleep_label.to(device)\n",
    "                \n",
    "                aug_1_modal_1, aug_2_modal_1  = aug_1(raw_modal_1), aug_2(raw_modal_1)\n",
    "                aug_1_modal_2, aug_2_modal_2  = aug_1(raw_modal_2), aug_2(raw_modal_2)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # x1_represent, x2_represent = model(raw_modal_1, raw_modal_2)\n",
    "                    enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
    "                    subj_pred = advs_model(enc_feature_1, enc_feature_2)\n",
    "                    \n",
    "                    advs_loss = advs_model.forward_adversarial_loss(subj_pred, subj_label)\n",
    "                    focal_loss = focal_loss_fn(enc_feature_1, enc_feature_2, subj_invariant_loss) # To-Do -> add regularization term about subject invariant\n",
    "                    \n",
    "                    advs_val_loss += advs_loss.item()\n",
    "                    focal_val_loss += focal_loss.item()\n",
    "                    \n",
    "                    # For efficient memory management\n",
    "                    del enc_feature_1, enc_feature_2, subj_pred, focal_loss, advs_loss\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            print(\"-----\"*10)\n",
    "            print(f\"(Validation) Epoch{ep} - Adversarial Loss: {advs_val_loss/ len(valid_loader)}, \\\n",
    "                Focal Loss: {focal_val_loss/ len(valid_loader)}\")                    \n",
    "                            \n",
    "            if focal_val_loss < best_val_loss:\n",
    "                best_val_loss = focal_val_loss\n",
    "                \n",
    "                # To-do -> fix the save model format\n",
    "                # torch.save(model.state_dict(), os.path.join(args.save_dir, 'focal_model.pth'))\n",
    "                # torch.save(advs_model.state_dict(), os.path.join(args.save_dir, 'advs_model.pth'))\n",
    "                print(\"************* Model Saved *************\")\n",
    "            print(\"-----\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 64-bit ('patchtst')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e0a0ed8c9d253a0f21f5456fde53cd73d7f33b56362cce5f62479a7d0aeeb66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
