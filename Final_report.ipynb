{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drIdzT9D0Mvk"
      },
      "source": [
        "# Final Report\n",
        "\n",
        "Developed by\n",
        "\n",
        "- Sunghwan Moon, Jungmin Kim, Seunghyeon Park, Bojing Gui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (1) Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our problem setting is characterized by three main issues:\n",
        "\n",
        "*  Lack of label in sleep stage classification label.\n",
        "*  Existence of multimodalities in bio-signal data.\n",
        "*  Downstream task for personalized model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (2) Related Work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Project Overview\n",
        "\n",
        "- to-do: explain overview of our project\n",
        "\n",
        "We will attempt to develop the foundation model using bio-signal (ECG, Heart Rate) [1] for applying the sleep stage classification from the personal data of edge devices [2], such as Apple Watch or Fit-bit. It is hard to get high performance by only using personally own data from edge devices and to train the model, as limitation of the amount of data for train and low hardware resources of edge devices. We expect that the foundation model generates informative representative feature from large bio-signal dataset, and it can improve the downstream task in the restricted environment that people cannot share bio-signal data to others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Image(filename='../asset/overall_task_architecture.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Foundation Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to-do: loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### (1) Multi Modal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### (2) Subject Invariant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Downsteram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experienment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y0XIJhA0Mvo"
      },
      "source": [
        "### 1. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F33sN0c0Mvo",
        "outputId": "18316e10-54ad-4a6c-8e31-68a979f18643"
      },
      "outputs": [],
      "source": [
        "# to-do: change the data table\n",
        "Image(filename='../asset/dataset_description_table.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to-do: add data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### (1) Training Foundation Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### (2) Training Basemodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIcTjzhq0Mvo"
      },
      "source": [
        "### 2. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwv_NPlB0Mvp"
      },
      "source": [
        "#### 2-1. Foundation Model\n",
        "We present the issue that the personalized model in the edge device lacks sufficient data to learn each personalized model. We intend to suggest a solution to the problem by creating a foundation model from a huge amount of bio-signal data that has been made public and improving the performance of the personalized model in the edge device using the foundation model. We plan to develop a foundation model that takes into account each of the 3 issues we established for the problem setting .\n",
        "\n",
        "\n",
        "Our proposed strategy involves using a huge quantity of publically available bio-signal data to build a foundation model, which can then be used to improve the performance of personalized models on edge devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FI_qmu00Mvp"
      },
      "source": [
        "#### Problem setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT2HsDi40Mvp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNr_3htB0Mvq"
      },
      "source": [
        "#### Why this Foundation model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqQDnIM80Mvq"
      },
      "source": [
        "To address these 3 primary issues, we aim to incorporate 3 characteristics into our foundation model:\n",
        "\n",
        "**1. Self-supervised constrastive learning**\n",
        "\n",
        "First, we assumed that the label had existed in MESA, which is currently open to the public, but bio-signal labels are typically expensive and obtained manually by professionals. Given this, we plan to employ the self-supervised method of continuous learning, which does not require a label, as the foundation model's learning approach.\n",
        "\n",
        "**2. Considering multimodalities**\n",
        "\n",
        "Second, we want to create a foundation model that can account for multimodality characteristics because we deal with biosignals from two separate modalities: ECG (256Hz) and heart rate (1Hz). The framework(FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Lantent Space, 2023) we referred to can fully utilize multimodal signal information by separating the shared features shared by the two modalities from the private features that each modality has on the latent space.\n",
        "\n",
        "**3. Subject-aware learning**\n",
        "\n",
        "Finally, because our foundation model is 'personalized', in which the subject is employed in each different models, it should be possible to avoid domain shift caused by inter-subject variability. Therefore, our foundation model will incorporate subject-invariance into the continuous learning framework so that it can learn domain-invariant characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejTuFn0OJgb8"
      },
      "source": [
        "#### Existing structure to refer to"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhOwi-zx0Mvq"
      },
      "source": [
        "- FOCAL Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IigiX37w0Mvq",
        "outputId": "745693f7-5a56-4d5a-d42b-83c48713b774"
      },
      "outputs": [],
      "source": [
        "Image(filename='../asset/FOCAL_figure.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MphqrUBzIi9O"
      },
      "source": [
        "The FOCAL Framework, proposed in [FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space, 2023], is a self-supervised multimodal contrastive framework. And, in this case, not only is shared information between sensory modalities extracted, but exclusive modality information is not explicitly considered, which could be essential to understanding the underlying sensing physics. We intend to integrate subject-aware learning to this strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvvM3w5X0Mvq"
      },
      "source": [
        "#### 2-2. Classification model for Downstream task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The architecture for the downstream classifier is shown below. This will be used as the baseline comparison for our model. The neural network will take in the two inputs, heart rate and ECG. The ECG will be processed through convolutional layers and a LSTM before the inputs are concatenated and fully connected layers are used for classification. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI3R_9rPIFFL"
      },
      "outputs": [],
      "source": [
        "Image(filename = '../asset/downstream model.png', width=800, height=400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specifically, the CNN+LSTM portion is based off of TinySleepNet [Github](https://github.com/akaraspt/tinysleepnet). The architecture is shown below: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Image(filename = '../asset/tinysleepnet.png', width=600, height=800)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blDUOAs40Mvq"
      },
      "source": [
        "### 3. Progress Result\n",
        "- We generate the private Github repository for working together\n",
        "    \n",
        "    URL: https://github.com/sunghwanism/FM_for_bio_signal\n",
        "\n",
        "- All of code from our project is restored to github\n",
        "\n",
        "- As it is hard to sharing all of our code, we want to share our github repo for grading\n",
        "\n",
        "\n",
        "(If you need to acces for grading, please say your github email. We will change our repo as public after finishing our project)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3-1 Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) MESA data\n",
        "- We needs to segment the bio signal (heart rate and ecg) to 30 seconds (1 epoch) because sleep stage is decided from 30 seconds data in down stream\n",
        "- We select validation epoch without any problems, such as disconnection error, mis-collection time between bio signals.\n",
        "\n",
        "- Heart Rate\n",
        "    - After selecting validation epoch, the heart rate was **interpolated** to have a value for every 1 second, **smoothed** and **filtered** to amplify periods of high change by convoloving with a diffrence of Gaussian filter and **normalized** by dividing by the 90th percentile in the absolute diffrence between each heart rate measurement and the mean heart rate over the sleep periods\n",
        "- Electrocardiogram (ECG=EKG)\n",
        "    - After selecting validation epoch, the ECG was **smoothed** and **filtered** by Gaussian filter for denoising the ECG\n",
        "\n",
        "\n",
        "2) Apple watch data\n",
        "- We collected Apple watch data which contains heart rate and acceleration\n",
        "- We plan to preprocess it following the previous study titled \"**Sleep stage prediction with raw acceleration and photoplethysmography heart rate data derived from a consumer wearable device** (Olivia, et al.)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 3-1-1 Preprocessing Result (MESA)\n",
        "\n",
        "We use **1,405 subjects** among 2,056 after quality contorl\n",
        "\n",
        "Total segementation (1 epoch during 30s) is **1,487,316**, which is **same number of label (sleep stage)**\n",
        "\n",
        "Following images show the result of preprocessing on **subject0001** from MESA dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Actiography\n",
        "Image(filename='../asset/actiography_0001.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ECG Image\n",
        "Image(filename=\"../asset/ecg_0001.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heart Rate\n",
        "Image(\"../asset/heartrate_0001.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Actiography\n",
        "Image(\"../asset/actiography_0001.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PSG Status (Sleep Stages)\n",
        "Image(\"../asset/psg_0001.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 3-1-2 Preprocessing Result (Apple watch, Not complete)\n",
        "- We collect real Apple watch dataset which was collected during 7 days\n",
        "\n",
        "- We have apple watch dataset, totally 31\n",
        "\n",
        "- We complete to code for preprocessing Apple watch data but not finishing preprocessing yet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../preproc/outputs/applewatch_public/c1_data.csv\")\n",
        "\n",
        "fig, ax = plt.subplots(3, 1, figsize=(12, 5))\n",
        "\n",
        "ax[0].plot(df[\"heart_rate\"])\n",
        "ax[0].set_title(\"Heart Rate\")\n",
        "\n",
        "ax[1].plot(df[\"x_move\"])\n",
        "ax[1].plot(df[\"y_move\"])\n",
        "ax[1].plot(df[\"z_move\"])\n",
        "ax[1].legend([\"X\", \"Y\", \"Z\"])\n",
        "ax[1].set_title(\"Acceleration\")\n",
        "\n",
        "ax[2].plot(df[\"psg_status\"])\n",
        "ax[2].set_title(\"PSG Status (Sleep Stages)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3L5h0k10Mvq"
      },
      "source": [
        "#### 3-2 DownStream Modeling for individual model\n",
        "\n",
        "- For confirming the effectiveness of proposed foundation model, we will train a model individually using conventional base line model in sleep stage classification.\n",
        "\n",
        "- We split train/validation/test data in each subject\n",
        "\n",
        "- We find the best model using validation data\n",
        "\n",
        "- Finally, we test the model performance using test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### TinySleepNet\n",
        "\n",
        "- During progress period, we run the model with sample dataset (partial MESA data) for checking running error\n",
        "\n",
        "- The below images only show that the model runs without error\n",
        "\n",
        "- We plan to run the model in our GPU Server (~ing)\n",
        "\n",
        "- Model is from [GitHub](https://github.com/akaraspt/tinysleepnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 3-2-1 DataLoader for model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "class SleepDataset(Dataset):\n",
        "    def __init__(self, hr_file, ecg_file, window_size=30):\n",
        "        self.df1 = pd.read_csv(hr_file)\n",
        "        self.df2 = pd.read_csv(ecg_file)\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.freq1 = 1\n",
        "        self.freq2 = 256\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        len1 = len(self.df1)/(self.freq1*self.window_size)\n",
        "        len2 = len(self.df2)/(self.freq2*self.window_size)\n",
        "        #should be the same but just in case \n",
        "        return round(min(len1,len2))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_window1 = self.window_size*idx*self.freq1\n",
        "        start_window2 = self.window_size*idx*self.freq2\n",
        "        # Extract heart rate data points and label for the current window\n",
        "        hr = self.df1['heart_rate'].iloc[start_window1:start_window1+self.window_size*self.freq1].values\n",
        "        act = self.df1['activity_count'].iloc[start_window1:start_window1+self.window_size*self.freq1].values\n",
        "        labels = self.df1['psg_status'].iloc[start_window1]\n",
        "        #optional: combine labels\n",
        "        #labels = np.where(labels != 0, 1, labels)\n",
        "        if labels in [2,3]:\n",
        "            labels = 2\n",
        "        elif labels in [4,5]:\n",
        "            labels = 3\n",
        "\n",
        "            \n",
        "        ecg = self.df2['ECG'].iloc[start_window2:start_window2+self.window_size*self.freq2].values\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        hr = torch.tensor(hr, dtype=torch.float).unsqueeze(0)  # Add extra dimension at index 0\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "        ecg = torch.tensor(ecg, dtype=torch.float).unsqueeze(0)  # Add extra dimension at index 0\n",
        "        act = torch.tensor(act, dtype=torch.float).unsqueeze(0)  # Add extra dimension at index 0\n",
        "\n",
        "        return hr, ecg, act, labels    \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Model for baseline using Multi modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#takes in ECG and either activity or heart rate data \n",
        "\"\"\"\n",
        "class SleepNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SleepNet,self).__init__()\n",
        "        self.pool1 = nn.MaxPool1d(8, 8) #kernel_size, stride\n",
        "        self.pool2 = nn.MaxPool1d(4, 4) #kernel_size, stride\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, 128, 8) #in_channels, out_chanels, kernel_size\n",
        "        self.conv2 = nn.Conv1d(128, 128, 8) #in_channels, out_chanels, kernel_size\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.lstm = nn.LSTM(128,128)\n",
        "        \n",
        "\n",
        "        self.fc1 = nn.Linear(29952, 16)\n",
        "\n",
        "        self.fc2 = nn.Linear(30, 16)\n",
        "        self.fc3 = nn.Linear(32, 4)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, ecg, hr):\n",
        "\n",
        "        ecg = self.pool1(ecg)\n",
        "        ecg = self.dropout(ecg)\n",
        "        ecg = F.relu(self.conv1(ecg))\n",
        "        ecg = F.relu(self.conv2(ecg))\n",
        "        ecg = self.pool2(F.relu(self.conv2(ecg)))\n",
        "        ecg = self.dropout(ecg)\n",
        "\n",
        "\n",
        "        # Transpose dimensions for LSTM input\n",
        "        ecg = ecg.permute(2, 0, 1)  # Shape: [seq_len, batch_size, input_size]\n",
        "        \n",
        "        ecg, _ = self.lstm(ecg)\n",
        "        ecg = self.dropout(ecg)\n",
        "\n",
        "        #Get size of final layer\n",
        "        x_dim = ecg.size(0) * ecg.size(2)\n",
        "\n",
        "        ecg = ecg.view(-1, x_dim) #[batch size, output size]\n",
        "        ecg = F.relu(self.fc1(ecg)) #[batch size, 16]\n",
        "\n",
        "        #fully connected layer for HR data \n",
        "        hr = hr.squeeze(1)\n",
        "        hr = F.relu(self.fc2(hr))  #[batch size, 16]\n",
        "\n",
        "\n",
        "        cat = torch.cat((ecg, hr), dim=1)\n",
        "        \n",
        "        cat = self.fc3(cat)\n",
        "        cat = cat.squeeze(1) # Flatten to [batch_size]\n",
        "        \n",
        "        return cat\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Image(\"../asset/trainingcurve_1.png\", width=600, height=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Image(\"../asset/trainingcurve_2.png\", width=600, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3-3 Foundation model\n",
        "\n",
        "- We develop the foundation model using [Foundation Model Paper](https://machinelearning.apple.com/research/large-scale-training) and [Foundation Model based on Multimodal data Paper](https://arxiv.org/abs/2310.20071) for solving problems we set\n",
        "\n",
        "- We plan to utilize the code from [Github](https://github.com/tomoyoshki/focal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Future Work\n",
        "\n",
        "- We continue to developing foundation model (FM) until week 3 of March\n",
        "\n",
        "- We have started training a model and evaludated performance of downstream\n",
        "\n",
        "- We will start to train a foundation model week 3\n",
        "\n",
        "- Finally, we will report our performance and simulate our model using private apple watch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Image(\"../asset/gantchart.png\", width=700, height=400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
