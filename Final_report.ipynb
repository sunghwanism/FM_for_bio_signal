{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lXwauXg2Srp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('./preproc/code')\n",
        "sys.path.append(\"./src\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drIdzT9D0Mvk"
      },
      "source": [
        "# Final Report\n",
        "\n",
        "Developed by\n",
        "\n",
        "- Sunghwan Moon, Jungmin Kim, Seunghyeon Park, Bojing Gui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZJl_qAo2Srr"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9Ca-8yy2Srr"
      },
      "source": [
        "### (1) Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiY9fC796LIN"
      },
      "source": [
        "With the emergence of edge devices such as apple watch and Fit-bit, the use of personalized health data is becoming more prevalent. Therefore, if we can make good use of personalized health data such as biometric signals in edge devices, we will be able to easily monitor our health conditions such as our sleep patterns.\n",
        "\n",
        "\n",
        "However, due to the nature of the edge device, personalized health data cannot be shared with one another, limiting the learning of a huge quantity of data. Moreover, since it must be embedded in the edge device model, the size of the personalized model itself is limited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETlX9jOn2Srr"
      },
      "source": [
        "Our problem setting is characterized by three main issues:\n",
        "\n",
        "*  Lack of label in sleep stage classification label.\n",
        "*  Existence of multimodalities in bio-signal data.\n",
        "*  Downstream task for personalized model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YKEVMVS2Srs"
      },
      "source": [
        "### (2) Related Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GonA8bTm5Fp_"
      },
      "source": [
        "- **FOCAL** Framework (GitHub)[] # to-do: add link to Github\n",
        "\n",
        "    The FOCAL Framework, proposed in [FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space, 2023], is a self-supervised multimodal contrastive framework. And, in this case, not only is shared information between sensory modalities extracted, but exclusive modality information is not explicitly considered, which could be essential to understanding the underlying sensing physics. We intend to integrate subject-aware learning to this strategy.\n",
        "\n",
        "<img src=\"asset/FOCAL_figure.png\" alt=\"focal\" style=\"width:900px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMxfQiSs5P3M"
      },
      "source": [
        "- **Subject-Aware Contrastive Learning for Biosignals** (GitHub)[] # to-do: add link to Github\n",
        "\n",
        "    This study presents a methodology for dealing with Intersubject variability when learning representations for biosignals. Intersubject variability can be interpreted as a different domain for each subject. The domain shift from subject to subject can be modeled and corrected with adversarial training. Through this adversarial training, the model can also be promoted to learn domain-invariant features. We also used adversarial training to learn subject invariant features in our learning methodology.\n",
        "\n",
        "<img src=\"asset/subject_aware_image.png\" alt=\"overview\" style=\"width:900px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt85bilQ2Srs"
      },
      "source": [
        "## Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA5aYzCx2Srs"
      },
      "source": [
        "### Project Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovPa26IF6pCY"
      },
      "source": [
        "In our big overview, we expect that the foundation model generates informative representative feature from large bio-signal dataset, and it can improve the downstream task in the restricted environment where people cannot share bio-signal data with others.\n",
        "\n",
        "Specifically, we will attempt to develop the foundation model using bio-signal (ECG, Heart Rate) [1] for applying the sleep stage classification from the personal data of edge devices [2], such as Apple Watch or Fit-bit. It is hard to get high performance by only using personally own data from edge devices and to train the model, as limitation of the amount of data for train and low hardware resources of edge devices. We expect that the foundation model generates informative representative feature from large bio-signal dataset, and it can improve the downstream task in the restricted environment that people cannot share bio-signal data to others.\n",
        "\n",
        "<img src=\"asset/overall_task_architecture.png\" alt=\"overview\" style=\"width:900px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXRa5qLm2Srs"
      },
      "source": [
        "### Foundation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxK_HbMf41gQ"
      },
      "source": [
        "\n",
        "We present the issue that the personalized model in the edge device lacks sufficient data to learn each personalized model. We intend to suggest a solution to the problem by creating a foundation model from a huge amount of bio-signal data that has been made public and improving the performance of the personalized model in the edge device using the foundation model. We plan to develop a foundation model that takes into account each of the 3 issues we established for the problem setting .\n",
        "\n",
        "Our proposed strategy involves using a huge quantity of publically available bio-signal data to build a foundation model, which can then be used to improve the performance of personalized models on edge devices. By leveraging the representation power of the foundation model, we expect the personalized sleep stage classification model to get better performance than the case with a classification model without the foundation model. Previous studies focusing on points each existed, but did not take into account of all the relevant factors essential for the personalized model, so we newly propose the following model satisfiying these points.\n",
        "\n",
        "<img src=\"asset/foundation_model_structure.png\" alt=\"FMArch\" style=\"width:900px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6I16QtU2Srt"
      },
      "source": [
        "#### 1. Multi Modal Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JCvKLWfEhGQ"
      },
      "source": [
        "##### 1-1 Contrastive Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4Q96YEA7sLq"
      },
      "source": [
        "First, we tried to fully utilize various modalities in public datset in order to create an advanced foundation model.\n",
        "\n",
        "To fully utilize the multimodal signal, contrastive learning was constructed by dividing the shared features and each private features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiZpVSiMEhGR"
      },
      "source": [
        "1) How to construct positive and negative pair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wSygJi4EhGR"
      },
      "source": [
        "2) Shared/Private/Orthogonal features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0DDaYnk2Srt"
      },
      "source": [
        "##### 1-2 Subject Invariant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOvlAMAw8B9q"
      },
      "source": [
        "In addition, for subject aware learning, we trained a sub-classifier to classify the subject using a hidden vector in the encoder, and we used the subject loss as an adversarial loss to induce the encoder to learn the subject invariant feature.\n",
        "\n",
        "\n",
        "1-2-1 adversarial classifier\n",
        " - adfadfadf\n",
        " - adfadfadsf\n",
        " - adfadfa\n",
        "\n",
        "\n",
        "1-2-2 subject invariance loss\n",
        " - adfadfadf\n",
        " - adfadfadsf\n",
        " - adfadfa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUZL4UPcEhGT"
      },
      "source": [
        "##### 1-3 Backbone Model for contrastive learning\n",
        "\n",
        "\n",
        "1-3-1 Deepsense \n",
        "\n",
        "- Explain ~~~~\n",
        "- Explain ~~~~\n",
        "\n",
        "- Reference: [GitHub](https://github.com/akaraspt/tinysleepnet) Link\n",
        "\n",
        "\n",
        "<img src=\"asset/tinysleepnet.png\" alt=\"DeepSence\" style=\"width:600px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_mJz7la2Srt"
      },
      "source": [
        "### Downstream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCIlQiaWEhGU"
      },
      "source": [
        "#### 1. Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLY7OcY6EhGV"
      },
      "source": [
        "##### 1-1 DeepSense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oVJqVwn9Wpj"
      },
      "source": [
        "The architecture for the downstream classifier is shown below. This will be used as the baseline comparison for our model. The neural network will take in the two inputs, heart rate and ECG. The ECG will be processed through convolutional layers and a LSTM before the inputs are concatenated and fully connected layers are used for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykOEzEg7EhGY"
      },
      "source": [
        "#### 2. Classifier With Foundation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEy4WucVEhGY"
      },
      "source": [
        "##### 2-1 Feed forward classifier - Foundation downstream classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l4b_E8K2Srt"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Dataset (to-do: Victoria, [Change the 내용])\n",
        "\n",
        "We used 356,062 samples from the public MESA data set of 125 subjects out of 6,814. And we chose to use ECG and Heart rate for modality since we could only gain those two from the apple watch data. And we collected individual apple watch dataset from 5 friends to perform our downstream task. We made a subject segment pair for our contrastive learning and we splitted the data into portion of 100 by 25 by 25.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"asset/data_description_table.png\" alt=\"DataTable\" style=\"width:900px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12BQQGjP2Sru"
      },
      "source": [
        "### 2. Preprocessing (to-do: Victoria, [Change the 내용])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQoHqeTg9p6A"
      },
      "source": [
        "1) MESA data\n",
        "- We needs to segment the bio signal (heart rate and ecg) to 30 seconds (1 epoch) because sleep stage is decided from 30 seconds data in down stream\n",
        "- We select validation epoch without any problems, such as disconnection error, mis-collection time between bio signals.\n",
        "\n",
        "- Heart Rate\n",
        "    - After selecting validation epoch, the heart rate was **interpolated** to have a value for every 1 second, **smoothed** and **filtered** to amplify periods of high change by convoloving with a diffrence of Gaussian filter and **normalized** by dividing by the 90th percentile in the absolute diffrence between each heart rate measurement and the mean heart rate over the sleep periods\n",
        "- Electrocardiogram (ECG=EKG)\n",
        "    - After selecting validation epoch, the ECG was **smoothed** and **filtered** by Gaussian filter for denoising the ECG\n",
        "\n",
        "\n",
        "2) Apple watch data\n",
        "- We collected Apple watch data which contains heart rate and acceleration\n",
        "- We plan to preprocess it following the previous study titled \"**Sleep stage prediction with raw acceleration and photoplethysmography heart rate data derived from a consumer wearable device** (Olivia, et al.)\"\n",
        "\n",
        "\n",
        "3) Generating Data Pair\n",
        "\n",
        "- explain ~~~~\n",
        "- explain ~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2PsRyvM-II0"
      },
      "source": [
        "##### 2-1-1 Preprocessing MESA\n",
        "We preprocessed bio signals (Heart Rate & ECG) like below\n",
        "\n",
        "- **Heart Rate**\n",
        "    - After selecting validation epoch, the heart rate was **interpolated** to have a value for every 1 second, **smoothed** and **filtered** to amplify periods of high change by convoloving with a diffrence of Gaussian filter and **normalized** by dividing by the 90th percentile in the absolute diffrence between each heart rate measurement and the mean heart rate over the sleep periods\n",
        "- **Electrocardiogram (ECG=EKG)**\n",
        "    - After selecting validation epoch, the ECG was **smoothed** and **filtered** by Gaussian filter for denoising the ECG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from apple import generate_data_to_dic, filter_to_epoch\n",
        "from mesa import mesa_preprocessing, get_subject_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATAPATH = None # For privay reasons, the path to the data is not provided\n",
        "subject_ids = get_subject_ids(PATH=os.path.join(DATAPATH, \"./mesa/polysomnography/annotations-events-nsrr\"))\n",
        "len(subject_ids)\n",
        "\n",
        "SAVEPATH= None # For privay reasons, the path to the data is not provided\n",
        "Error = mesa_preprocessing(subject_ids, savepath=SAVEPATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_datapath = None # For privay reasons, the path to the data is not provided\n",
        "\n",
        "for subj in subject_ids:\n",
        "\n",
        "    if subj not in Error:\n",
        "        activity = np.load(SAVEPATH+subj+\"_activity_count.npy\").reshape(-1)\n",
        "        hr = np.load(SAVEPATH+subj+\"_heart_rate.npy\").reshape(-1)\n",
        "        ecg = np.load(SAVEPATH+subj+\"_ecg.npy\").reshape(-1)\n",
        "        ecg = -ecg\n",
        "        psg_status = np.load(SAVEPATH+subj+\"_labeled_sleep.npy\").squeeze()\n",
        "        repeat_psg = []\n",
        "        repeat_cosine = []\n",
        "        session = []\n",
        "        session_ecg = []\n",
        "        psg_ecg = []\n",
        "\n",
        "        ses = 0\n",
        "\n",
        "        for i, psg in enumerate(psg_status):\n",
        "            repeat_psg.extend([psg]*30)            \n",
        "            session.extend([ses]*30)\n",
        "            session_ecg.extend([ses]*256)\n",
        "            psg_ecg.extend([psg]*256)\n",
        "            \n",
        "            ses += 1\n",
        "\n",
        "        print(np.array(activity).shape, np.array(hr).shape, np.array(ecg).shape, len(repeat_psg), len(psg_ecg), len(session_ecg))\n",
        "\n",
        "        df = pd.DataFrame({'activity_count': activity, 'heart_rate': hr, 'session_id':session, 'psg_status': repeat_psg})\n",
        "        df_ecg = pd.DataFrame({\"ECG\":ecg, 'session_id':session_ecg, 'psg_status': psg_ecg})\n",
        "        \n",
        "        df.to_csv(csv_datapath, index=False)\n",
        "        df_ecg.to_csv(csv_datapath, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the preprocessing, we filtered **1,405 subjects** among 2,056 after quality contorl and, finally, we select **100 subjects** for training our foundation model.\n",
        "The total segments we used is XXXX (1 epoch during 30s).\n",
        "\n",
        "Below image show the result of preprocessing on **subject0001** from MESA dataset\n",
        "\n",
        "[ECG]\n",
        "\n",
        "<img src=\"asset/ecg_0001.png\" alt=\"ECG_MESA\" style=\"width:900px;\">\n",
        "\n",
        "[Heart Rate]\n",
        "\n",
        "<img src=\"asset/heartrate_0001.png\" alt=\"ECG_MESA\" style=\"width:900px;\">\n",
        "\n",
        "[Sleep Stage]\n",
        "\n",
        "<img src=\"asset/psg_0001.png\" alt=\"ECG_MESA\" style=\"width:900px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeyMXJYq-V_L"
      },
      "source": [
        "##### 2-1-2 Preprocessing Apple watch\n",
        "- We collect 5 real Apple watch dataset which was collected during 7 days\n",
        "- We preprocessed dataset with same pipeline with MESA dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PATH = None # For privay reasons, the path to the data is not provided\n",
        "\n",
        "subject_ids = []\n",
        "for n, filename in enumerate(os.listdir(PATH)):\n",
        "    filename = filename.split('_')\n",
        "    subject_id = int(filename[0])\n",
        "\n",
        "    if subject_id not in subject_ids:\n",
        "        subject_ids.append(subject_id)\n",
        "    \n",
        "# sort the list\n",
        "sorted_subject_ids = sorted(subject_ids)\n",
        "np.array(sorted_subject_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = None # For privay reasons, the path to the data is not provided\n",
        "\n",
        "apple_data = generate_data_to_dic(PATH,\n",
        "                                  second_column_step=1,val_to_fill_nans=-100,\n",
        "                                  print_analysis=False)\n",
        "\n",
        "\n",
        "for subj in sorted_subject_ids:\n",
        "    savepath = \"../data/watch/processed/temp/subject_\" + str(subj) + \".csv\"\n",
        "    apple_data[subj].to_csv(savepath, index=False)\n",
        "\n",
        "\n",
        "df, psg_status = filter_to_epoch(\"../data/watch/processed/temp/\", bin_size=30)\n",
        "\n",
        "for subj in sorted_subject_ids:\n",
        "    ses_id = psg_status[subj].keys()\n",
        "    last_ses = list(ses_id)[-1]\n",
        "    stage = psg_status[subj].values()\n",
        "    \n",
        "    temp = pd.DataFrame({'session_id': ses_id, 'new_psg_status': stage})\n",
        "    \n",
        "    new_df = pd.merge(df[subj], temp, on='session_id')\n",
        "    new_df.drop(columns=[\"psg_status\"], inplace=True)\n",
        "    \n",
        "    new_df.rename(columns={'new_psg_status': 'psg_status'}, inplace=True)\n",
        "    \n",
        "    last_info = new_df[new_df['session_id'] == last_ses]\n",
        "    \n",
        "    if len(last_info) != 30:\n",
        "        new_df.drop(new_df[new_df['session_id'] == last_ses].index, inplace=True)\n",
        "    \n",
        "    savepath = save_path + str(subj) + \".csv\"\n",
        "    new_df.to_csv(savepath, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2-2 Generating Pair dataset for Contrastive Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_directory = None # For privay reasons, the path to the data is not provided\n",
        "save_dir = None # For privay reasons, the path to the data is not provided\n",
        "\n",
        "datas = os.listdir(data_directory)\n",
        "datas.sort()\n",
        "datas = datas[:4*100]\n",
        "ecg_datas = [data for data in datas if 'ecg' in data]\n",
        "hr_datas = [data for data in datas if 'heart_rate' in data]\n",
        "sleep_datas = [data for data in datas if 'sleep' in data]\n",
        "act_datas = [data for data in datas if 'activity' in data]\n",
        "\n",
        "subj_data = {'ecg':ecg_datas, 'hr': hr_datas, 'stage':sleep_datas, 'act':act_datas}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_pair(ecg, hr, sleep, act, subject_idx, subject_id, savepath):\n",
        "\n",
        "    for seg_idx in range(len(sleep)):\n",
        "\n",
        "        ecg_data = ecg.squeeze()[seg_idx]\n",
        "        hr_data = hr.squeeze()[seg_idx]\n",
        "        sleep_data = sleep.squeeze()[seg_idx]\n",
        "        act_data = act.squeeze()[seg_idx]\n",
        "\n",
        "        data_dict = {\n",
        "                    'ecg': ecg_data,\n",
        "                    'hr': hr_data,\n",
        "                    'stage': sleep_data,\n",
        "                    'activty': act_data,\n",
        "                    'subject_idx': subject_idx\n",
        "                    }\n",
        "        \n",
        "        savepath = os.path.join(save_dir, f'{subject_id}_{seg_idx}.npz')\n",
        "        np.savez(savepath, **data_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in tqdm(range(100)):\n",
        "\n",
        "    ecg = subj_data['ecg'][i]\n",
        "    hr = subj_data['hr'][i]\n",
        "    sleep = subj_data['stage'][i]\n",
        "    act = subj_data['act'][i]\n",
        "    \n",
        "    ecg_list = np.load(os.path.join(data_directory, ecg))\n",
        "    hr_list = np.load(os.path.join(data_directory, hr))\n",
        "    sleep_list = np.load(os.path.join(data_directory, sleep))\n",
        "    act_list = np.load(os.path.join(data_directory, act))\n",
        "    real_subject = ecg.split('_')[0]\n",
        "\n",
        "    make_pair(ecg_list, hr_list, sleep_list, act_list, i, real_subject, save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw-TIOd82Sru"
      },
      "source": [
        "#### (1) Training Foundation Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOuKQJW_2Sru"
      },
      "outputs": [],
      "source": [
        "sys.path.append('./src/foundation')\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('./')\n",
        "\n",
        "import args\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from models.AdversarialModel import AdversarialModel\n",
        "from models.FOCALModules import FOCAL\n",
        "from models.loss import FOCALLoss\n",
        "from models.Backbone import DeepSense\n",
        "from trainutils.metric import save_metrics\n",
        "\n",
        "from data.Dataset import MESAPairDataset\n",
        "from data.Augmentaion import init_augmenter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Function of Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_SA_Focal(train_loader, valid_loader, model, advs_model, \n",
        "                   optimizer, advs_optimizer, focal_loss_fn, args):\n",
        "    torch.manual_seed(args.SEED)\n",
        "    torch.cuda.manual_seed(args.SEED)\n",
        "    torch.cuda.manual_seed_all(args.SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    \n",
        "    start_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "    model_save_dir = args.trainer_config[\"model_save_dir\"]\n",
        "    model_save_dir = os.path.join(model_save_dir, start_time)\n",
        "    \n",
        "    if not os.path.exists(model_save_dir):\n",
        "        os.makedirs(model_save_dir)\n",
        "\n",
        "    trainer_config = args.trainer_config\n",
        "    log_save_dir = args.trainer_config[\"log_save_dir\"]\n",
        "    \n",
        "    model_save_format = args.model_save_format\n",
        "    model_save_format[\"focal_config\"] = args.focal_config\n",
        "    model_save_format['subj_invariant_config'] = args.subj_invariant_config\n",
        "    model_save_format['trainer_config'] = args.trainer_config\n",
        "    model_save_format['data_config'] = args.data_config\n",
        "    \n",
        "    aug_1_name = args.data_config['augmentation'][0]\n",
        "    aug_1_config = args.data_config['augmenter_config'].get(aug_1_name, {})\n",
        "    aug_2_name = args.data_config['augmentation'][1]\n",
        "    aug_2_config = args.data_config['augmenter_config'].get(aug_2_name, {})\n",
        "    \n",
        "    aug_1 = init_augmenter(aug_1_name, aug_1_config)\n",
        "    aug_2 = init_augmenter(aug_2_name, aug_2_config)\n",
        "    \n",
        "    model.train()\n",
        "    best_val_loss = float('inf')\n",
        "    \n",
        "    train_focal_losses, val_focal_losses = [], []\n",
        "    train_advs_losses = []\n",
        "    train_accuracies = []\n",
        "    \n",
        "    for ep in tqdm(range(trainer_config['epochs'])):\n",
        "        \n",
        "        model.train()\n",
        "        advs_model.train()\n",
        "        focal_loss_fn.train()\n",
        "        \n",
        "        # Save Result\n",
        "        focal_train_loss = 0\n",
        "        running_advs_train_loss = 0\n",
        "        \n",
        "        correct_preds = 0\n",
        "        total_preds = 0\n",
        "        \n",
        "        \n",
        "        for raw_modal_1, raw_modal_2, subj_label, sleep_label in train_loader:\n",
        "            raw_modal_1, raw_modal_2, subj_label, sleep_label = raw_modal_1.to(args.focal_config[\"device\"]), raw_modal_2.to(args.focal_config[\"device\"]), subj_label.to(args.focal_config[\"device\"]), sleep_label.to(args.focal_config[\"device\"]) # [B, 30], [B, 30*256], [B, 1]\n",
        "            \n",
        "            aug_1_modal_1 = aug_1(raw_modal_1)\n",
        "            aug_2_modal_1 = aug_2(raw_modal_1)\n",
        "            \n",
        "            aug_1_modal_2 = aug_1(raw_modal_2)\n",
        "            aug_2_modal_2 = aug_2(raw_modal_2)\n",
        "            \n",
        "            # For updating the only advs_model (classifier)\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in advs_model.parameters():\n",
        "                param.requires_grad = True\n",
        "                \n",
        "            advs_optimizer.zero_grad()\n",
        "            \n",
        "            # Using Encoder for classify the subject\n",
        "            enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
        "            \n",
        "            # Predict the subject\n",
        "            subj_pred = advs_model(enc_feature_1, enc_feature_2) \n",
        "            advs_loss = advs_model.forward_adversarial_loss(subj_pred, subj_label)\n",
        "            \n",
        "            advs_loss.backward()\n",
        "            advs_optimizer.step()\n",
        "            \n",
        "            running_advs_train_loss += advs_loss.item()\n",
        "            \n",
        "            # For efficient memory management\n",
        "            del enc_feature_1, enc_feature_2, subj_pred, advs_loss\n",
        "            \n",
        "            # For updating the only Focal model (SSL model)\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = True\n",
        "            for param in advs_model.parameters():\n",
        "                param.requires_grad = False\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            enc_feature_1, enc_feature_2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, proj_head=True)\n",
        "            \n",
        "            subj_pred = advs_model(enc_feature_1, enc_feature_2) \n",
        "            subj_invariant_loss = advs_model.forward_subject_invariance_loss(subj_pred, subj_label)\n",
        "            \n",
        "            focal_loss = focal_loss_fn(enc_feature_1, enc_feature_2, subj_invariant_loss) # To-Do -> add regularization term about subject invariant\n",
        "            focal_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            focal_train_loss += focal_loss.item()\n",
        "            \n",
        "            # Calculate accuracy\n",
        "            preds = torch.argmax(subj_pred, dim=1)\n",
        "            correct_preds += (preds == subj_label).sum().item()\n",
        "            total_preds += subj_label.size(0)\n",
        "            \n",
        "            # For efficient memory management\n",
        "            del enc_feature_1, enc_feature_2, subj_pred, focal_loss\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "        # Calculate and store train accuracy and losses for plotting\n",
        "        train_accuracy = correct_preds / total_preds\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        train_advs_losses.append(running_advs_train_loss / len(train_loader))\n",
        "        train_focal_losses.append(focal_train_loss / len(train_loader))\n",
        "        \n",
        "        print(f\"Epoch {ep} - Adversarial Loss: {running_advs_train_loss / len(train_loader)}, \\\n",
        "            Focal Loss: {focal_train_loss / len(train_loader)}, Accuracy: {train_accuracy}\")\n",
        "                \n",
        "        if ep % trainer_config['val_interval'] == 0:\n",
        "            model.eval()\n",
        "            advs_model.eval()\n",
        "            focal_loss_fn.eval()\n",
        "            \n",
        "            focal_val_loss = 0\n",
        "            \n",
        "            for raw_modal_1, raw_modal_2, subj_label, sleep_label in valid_loader:\n",
        "                raw_modal_1, raw_modal_2, subj_label, sleep_label = raw_modal_1.to(args.focal_config[\"device\"]), raw_modal_2.to(args.focal_config[\"device\"]), \\\n",
        "                                                                    subj_label.to(args.focal_config[\"device\"]), sleep_label.to(args.focal_config[\"device\"])\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    enc_feature_1, enc_feature_2 = model(raw_modal_1, raw_modal_2, raw_modal_1, raw_modal_2, proj_head=True)                    \n",
        "                    focal_loss = focal_loss_fn(enc_feature_1, enc_feature_2, 0) # To-Do -> add regularization term about subject invariant\n",
        "                    focal_val_loss += focal_loss.item()\n",
        "                    \n",
        "                    # For efficient memory management\n",
        "                    del enc_feature_1, enc_feature_2, focal_loss\n",
        "                    torch.cuda.empty_cache()\n",
        "                    \n",
        "            print(\"-----\"*20)\n",
        "            print(f\"(Validation) Epoch{ep} - Focal Loss: {focal_val_loss/ len(valid_loader)}\")                    \n",
        "            \n",
        "            val_focal_losses.append(focal_val_loss / len(valid_loader))\n",
        "                            \n",
        "            if focal_val_loss < best_val_loss:\n",
        "                best_val_loss = focal_val_loss\n",
        "                \n",
        "                time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                focal_model_checkpoint = os.path.join(model_save_dir, f'SSL_focal_model_ep_{ep}.pth')\n",
        "                \n",
        "                # Save ckpt & arguments\n",
        "                model_save_format[\"train_acc\"] = train_accuracy\n",
        "                model_save_format[\"train_loss\"] = focal_train_loss / len(train_loader)\n",
        "                model_save_format[\"val_loss\"] = focal_val_loss / len(valid_loader)\n",
        "                model_save_format[\"train_epoch\"] = ep\n",
        "                model_save_format[\"focalmodel_path\"] = focal_model_checkpoint\n",
        "                model_save_format[\"focal_state_dict\"] = model.state_dict()\n",
        "                model_save_format['advs_state_dict'] = advs_model.state_dict()\n",
        "                \n",
        "                torch.save(model_save_format, focal_model_checkpoint)                \n",
        "                \n",
        "                print(f\"Model Saved - Focal Model: {focal_model_checkpoint}\")\n",
        "            print(\"-----\"*20)\n",
        "    \n",
        "    LOGPATH = os.path.join(args.trainer_config[\"log_save_dir\"], f'SSL_focal_log_{start_time}.npz')\n",
        "    train_log = np.array([train_focal_losses, val_focal_losses, train_accuracies, train_advs_losses])\n",
        "    np.savez(LOGPATH, train_log)\n",
        "    \n",
        "    save_metrics(train_focal_losses, val_focal_losses, train_accuracies, train_advs_losses, start_time)\n",
        "                \n",
        "def print_args(args):\n",
        "    \n",
        "    print(\"Data Configs:\")\n",
        "    for k, v in args.data_config.items():\n",
        "        print(f\"\\t{k}: {v}\")\n",
        "    print(\"----------\"*10)\n",
        "    \n",
        "    print(\"Focal Configs:\")\n",
        "    for k, v in args.focal_config.items():\n",
        "        print(f\"\\t{k}: {v}\")\n",
        "    print(\"----------\"*10)\n",
        "    \n",
        "    print(\"Subject Invariant Configs:\")\n",
        "    for k, v in args.subj_invariant_config.items():\n",
        "        print(f\"\\t{k}: {v}\")\n",
        "    print(\"----------\"*10)\n",
        "    \n",
        "    print(\"Trainer Configs:\")\n",
        "    for k, v in args.trainer_config.items():\n",
        "        print(f\"\\t{k}: {v}\")\n",
        "    print(\"----------\"*10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Generate DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_args(args)\n",
        "\n",
        "train_dataset = MESAPairDataset(file_path=args.data_config['train_data_dir'],\n",
        "                                modalities=args.data_config['modalities'],\n",
        "                                subject_idx=args.data_config['subject_key'],\n",
        "                                stage=args.data_config['label_key'])\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                            batch_size=args.trainer_config['batch_size'],\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=4)\n",
        "\n",
        "val_dataset = MESAPairDataset(file_path=args.data_config['val_data_dir'],\n",
        "                                modalities=args.data_config['modalities'],\n",
        "                                subject_idx=args.data_config['subject_key'],\n",
        "                                stage=args.data_config['label_key'])\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                            batch_size=args.trainer_config['batch_size']//3,\n",
        "                                            shuffle=False,\n",
        "                                            num_workers=2)\n",
        "\n",
        "print(\"****** Successfully Dataset ******\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training the Foundation Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "advs_model = AdversarialModel(args).to(args.subj_invariant_config[\"device\"])\n",
        "advs_optimizer = torch.optim.Adam(advs_model.parameters(), lr=args.subj_invariant_config['lr'])\n",
        "print(\"****** Complete Loading the Adversarial Model ******\")\n",
        "\n",
        "\n",
        "if str(list(args.focal_config[\"backbone\"].keys())[0]) == \"DeepSense\":\n",
        "    backbone = DeepSense(args).to(args.focal_config[\"device\"])\n",
        "    \n",
        "else:\n",
        "    raise ValueError(\"Not Supported Backbone\")\n",
        "\n",
        "focal_model = FOCAL(args, backbone).to(args.focal_config[\"device\"])\n",
        "focal_optimizer = torch.optim.Adam(focal_model.parameters(), lr=args.focal_config[\"lr\"])\n",
        "focal_loss_fn = FOCALLoss(args)\n",
        "print(\"****** Complete Loading the FOCAL Model ******\")\n",
        "\n",
        "print(\"Start Training SA Focal Model\")\n",
        "\n",
        "\n",
        "train_SA_Focal(train_loader, val_loader, focal_model, advs_model,\n",
        "                focal_optimizer, advs_optimizer, focal_loss_fn, args)\n",
        "\n",
        "print(\"Finished Training SA Focal Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh6p4Rd92Sru"
      },
      "source": [
        "#### (2) Training Basemodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sys.path.append(\"./basemodel\")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import baselineargs as args\n",
        "from DeepSense import DeepSense\n",
        "\n",
        "torch.manual_seed(args.SEED)\n",
        "torch.cuda.manual_seed(args.SEED)\n",
        "torch.cuda.manual_seed_all(args.SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "data_config = args.data_config\n",
        "model_config = args.trainer_config\n",
        "model_save_format = args.model_save_format\n",
        "\n",
        "model_save_format[\"trainer_config\"] = model_config\n",
        "model_save_format[\"data_config\"] = data_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AppleDataset(Dataset):\n",
        "    def __init__(self, file_path, modalities=['ecg', 'hr'], subject_idx='subject_idx', stage='stage'):\n",
        "        super(AppleDataset, self).__init__()\n",
        "        self.root_dir = file_path\n",
        "        self.files = os.listdir(file_path)\n",
        "        self.modalities = modalities\n",
        "        self.subject_idx = subject_idx\n",
        "        self.stage = stage\n",
        "        \n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.files)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = np.load(os.path.join(self.root_dir, self.files[idx])) # numpy file on each sample (segments)\n",
        "        \n",
        "        self.modality_1 = torch.tensor(data[self.modalities[0]], dtype=torch.float)\n",
        "        self.modality_2 = torch.tensor(data[self.modalities[1]], dtype=torch.float)\n",
        "        self.subject_id = torch.tensor(data[self.subject_idx], dtype=torch.long)\n",
        "        stage = data[self.stage]\n",
        "        \n",
        "        #if self.num_outputs == 4:\n",
        "        if stage in [1, 2]:\n",
        "            stage = 1\n",
        "        elif stage in [3, 4]:\n",
        "            stage = 2\n",
        "        elif stage == 5:\n",
        "            stage = 3\n",
        "            \n",
        "        self.sleep_stage = torch.tensor(stage, dtype=torch.long)\n",
        "        sample = [self.modality_1, self.modality_2, self.subject_id, self.sleep_stage]\n",
        "        \n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZIBeAwI2Sru"
      },
      "outputs": [],
      "source": [
        "def get_accuracy_from_train_process(logit_arr, true_label):\n",
        "    \n",
        "    predicted_label = torch.argmax(logit_arr, dim=1)\n",
        "    acc = torch.sum(predicted_label == true_label).item() / true_label.size(0)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def get_acc_loss_from_dataloader(model, dataloder, device, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    total_loss = 0\n",
        "    \n",
        "    for i, data in enumerate(dataloder):\n",
        "        ecg, hr, _, sleep_stage = data\n",
        "        ecg = ecg.to(device)\n",
        "        hr = hr.to(device)\n",
        "        sleep_stage = sleep_stage.to(device)\n",
        "        \n",
        "        output = model(ecg, hr, class_head=True, proj_head=True)\n",
        "        loss = criterion(output, sleep_stage)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        total_correct += torch.sum(torch.argmax(output, dim=1) == sleep_stage).item()\n",
        "        total_samples += sleep_stage.size(0)\n",
        "        \n",
        "    return total_correct / total_samples, total_loss / (i+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pretrain(model, model_name, train_loader, val_lodaer, optimizer, loss_fn, model_config, device):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    best_acc = 0\n",
        "    \n",
        "    folder = os.path.join(model_config[\"model_save_dir\"], f'{model_name}_{start_time}')\n",
        "    \n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "    \n",
        "    plot_train_loss = []\n",
        "    plot_val_loss = []\n",
        "    plot_val_acc = []\n",
        "    plot_train_acc = []\n",
        "    \n",
        "    model_save_format[\"lr\"] = model_config[\"lr\"]\n",
        "    \n",
        "    for ep in tqdm(range(model_config[\"epoch\"])):\n",
        "        prediction_arr = []\n",
        "        true_arr = []\n",
        "        train_loss = 0\n",
        "        model.train()\n",
        "        for i, data in enumerate(train_loader):\n",
        "            ecg, hr, _, sleep_stage = data\n",
        "            ecg = ecg.to(device)\n",
        "            hr = hr.to(device)\n",
        "            \n",
        "            sleep_stage = sleep_stage.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            prediction = model(ecg, hr, class_head=True, proj_head=True)\n",
        "            \n",
        "            loss = loss_fn(prediction, sleep_stage)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            prediction_arr.extend(prediction.detach().cpu().squeeze().numpy())\n",
        "            true_arr.extend(sleep_stage.detach().cpu().squeeze().numpy())\n",
        "            \n",
        "        model.eval()\n",
        "        train_loss /= (i+1)\n",
        "        prediction_arr = torch.tensor(np.array(prediction_arr))\n",
        "        true_arr = torch.tensor(np.array(true_arr))\n",
        "        train_acc = get_accuracy_from_train_process(prediction_arr, true_arr)\n",
        "        \n",
        "        plot_train_loss.append(train_loss)\n",
        "        plot_train_acc.append(train_acc)\n",
        "        print(f'Epoch: {ep}, Batch: {i+1}, TrainLoss: {loss.item()}, TrainAcc: {train_acc}')\n",
        "            \n",
        "        if ep % model_config['val_interval'] == 0:\n",
        "            val_acc, val_loss = get_acc_loss_from_dataloader(model, val_lodaer, device, loss_fn)\n",
        "            print(f'(Validation) Epoch: {ep},  ValLoss: {val_loss}, ValAcc: {val_acc}')\n",
        "            plot_val_acc.append(val_acc)\n",
        "            plot_val_loss.append(val_loss)\n",
        "            \n",
        "            if val_acc > best_acc:\n",
        "                print(\"--------\"*15)\n",
        "                best_acc = val_acc                \n",
        "                MODELPATH = os.path.join(folder, f'{model_name}_{ep}.pth')\n",
        "                model_save_format[\"epoch\"] = ep\n",
        "                model_save_format[\"state_dict\"] = model.state_dict()\n",
        "                model_save_format[\"model_path\"] = MODELPATH\n",
        "                model_save_format[\"train_acc\"] = train_acc\n",
        "                model_save_format[\"train_loss\"] = train_loss\n",
        "                model_save_format[\"val_acc\"] = val_acc\n",
        "                model_save_format[\"val_loss\"] = val_loss\n",
        "                \n",
        "                torch.save(model_save_format, MODELPATH)\n",
        "                print(\"Best Model Saved!\")\n",
        "                print(\"--------\"*15)\n",
        "    \n",
        "    print(\"Finished Training\")\n",
        "    print(f'Best Validation Accuracy: {best_acc}')\n",
        "    \n",
        "    return model_save_format, (plot_train_loss, plot_train_acc, plot_val_loss, plot_val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_bmalgO2Sru"
      },
      "outputs": [],
      "source": [
        "model_save_format[\"batch_size\"] = model_config[\"batch_size\"]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(AppleDataset(data_config[\"train_data_dir\"]), batch_size=model_config[\"batch_size\"], shuffle=True, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(AppleDataset(data_config[\"val_data_dir\"]), batch_size=model_config[\"batch_size\"], shuffle=False, num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(AppleDataset(data_config[\"test_data_dir\"]), batch_size=model_config[\"batch_size\"], shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2A8vJyK2Srv"
      },
      "outputs": [],
      "source": [
        "model = DeepSense(args)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=model_config[\"lr\"], weight_decay=model_config[\"weight_decay\"])\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "model_name = list(model_config[\"model_name\"].keys())[0]\n",
        "model_ckpt, (train_loss, train_acc, val_loss, val_acc) = pretrain(model, model_name, train_loader, val_loader, optimizer, loss_fn, model_config, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-84G8vS2Srv"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(args.trainer_config[\"log_save_dir\"]):\n",
        "    os.makedirs(args.trainer_config[\"log_save_dir\"])\n",
        "    \n",
        "LOGPATH = os.path.join(args.trainer_config[\"log_save_dir\"], f'{model_name}_{start_time}.npz')\n",
        "train_log = np.array([train_loss, train_acc, val_loss, val_acc])\n",
        "np.savez(LOGPATH, train_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(model_save_format[\"state_dict\"])\n",
        "\n",
        "test_acc, test_loss = get_acc_loss_from_dataloader(model, test_loader, device, loss_fn)\n",
        "\n",
        "test_acc = round(test_acc,2)\n",
        "test_loss = round(test_loss,2)\n",
        "\n",
        "LOGPATH = os.path.join(args.trainer_config[\"log_save_dir\"], f'{model_name}_{start_time}_acc{test_acc}_loss{test_loss}.npz')\n",
        "test_log = np.array([test_acc, test_loss])\n",
        "np.savez(LOGPATH, test_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### (3) Transfer Learning using Foundation Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sys.path.append(\"./src/downstream\")\n",
        "import downargs as args\n",
        "from classifier import SleepStageClassifier\n",
        "\n",
        "from foundation.models.FOCALModules import FOCAL\n",
        "from foundation.models.Backbone import DeepSense\n",
        "\n",
        "from foundation.data.Augmentaion import init_augmenter\n",
        "from foundation.data.Dataset import ApplePairDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aug_1 = init_augmenter(\"NoAugmenter\", None).to(device)\n",
        "aug_2 = init_augmenter(\"NoAugmenter\", None).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_accuracy_from_train_process(logit_arr, true_label):\n",
        "    \n",
        "    predicted_label = torch.argmax(logit_arr, dim=1)\n",
        "    acc = torch.sum(predicted_label == true_label).item() / true_label.size(0)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def get_acc_loss_from_dataloader(model, downstream_model, dataloder, loss_fn, device):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    total_loss = 0\n",
        "    \n",
        "    for i, data in enumerate(dataloder):\n",
        "        ecg, hr, _, sleep_stage = data\n",
        "        ecg = ecg.to(device)\n",
        "        hr = hr.to(device)\n",
        "        \n",
        "        aug_1_modal_1 = aug_1(ecg)\n",
        "        aug_2_modal_1 = aug_2(ecg)\n",
        "        \n",
        "        aug_1_modal_2 = aug_1(hr)\n",
        "        aug_2_modal_2 = aug_2(hr)\n",
        "        \n",
        "        sleep_stage = sleep_stage.to(device)\n",
        "        \n",
        "        mod_feature1, mod_feature2 = model(aug_1_modal_1, aug_1_modal_2, \n",
        "                                           aug_2_modal_1, aug_2_modal_2, proj_head=True, class_head=False)\n",
        "        \n",
        "        prediction = downstream_model(mod_feature1, mod_feature2)\n",
        "        loss = loss_fn(prediction, sleep_stage)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        total_correct += torch.sum(torch.argmax(prediction, dim=1) == sleep_stage).item()\n",
        "        total_samples += sleep_stage.size(0)\n",
        "        \n",
        "    return total_correct / total_samples, total_loss / (i+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def downstream(model, downstream_model, train_loader, val_lodaer, optimizer, loss_fn, downargs, device, model_idx):\n",
        "    # model.to(device)\n",
        "    model.train()\n",
        "    best_acc = 0\n",
        "    \n",
        "    plot_train_loss = []\n",
        "    plot_val_loss = []\n",
        "    plot_val_acc = []\n",
        "    plot_train_acc = []\n",
        "    \n",
        "    model_save_format = downargs.model_save_format\n",
        "    model_save_format[\"lr\"] = downargs.downstream_config[\"lr\"]\n",
        "\n",
        "    modelPATH = os.path.join(downargs.downstream_config[\"model_save_dir\"], downargs.SUBJECT_ID)\n",
        "    \n",
        "    if not os.path.exists(modelPATH):\n",
        "        os.makedirs(modelPATH)\n",
        "        \n",
        "    for ep in tqdm(range(downargs.downstream_config[\"epoch\"])):\n",
        "        prediction_arr = []\n",
        "        true_arr = []\n",
        "        train_loss = 0\n",
        "        model.train()\n",
        "        \n",
        "        for i, data in enumerate(train_loader):\n",
        "            ecg, hr, _, sleep_stage = data\n",
        "            ecg = ecg.to(device)\n",
        "            hr = hr.to(device)\n",
        "\n",
        "            aug_1_modal_1 = aug_1(ecg)\n",
        "            aug_2_modal_1 = aug_2(ecg)\n",
        "            \n",
        "            aug_1_modal_2 = aug_1(hr)\n",
        "            aug_2_modal_2 = aug_2(hr)\n",
        "            \n",
        "            sleep_stage = sleep_stage.to(device)\n",
        "            \n",
        "            # For updating the only downstream model\n",
        "            for param in downstream_model.parameters():\n",
        "                param.requires_grad = True\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = False\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                mod_feature1, mod_feature2 = model(aug_1_modal_1, aug_1_modal_2, aug_2_modal_1, aug_2_modal_2, \n",
        "                                                   proj_head=True, class_head=False)\n",
        "                \n",
        "            prediction = downstream_model(mod_feature1, mod_feature2)\n",
        "            \n",
        "            loss = loss_fn(prediction, sleep_stage)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            prediction_arr.extend(prediction.detach().cpu().squeeze().numpy())\n",
        "            true_arr.extend(sleep_stage.detach().cpu().squeeze().numpy())\n",
        "            \n",
        "        model.eval()\n",
        "        \n",
        "        train_loss /= len(train_loader)\n",
        "        \n",
        "        prediction_arr = torch.tensor(np.array(prediction_arr))\n",
        "        true_arr = torch.tensor(np.array(true_arr))\n",
        "        \n",
        "        train_acc = get_accuracy_from_train_process(prediction_arr, true_arr)\n",
        "        \n",
        "        plot_train_loss.append(train_loss)\n",
        "        plot_train_acc.append(train_acc)\n",
        "        \n",
        "        print(f'Epoch: {ep}, TrainLoss: {train_loss}, TrainAcc: {train_acc}')\n",
        "        \n",
        "        \n",
        "        if ep % downargs.downstream_config['val_freq'] == 0:\n",
        "            \n",
        "            val_acc, val_loss = get_acc_loss_from_dataloader(model, downstream_model, val_lodaer, loss_fn, device)\n",
        "            print(f'(Validation) Epoch: {ep},  ValLoss: {val_loss}, ValAcc: {val_acc}')\n",
        "            \n",
        "            plot_val_acc.append(val_acc)\n",
        "            plot_val_loss.append(val_loss)\n",
        "            \n",
        "            if val_acc > best_acc:\n",
        "                print(\"--------\"*15)\n",
        "                best_acc = val_acc\n",
        "                \n",
        "                MODELPATH = os.path.join(modelPATH, f'FM_based_classfier_{model_idx}.pth')\n",
        "                model_save_format[\"epoch\"] = ep\n",
        "                model_save_format[\"down_state_dict\"] = downstream_model.state_dict()\n",
        "                model_save_format[\"down_config\"] = downargs.downstream_config\n",
        "                model_save_format[\"focal_state_dict\"] = model.state_dict()\n",
        "                model_save_format['focal_config'] = args.focal_config\n",
        "                model_save_format[\"focal_trainer_config\"] = args.trainer_config\n",
        "                model_save_format[\"focal_data_config\"] = args.data_config\n",
        "                model_save_format[\"model_path\"] = MODELPATH\n",
        "                model_save_format[\"train_acc\"] = train_acc\n",
        "                model_save_format[\"train_loss\"] = train_loss\n",
        "                model_save_format[\"val_acc\"] = val_acc\n",
        "                model_save_format[\"val_loss\"] = val_loss\n",
        "                \n",
        "                torch.save(model_save_format, MODELPATH)\n",
        "                print(\"Best Model Saved!\")\n",
        "                print(\"--------\"*15)\n",
        "    \n",
        "    print(\"Finished Training\")\n",
        "    print(f'Best Validation Accuracy: {best_acc}')\n",
        "    \n",
        "    return model_save_format, (plot_train_loss, plot_train_acc, plot_val_loss, plot_val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelpath = args.trainer_config[\"model_save_dir\"]\n",
        "log_path = args.trainer_config[\"log_save_dir\"]\n",
        "\n",
        "model_list = ['SSL_focal_model_0140.pth', \"SSL_focal_model_0143.pth\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = ApplePairDataset(file_path=args.data_config['train_data_dir'], \n",
        "                                modalities=args.data_config['modalities'],\n",
        "                                subject_idx=args.data_config['subject_key'],\n",
        "                                stage=args.data_config['label_key'])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                            batch_size=args.trainer_config['batch_size'],\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=4)\n",
        "\n",
        "val_dataset = ApplePairDataset(file_path=args.data_config['val_data_dir'],\n",
        "                                modalities=args.data_config['modalities'],\n",
        "                                subject_idx=args.data_config['subject_key'],\n",
        "                                stage=args.data_config['label_key'])\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                            batch_size=args.trainer_config['batch_size']//4,\n",
        "                                            shuffle=False,\n",
        "                                            num_workers=2)\n",
        "\n",
        "test_dataset = ApplePairDataset(file_path=args.data_config['test_data_dir'],\n",
        "                                modalities=args.data_config['modalities'],\n",
        "                                subject_idx=args.data_config['subject_key'],\n",
        "                                stage=args.data_config['label_key'])\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                            batch_size=args.trainer_config['batch_size']//4,\n",
        "                                            shuffle=False,\n",
        "                                            num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model_name in model_list:\n",
        "    print(\"===========\"*10)\n",
        "    print(model_name)\n",
        "    print(\"===========\"*10)\n",
        "    model_index = model_name.split(\"_\")[3].split(\".\")[0]\n",
        "    model_ckpt = torch.load(os.path.join(modelpath, model_name), map_location=device)\n",
        "    \n",
        "    args.trainer_config = model_ckpt['trainer_config']\n",
        "    args.focal_config = model_ckpt[\"focal_config\"]\n",
        "    args.data_config = model_ckpt[\"data_config\"]\n",
        "    \n",
        "    args.downstream_config['embedding_dim'] = model_ckpt['focal_config']['embedding_dim']\n",
        "    \n",
        "    backbone = DeepSense(args).to(device)\n",
        "    focal_model = FOCAL(args, backbone).to(device)\n",
        "    \n",
        "    backbone = DeepSense(args).to(device)\n",
        "    focal_model = FOCAL(args, backbone).to(device)\n",
        "    focal_model.load_state_dict(model_ckpt[\"focal_state_dict\"], strict=False)\n",
        "    \n",
        "    downstream_model = SleepStageClassifier(args).to(device)\n",
        "    \n",
        "    downstream_loss_fn = nn.CrossEntropyLoss()\n",
        "    downstream_optimizer = torch.optim.Adam(downstream_model.parameters(), lr=args.downstream_config['lr'])\n",
        "            \n",
        "    ckpt, logs = downstream(focal_model, downstream_model, train_loader, val_loader,\n",
        "                            downstream_optimizer, downstream_loss_fn, args, device, model_index)\n",
        "    \n",
        "    \n",
        "    logPATH = os.path.join(args.downstream_config[\"log_save_dir\"], args.SUBJECT_ID)\n",
        "    \n",
        "    if not os.path.exists(logPATH):\n",
        "        os.makedirs(logPATH)\n",
        "        \n",
        "    LOGPATH = os.path.join(args.trainer_config[\"log_save_dir\"], f'FM_based_classfier_{model_index}.npz')\n",
        "    result_log = np.array(logs)\n",
        "    np.savez(LOGPATH, result_log)\n",
        "    \n",
        "    \n",
        "    test_acc, test_loss = get_acc_loss_from_dataloader(focal_model, downstream_model, test_loader, downstream_loss_fn, device)\n",
        "    test_acc = round(test_acc, 2)\n",
        "    test_loss = round(test_loss, 2)\n",
        "    \n",
        "    result_log = np.array([test_acc, test_loss])\n",
        "    LOGPATH = os.path.join(args.trainer_config[\"log_save_dir\"], f'FM_based_{model_index}_acc{test_acc}_loss{test_loss}.npz')\n",
        "    \n",
        "    np.savez(LOGPATH, result_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u49ZoQ42Sr3"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from demoutils import predict_using_individual_model, loss_acc_plotting, predict_using_fm_classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subj_index = \"0558\"\n",
        "ind_558_acc, ind_558_f1 = predict_using_individual_model(subj_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subj_index = \"0565\"\n",
        "ind_565_acc, ind_565_f1 = predict_using_individual_model(subj_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subj_index = \"0560\"\n",
        "ind_560_acc, ind_560_f1 = predict_using_individual_model(subj_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subj_index = \"0571\"\n",
        "ind_571_acc, ind_571_f1 = predict_using_individual_model(subj_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subj_index = \"0583\"\n",
        "ind_583_acc, ind_583_f1 = predict_using_individual_model(subj_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Individual Transfer Learning with Foundation Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subj_index = \"0558\"\n",
        "model_index = \"0143\"\n",
        "\n",
        "loss_acc_plotting(subj_index, model_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subj_index = \"0560\"\n",
        "model_index = \"0143\"\n",
        "loss_acc_plotting(subj_index, model_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subj_index = \"0565\"\n",
        "model_index = \"0143\"\n",
        "loss_acc_plotting(subj_index, model_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subj_index = \"0571\"\n",
        "model_index = \"0143\"\n",
        "loss_acc_plotting(subj_index, model_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subj_index = \"0583\"\n",
        "model_index = \"0143\"\n",
        "loss_acc_plotting(subj_index, model_index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FM based Prediction Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fm_558_acc, fm_558_f1 = predict_using_fm_classifier(subj_index=\"0558\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fm_565_acc, fm_565_f1 = predict_using_fm_classifier(subj_index=\"0565\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fm_560_acc, fm_560_f1 = predict_using_fm_classifier(subj_index=\"0560\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fm_571_acc, fm_571_f1 = predict_using_fm_classifier(subj_index=\"0571\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fm_583_acc, fm_583_f1 = predict_using_fm_classifier(subj_index=\"0583\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Compare individual Training VS. FM Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Subject 0558\")\n",
        "print(\"----------------\"*3)\n",
        "print(\"Individual Model Accuracy: \", ind_558_acc)\n",
        "print(\"*Foundation Model Accuracy: \", fm_558_acc)\n",
        "\n",
        "print(\"Individual Model F1 Score: \", ind_558_f1)\n",
        "print(\"*Foundation Model F1 Score: \", fm_558_f1)\n",
        "print(\"----------------\"*3)\n",
        "\n",
        "if fm_558_acc > ind_558_acc:\n",
        "    print(\"Foundation Model is better than Individual Model in terms of Accuracy\")\n",
        "    \n",
        "if fm_558_f1 > ind_558_f1:\n",
        "    print(\"Foundation Model is better than Individual Model in terms of F1 Score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Subject 0565\")\n",
        "print(\"----------------\"*3)\n",
        "print(\"Individual Model Accuracy: \", ind_565_acc)\n",
        "print(\"*Foundation Model Accuracy: \", fm_565_acc)\n",
        "\n",
        "print(\"Individual Model F1 Score: \", ind_565_f1)\n",
        "print(\"*Foundation Model F1 Score: \", fm_565_f1)\n",
        "print(\"----------------\"*3)\n",
        "\n",
        "if fm_565_acc > ind_565_acc:\n",
        "    print(\"Foundation Model is better than Individual Model in terms of Accuracy\")\n",
        "\n",
        "if fm_565_f1 > ind_565_f1:\n",
        "    print(\"Foundation Model is better than Individual Model in terms of F1 Score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Subject 0560\")\n",
        "print(\"----------------\"*3)\n",
        "print(\"Individual Model Accuracy: \", ind_560_acc)\n",
        "print(\"Foundation Model Accuracy: \", fm_560_acc)\n",
        "\n",
        "print(\"Individual Model F1 Score: \", ind_560_f1)\n",
        "print(\"Foundation Model F1 Score: \", fm_560_f1)\n",
        "print(\"----------------\"*3)\n",
        "\n",
        "if fm_560_acc > ind_560_acc:\n",
        "    print(\"Foundation Model is better than Individual Model in terms of Accuracy\")\n",
        "    \n",
        "if fm_560_f1 > ind_560_f1:\n",
        "    print(\"Foundation Model is better than Individual Model in terms of F1 Score\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Subject 0571\")\n",
        "print(\"----------------\"*3)\n",
        "print(\"Individual Model Accuracy: \", ind_571_acc)\n",
        "print(\"Foundation Model Accuracy: \", fm_571_acc)\n",
        "\n",
        "print(\"Individual Model F1 Score: \", ind_571_f1)\n",
        "print(\"Foundation Model F1 Score: \", fm_571_f1)\n",
        "print(\"----------------\"*3)\n",
        "\n",
        "if fm_571_acc > ind_571_acc:\n",
        "    print(\"Foundation Model is better than Individual Model in terms of Accuracy\")\n",
        "    \n",
        "if fm_571_f1 > ind_571_f1:\n",
        "    print(\"Foundation Model is better than Individual Model in terms of F1 Score\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Subject 0583\")\n",
        "print(\"----------------\"*3)\n",
        "print(\"Individual Model Accuracy: \", ind_583_acc)\n",
        "print(\"Foundation Model Accuracy: \", fm_583_acc)\n",
        "\n",
        "print(\"Individual Model F1 Score: \", ind_583_f1)\n",
        "print(\"Foundation Model F1 Score: \", fm_583_f1)\n",
        "print(\"----------------\"*3)\n",
        "\n",
        "if fm_571_acc > ind_583_acc:\n",
        "    print(\"Foundation Model is better than Individual Model in terms of Accuracy\")\n",
        "    \n",
        "if fm_571_f1 > ind_583_f1:\n",
        "    print(\"Foundation Model is better than Individual Model in terms of F1 Score\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
