{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drIdzT9D0Mvk"
      },
      "source": [
        "# Progress Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIjCJRgE0Mvm"
      },
      "source": [
        "### Project description\n",
        "We will attempt to develop the foundation model using bio-signal (ECG, Heart Rate) [1] for applying the sleep stage classification from the personal data of edge devices [2], such as Apple Watch or Fit-bit. It is hard to get high performance by only using personally own data from edge devices and to train the model, as limitation of the amount of data for train and low hardware resources of edge devices. We expect that the foundation model generates informative representative feature from large bio-signal dataset, and it can improve the downstream task in the restricted environment that people cannot share bio-signal data to others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMEKQBEi0Mvm"
      },
      "source": [
        "### Overall tasks structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr9ZzK_e0Mvn",
        "outputId": "e10b6114-7931-49e6-d9b9-6cf5189a2de9"
      },
      "outputs": [],
      "source": [
        "Image(filename='../asset/overall_task_architecture.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tEV_YF80Mvo"
      },
      "source": [
        "### To do (remove later)\n",
        "1. Data: Obtained all or most of the data\n",
        "2. Model: Come up with a reasonable model\n",
        "3. Result: Produced at least one promising result\n",
        "4. Reasoning: Provide a convincing argument for the feasibility of completing the project within\n",
        "the time remaining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y0XIJhA0Mvo"
      },
      "source": [
        "### 1. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F33sN0c0Mvo",
        "outputId": "18316e10-54ad-4a6c-8e31-68a979f18643"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='../asset/dataset_description_table.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIcTjzhq0Mvo"
      },
      "source": [
        "### 2. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwv_NPlB0Mvp"
      },
      "source": [
        "#### 2-1. Foundation Model\n",
        "We present the issue that the personalized model in the edge device lacks sufficient data to learn each personalized model. We intend to suggest a solution to the problem by creating a foundation model from a huge amount of bio-signal data that has been made public and improving the performance of the personalized model in the edge device using the foundation model. We plan to develop a foundation model that takes into account each of the 3 issues we established for the problem setting .\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2VaDxfm0Mvp"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Our proposed strategy involves using a huge quantity of publically available bio-signal data to build a foundation model, which can then be used to improve the performance of personalized models on edge devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FI_qmu00Mvp"
      },
      "source": [
        "#### Problem setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT2HsDi40Mvp"
      },
      "source": [
        "Our problem setting is characterized by three main issues:\n",
        "\n",
        "*  Lack of label in sleep stage classification label.\n",
        "*  Existence of multimodalities in bio-signal data.\n",
        "*  Downstream task for personalized model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNr_3htB0Mvq"
      },
      "source": [
        "#### Why this Foundation model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqQDnIM80Mvq"
      },
      "source": [
        "To address these 3 primary issues, we aim to incorporate 3 characteristics into our foundation model:\n",
        "\n",
        "**1. Self-supervised constrastive learning**\n",
        "\n",
        "First, we assumed that the label had existed in MESA, which is currently open to the public, but bio-signal labels are typically expensive and obtained manually by professionals. Given this, we plan to employ the self-supervised method of continuous learning, which does not require a label, as the foundation model's learning approach.\n",
        "\n",
        "**2. Considering multimodalities**\n",
        "\n",
        "Second, we want to create a foundation model that can account for multimodality characteristics because we deal with biosignals from two separate modalities: ECG (256Hz) and heart rate (1Hz). The framework(FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Lantent Space, 2023) we referred to can fully utilize multimodal signal information by separating the shared features shared by the two modalities from the private features that each modality has on the latent space.\n",
        "\n",
        "**3. Subject-aware learning**\n",
        "\n",
        "Finally, because our foundation model is 'personalized', in which the subject is employed in each different models, it should be possible to avoid domain shift caused by inter-subject variability. Therefore, our foundation model will incorporate subject-invariance into the continuous learning framework so that it can learn domain-invariant characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejTuFn0OJgb8"
      },
      "source": [
        "#### Existing structure to refer to"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhOwi-zx0Mvq"
      },
      "source": [
        "- FOCAL Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IigiX37w0Mvq",
        "outputId": "745693f7-5a56-4d5a-d42b-83c48713b774"
      },
      "outputs": [],
      "source": [
        "Image(filename='../asset/FOCAL_figure.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MphqrUBzIi9O"
      },
      "source": [
        "The FOCAL Framework, proposed in [FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space, 2023], is a self-supervised multimodal contrastive framework. And, in this case, not only is shared information between sensory modalities extracted, but exclusive modality information is not explicitly considered, which could be essential to understanding the underlying sensing physics. We intend to integrate subject-aware learning to this strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvvM3w5X0Mvq"
      },
      "source": [
        "#### 2-2. Classification model for Downstream task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI3R_9rPIFFL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blDUOAs40Mvq"
      },
      "source": [
        "### 3. Progress Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) MESA data\n",
        "- We needs to segment the bio signal (heart rate and ecg) to 30 seconds (1 epoch) because sleep stage is decided from 30 seconds data in down stream\n",
        "- We select validation epoch without any problems, such as disconnection error, mis-collection time between bio signals.\n",
        "\n",
        "- Heart Rate\n",
        "    - After selecting validation epoch, the heart rate was **interpolated** to have a value for every 1 second, **smoothed** and **filtered** to amplify periods of high change by convoloving with a diffrence of Gaussian filter and **normalized** by dividing by the 90th percentile in the absolute diffrence between each heart rate measurement and the mean heart rate over the sleep periods\n",
        "- Electrocardiogram (ECG=EKG)\n",
        "    - After selecting validation epoch, the ECG was **smoothed** and **filtered** by Gaussian filter for denoising the ECG\n",
        "\n",
        "\n",
        "2) Apple watch data\n",
        "- We collected Apple watch data which contains heart rate and acceleration\n",
        "- We plan to preprocess it following the previous study titled \"**Sleep stage prediction with raw acceleration and photoplethysmography heart rate data derived from a consumer wearable device** (Olivia, et al.)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocessing Result\n",
        "\n",
        "Following images show the result of preprocessing from MESA dataset and raw data collected from Apple watch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### MESA data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Actiography\n",
        "Image(filename='../asset/actiography_0001.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ECG Image\n",
        "Image(filename=\"../asset/ecg_0001.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heart Rate\n",
        "Image(\"../asset/heartrate_0001.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Actiography\n",
        "Image(\"../asset/actiography_0001.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PSG Status (Sleep Stages)\n",
        "Image(\"../asset/psg_0001.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Apple watch data\n",
        "- We collect real Apple watch data which was collected during 7 days\n",
        "- We complete to code for preprocessing Apple watch data but not finished yet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../preproc/outputs/applewatch_public/c1_data.csv\")\n",
        "\n",
        "fig, ax = plt.subplots(3, 1, figsize=(16, 5))\n",
        "\n",
        "ax[0].plot(df[\"heart_rate\"])\n",
        "ax[0].set_title(\"Heart Rate\")\n",
        "\n",
        "ax[1].plot(df[\"x_move\"])\n",
        "ax[1].plot(df[\"y_move\"])\n",
        "ax[1].plot(df[\"z_move\"])\n",
        "ax[1].legend([\"X\", \"Y\", \"Z\"])\n",
        "ax[1].set_title(\"Acceleration\")\n",
        "\n",
        "ax[2].plot(df[\"psg_status\"])\n",
        "ax[2].set_title(\"PSG Status (Sleep Stages)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3L5h0k10Mvq"
      },
      "source": [
        "### DownStream Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
