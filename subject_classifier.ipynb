{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACLAdversary(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    see  Figure 1 in arxiv.org/pdf/2007.04871.pdf for diagram\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_subjects, dropout_rate=0.5):\n",
    "        super(SACLAdversary, self).__init__()\n",
    "        self.model = torch.nn.Sequential(torch.nn.Linear(embed_dim, embed_dim//2), \n",
    "                                                torch.nn.ReLU(), \n",
    "                                                torch.nn.Linear(embed_dim//2, embed_dim//2), \n",
    "                                                torch.nn.ReLU(), \n",
    "                                                torch.nn.Linear(embed_dim//2, embed_dim//2), \n",
    "                                                torch.nn.ReLU(), \n",
    "                                                torch.nn.Linear(embed_dim//2, num_subjects), \n",
    "                                                torch.nn.Sigmoid() # ADDED BY ZAC TO ADDRESS NANs IN ADVERSARIAL LOSS\n",
    "        )\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversary -> subject classifier\n",
    "adversary = SACLAdversary(embed_dim, num_subjects, dropout_rate=dropout_rate).to(device)\n",
    "if former_adversary_state_dict_file is not None:\n",
    "    adversary.load_state_dict(torch.load(former_adversary_state_dict_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Adversarial Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAContrastiveAdversarialLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    see Section 3.1 of arxiv.org/pdf/2007.04871.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature, adversarial_weighting_factor=1):\n",
    "        super(SAContrastiveAdversarialLoss, self).__init__()\n",
    "        self.BATCH_DIM = 0\n",
    "        self.tau = temperature\n",
    "        self.lam = adversarial_weighting_factor\n",
    "        self.cos_sim = torch.nn.CosineSimilarity(0)\n",
    "        self.log_noise = 1e-12 # 8 # see https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons\n",
    "        # self.contrastive_loss = ContrastiveLoss(temperature)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, z1s, z2s, z1_c_outs, z1_subject_labels):\n",
    "        \"\"\"\n",
    "        z1s represents the (batched) representation(s) of the t1-transformed signal(s)\n",
    "        z2s represents the (batched) representation(s) of the t2-transformed signal(s)\n",
    "        z1_c_outs represents the (batched) subject predictions produced by the adversary\n",
    "        z1_subject_labels represents the (batched) subject labels, representing the ground truth for the adversary\n",
    "\n",
    "        see Sectoin 3.1 of arxiv.org/pdf/2007.04871.pdf\n",
    "        \"\"\"\n",
    "        z1_c_outs = torch.nn.functional.normalize(z1_c_outs, p=2, dim=1) # see https://discuss.pytorch.org/t/how-to-normalize-embedding-vectors/1209\n",
    "\n",
    "        loss = 0.\n",
    "        curr_batch_size = z1s.size(self.BATCH_DIM)\n",
    "\n",
    "        # get contrastive loss of representations\n",
    "        # loss += self.contrastive_loss(z1s, z2s)\n",
    "        z1s = z1s.view(curr_batch_size, -1)\n",
    "        z2s = z2s.view(curr_batch_size, -1)\n",
    "\n",
    "        for i in range(curr_batch_size):\n",
    "            # see https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html\n",
    "\n",
    "            # compute loss contributions of t1-to-other pairings\n",
    "            numerator1 = torch.exp(self.cos_sim(z1s[i,:], z2s[i,:]) / self.tau)\n",
    "            denominator1 = 0.\n",
    "            for k in range(curr_batch_size):\n",
    "                denominator1 += torch.exp(self.cos_sim(z1s[i,:], z2s[k,:]) / self.tau) # compare t1 ith signal with all t2 signals\n",
    "                if k != i:                                                             # compare t1 ith signal to all other t1 signals, skipping the t1 ith signal\n",
    "                    denominator1 += torch.exp(self.cos_sim(z1s[i,:], z1s[k,:]) / self.tau)\n",
    "            loss += -1.*torch.log(self.log_noise + numerator1/denominator1)\n",
    "            # print(\"SAContrastiveAdversarialLoss.forward: \\t loss == \", loss)\n",
    "            \n",
    "            # SKIP loss contributions of t2-to-other pairings because they came from momentum-updated network\n",
    "            # numerator2 = torch.exp(self.cos_sim(z2s[i,:], z1s[i,:]) / self.tau)\n",
    "            # denominator2 = 0.\n",
    "            # for k in range(curr_batch_size):\n",
    "            #     denominator2 += torch.exp(self.cos_sim(z2s[i,:], z1s[k,:]) / self.tau) # compare augmented ith signal with all orig signals\n",
    "            #     if k != i:                                                             # compare augmented ith signal to all other augmented signals, skipping the augmented ith signal\n",
    "            #         denominator2 += torch.exp(self.cos_sim(z2s[i,:], z2s[k,:]) / self.tau)\n",
    "            # loss += -1.*torch.log(numerator2/denominator2)\n",
    "\n",
    "        loss = loss / (curr_batch_size*(2.*curr_batch_size - 1.)) # loss / (curr_batch_size*2.*(2.*curr_batch_size - 1.)) # take the average loss across the t1 signals \n",
    "\n",
    "        for i in range(curr_batch_size):\n",
    "            j = torch.argmax(z1_subject_labels[i,:])\n",
    "            loss += self.lam *(-1.)*torch.log(self.log_noise + (1. - z1_c_outs[i,j])) # see equation 3 of arxiv.org/pdf/2007.04871.pdf\n",
    "            # print(\"SAContrastiveAdversarialLoss.forward: \\t loss == \", loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def get_number_of_correct_reps(self, z1s, z2s, z1_c_outs, z1_subject_labels):\n",
    "        curr_batch_size = z1s.size(self.BATCH_DIM)\n",
    "\n",
    "        z1s = z1s.view(curr_batch_size, -1)\n",
    "        z2s = z2s.view(curr_batch_size, -1)\n",
    "\n",
    "        num_correct_reps = 0.\n",
    "        for i in range(curr_batch_size):\n",
    "            # see https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html\n",
    "\n",
    "            # compute accuracy contributions of orig-to-other pairings\n",
    "            sim_measure_of_interest = self.cos_sim(z1s[i,:], z2s[i,:])\n",
    "            representation_is_correct = True\n",
    "            for k in range(curr_batch_size):\n",
    "                other_sim_measure = self.cos_sim(z1s[i,:], z2s[k,:]) # compare t1 ith signal with all augmented signals\n",
    "                if other_sim_measure > sim_measure_of_interest:\n",
    "                    representation_is_correct = False\n",
    "                    break\n",
    "                if k != i:                                           # compare t1 ith signal to all other orig signals, skipping the t1 ith signal\n",
    "                    other_sim_measure = self.cos_sim(z1s[i,:], z1s[k,:])\n",
    "                    if other_sim_measure > sim_measure_of_interest:\n",
    "                        representation_is_correct = False\n",
    "                        break\n",
    "                \n",
    "            if torch.argmax(z1_subject_labels[i,:]) == torch.argmax(z1_c_outs[i,:]):\n",
    "                representation_is_correct = False\n",
    "\n",
    "            if representation_is_correct:\n",
    "                num_correct_reps += 1.\n",
    "            \n",
    "            # SKIP loss contributions of t2-to-other pairings because they were generated by momentum-updated network\n",
    "            # sim_measure_of_interest = self.cos_sim(z2s[i,:], z1s[i,:])\n",
    "            # representation_is_correct = True\n",
    "            # for k in range(curr_batch_size):\n",
    "            #     other_sim_measure += self.cos_sim(z2s[i,:], z1s[k,:]) # compare augmented ith signal with all orig signals\n",
    "            #     if other_sim_measure > sim_measure_of_interest:\n",
    "            #         representation_is_correct = False\n",
    "            #         break\n",
    "            #     if k != i:                                                             # compare augmented ith signal to all other augmented signals, skipping the augmented ith signal\n",
    "            #         other_sim_measure += self.cos_sim(z2s[i,:], z2s[k,:])\n",
    "            #         if other_sim_measure > sim_measure_of_interest:\n",
    "            #             representation_is_correct = False\n",
    "            #             break\n",
    "            # if representation_is_correct:\n",
    "            #     num_correct_reps += 1.\n",
    "\n",
    "        return num_correct_reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to calculate Adversarial Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAAdversarialLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    see Section 3.1 of arxiv.org/pdf/2007.04871.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SAAdversarialLoss, self).__init__()\n",
    "        self.BATCH_DIM = 0\n",
    "        self.log_noise = 1e-12 # 8 # see https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons\n",
    "        pass\n",
    "    \n",
    "    def forward(self, z1_c_outs, z1_subject_labels):\n",
    "        \"\"\"\n",
    "        z1_c_outs represents the (batched) subject predictions produced by the adversary\n",
    "        z1_subject_labels represents the (batched) subject labels, representing the ground truth for the adversary\n",
    "\n",
    "        see Sectoin 3.1 of arxiv.org/pdf/2007.04871.pdf\n",
    "        \"\"\"\n",
    "        # print(\"z1_c_outs.shape == \", z1_c_outs.shape)\n",
    "        # print(\"z1_c_outs == \", z1_c_outs)\n",
    "        z1_c_outs = torch.nn.functional.normalize(z1_c_outs, p=2, dim=1) # see https://discuss.pytorch.org/t/how-to-normalize-embedding-vectors/1209\n",
    "        # print(\"z1_c_outs == \", z1_c_outs)\n",
    "        # print(\"z1_c_outs.shape == \", z1_c_outs.shape)\n",
    "\n",
    "        loss = 0.\n",
    "        curr_batch_size = z1_c_outs.size(self.BATCH_DIM)\n",
    "\n",
    "        for i in range(curr_batch_size):\n",
    "            j = torch.argmax(z1_subject_labels[i,:])\n",
    "            loss += -1.*torch.log(self.log_noise + z1_c_outs[i,j]) # see equation 3 of arxiv.org/pdf/2007.04871.pdf\n",
    "            # print(\"SAAdversarialLoss.forward: \\t loss == \", loss, \" (i,j) == \", (i,j), \" z1_c_outs[i,j] == \", z1_c_outs[i,j])\n",
    "        # raise NotImplementedError()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to utilize adversarial loss in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SA_model(save_dir_for_model, model_file_name=\"final_SA_model.bin\", batch_size=256, shuffle=True, # hyper parameters for training loop\n",
    "                    max_epochs=100, learning_rate=5e-4, beta_vals=(0.9, 0.999), weight_decay=0.001, #num_workers=4, \n",
    "                    max_evals_after_saving=6, save_freq=20, former_state_dict_file=None, ct_dim=None, h_dim=None, \n",
    "                    channels=11, temporal_len=3000, dropout_rate=0.5, embed_dim=100, encoder_type=None, bw=5, # hyper parameters for SA Model\n",
    "                    randomized_augmentation=False, num_upstream_decode_features=32, temperature=0.05, NUM_AUGMENTATIONS=2, perturb_orig_signal=True, former_adversary_state_dict_file=None, adversarial_weighting_factor=1., momentum=0.999, # hyper parameters for SA Model\n",
    "                    cached_datasets_list_dir=None, total_points_val=2000, tpos_val=None, tneg_val=None, window_size=3, #hyper parameters for data loaders\n",
    "                    sfreq=1000, Nc=None, Np=None, Nb=None, max_Nb_iters=None, total_points_factor=None, \n",
    "                    windowed_data_name=\"_Windowed_Pretext_Preprocess.npy\", \n",
    "                    windowed_start_time_name=\"_Windowed_StartTime.npy\", data_folder_name=\"Mouse_Training_Data\", \n",
    "                    data_root_name=\"Windowed_Data\", file_names_list=\"training_names.txt\", train_portion=0.7, \n",
    "                    val_portion=0.2, test_portion=0.1, random_seed=0):\n",
    "    \n",
    "    # First, load the training, validation, and test sets\n",
    "    train_set, val_set, test_set = load_SSL_Dataset_Based_On_Subjects('SA', \n",
    "                                                    cached_datasets_list_dir=cached_datasets_list_dir, \n",
    "                                                    total_points_val=total_points_val, \n",
    "                                                    tpos_val=tpos_val, \n",
    "                                                    tneg_val=tneg_val, \n",
    "                                                    window_size=window_size, \n",
    "                                                    sfreq=sfreq, \n",
    "                                                    Nc=Nc, \n",
    "                                                    Np=Np, \n",
    "                                                    Nb=Nb, # this used to be 2 not 4, but 4 would work better\n",
    "                                                    max_Nb_iters=max_Nb_iters, \n",
    "                                                    total_points_factor=total_points_factor, \n",
    "                                                    bw=bw,                                              # items for SA data loading\n",
    "                                                    randomized_augmentation=randomized_augmentation,    # items for SA data loading\n",
    "                                                    num_channels=channels,                              # items for SA data loading\n",
    "                                                    temporal_len=temporal_len,                          # items for SA data loading\n",
    "                                                    NUM_AUGMENTATIONS=NUM_AUGMENTATIONS,                # items for SA data loading\n",
    "                                                    perturb_orig_signal=perturb_orig_signal,            # items for SA data loading\n",
    "                                                    windowed_data_name=windowed_data_name,\n",
    "                                                    windowed_start_time_name=windowed_start_time_name,\n",
    "                                                    data_folder_name=data_folder_name, \n",
    "                                                    data_root_name=data_root_name, \n",
    "                                                    file_names_list=file_names_list, \n",
    "                                                    train_portion=train_portion, \n",
    "                                                    val_portion=val_portion, \n",
    "                                                    test_portion=test_portion, \n",
    "                                                    random_seed=random_seed\n",
    "    )\n",
    "\n",
    "    # initialize data loaders for training\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=shuffle#, num_workers=num_workers # see https://www.programmersought.com/article/93393550792/\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, \n",
    "                                             batch_size=batch_size, \n",
    "                                             shuffle=shuffle#, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    # print(\"train_SA_model: len of the train_loader is \", len(train_loader))\n",
    "\n",
    "    # cuda setup if allowed\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Pytorch v0.4.0\n",
    "\n",
    "    # initialize models - see Figure 1 of arxiv.org/pdf/2007.04871.pdf\n",
    "    model = SACLNet(channels, temporal_len, dropout_rate=dropout_rate, embed_dim=embed_dim, num_upstream_decode_features=num_upstream_decode_features)\n",
    "    if former_state_dict_file is not None:\n",
    "        model.load_state_dict(torch.load(former_state_dict_file))\n",
    "    momentum_model = copy.deepcopy(model) # see https://discuss.pytorch.org/t/copying-weights-from-one-net-to-another/1492 and https://www.geeksforgeeks.org/copy-python-deep-copy-shallow-copy/\n",
    "    model = model.to(device)\n",
    "    momentum_model = momentum_model.to(device)\n",
    "\n",
    "    _, _, y0 = next(iter(train_loader))\n",
    "    assert len(y0.shape) == 2 \n",
    "    num_subjects = y0.shape[1]\n",
    "    adversary = SACLAdversary(embed_dim, num_subjects, dropout_rate=dropout_rate).to(device)\n",
    "    if former_adversary_state_dict_file is not None:\n",
    "        adversary.load_state_dict(torch.load(former_adversary_state_dict_file))\n",
    "\n",
    "    print(\"train_SA_model: START OF TRAINING\")\n",
    "    # initialize training state\n",
    "    min_val_inaccuracy = float(\"inf\")\n",
    "    min_state = None\n",
    "    num_evaluations_since_model_saved = 0\n",
    "    saved_model = None\n",
    "    saved_momentum_model = None\n",
    "    loss_fn = SAContrastiveAdversarialLoss(temperature, adversarial_weighting_factor=adversarial_weighting_factor)\n",
    "    # learning_rate = learning_rate\n",
    "    # beta_vals = beta_vals\n",
    "    optimizer = torch.optim.Adam(model.parameters(), betas=beta_vals, lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    saved_adversary = None\n",
    "    ##############################################################################################################################\n",
    "    adversarial_loss_fn = SAAdversarialLoss()\n",
    "    adversarial_optimizer = torch.optim.Adam(adversary.parameters(), betas=beta_vals, lr=learning_rate, weight_decay=weight_decay)\n",
    "    ##############################################################################################################################\n",
    "\n",
    "    # Iterate over epochs\n",
    "    avg_train_losses = []\n",
    "    avg_train_accs = []\n",
    "    avg_val_accs = []\n",
    "    avg_adversary_train_losses = []\n",
    "    avg_adversary_train_accs = []\n",
    "    avg_adversary_val_accs = []\n",
    "    for epoch in range(max_epochs):\n",
    "        # print(\"train_SA_model: epoch \", epoch, \" of \", max_epochs)\n",
    "\n",
    "        model.train()\n",
    "        momentum_model.train()\n",
    "        adversary.train()\n",
    "\n",
    "        running_train_loss = 0\n",
    "        num_correct_train_preds = 0\n",
    "        total_num_train_preds = 0\n",
    "        running_adversary_train_loss = 0\n",
    "        num_adversary_correct_train_preds = 0\n",
    "        total_num_adversary_train_preds = 0\n",
    "        \n",
    "        # iterate over training batches\n",
    "        # print(\"train_SA_model: \\tNow performing training updates\")\n",
    "        counter = 0\n",
    "        for x_t1, x_t2, y in train_loader:\n",
    "            # transfer to GPU\n",
    "            x_t1, x_t2, y = x_t1.to(device), x_t2.to(device), y.to(device)\n",
    "            # print(\"x_t1 == \", x_t1.shape)\n",
    "            # print(\"x_t2 == \", x_t2.shape)\n",
    "            # print(\"y == \", y.shape)\n",
    "\n",
    "            ##############################################################################################################################\n",
    "            # UPDATE ADVERSARY\n",
    "            for p in model.parameters():\n",
    "                p.requires_grad = False\n",
    "            for p in momentum_model.parameters():\n",
    "                p.requires_grad = False\n",
    "            for p in adversary.parameters():\n",
    "                p.requires_grad = True\n",
    "            \n",
    "            adversarial_optimizer.zero_grad()\n",
    "            \n",
    "            x_t1_initial_reps = model.embed_model(x_t1) # x_t1_initial_reps -> hidden vector\n",
    "            x_t1_initial_subject_preds = adversary(x_t1_initial_reps) # Input hidden vector in subject classifier\n",
    "            # x_t1_initial_subject_preds\n",
    "\n",
    "            adversarial_loss = adversarial_loss_fn(x_t1_initial_subject_preds, y) # SAAdversarialLoss(), y is a subject-class label\n",
    "            num_adversary_correct_train_preds += adversarial_loss_fn.get_number_of_correct_preds(x_t1_initial_subject_preds, y)\n",
    "            total_num_adversary_train_preds += len(x_t1_initial_subject_preds)\n",
    "\n",
    "            adversarial_loss.backward()\n",
    "            adversarial_optimizer.step()\n",
    "\n",
    "            running_adversary_train_loss += adversarial_loss.item()\n",
    "\n",
    "            del x_t1_initial_reps\n",
    "            del x_t1_initial_subject_preds\n",
    "            del adversarial_loss\n",
    "            torch.cuda.empty_cache()\n",
    "            ##############################################################################################################################\n",
    "        \n",
    "            # UPDATE MODEL - references Algorithm 1 of arxiv.org/pdf/1911.05722.pdf and Figure 1 of arxiv.org/pdf/2007.04871.pdf\n",
    "            for p in model.parameters():\n",
    "                p.requires_grad = True\n",
    "            for p in momentum_model.parameters():\n",
    "                p.requires_grad = False\n",
    "            for p in adversary.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # zero out any pre-existing gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # make prediction and compute resulting loss\n",
    "            # print(\"train_SA_model: \\t\\tembedding x1\")\n",
    "            x1_rep = model(x_t1)\n",
    "            # print(\"train_SA_model: \\t\\tembedding x2\")\n",
    "            x2_rep = momentum_model(x_t2)\n",
    "            # x2_rep.detatch()\n",
    "            # print(\"x1_rep == \", x1_rep.shape)\n",
    "            # print(\"x2_rep == \", x2_rep.shape)\n",
    "            x1_embeds = model.embed_model(x_t1)\n",
    "            x1_subject_preds = adversary(x1_embeds)\n",
    "            # print(\"train_SA_model: \\t\\tcomputing loss\")\n",
    "            loss = loss_fn(x1_rep, x2_rep, x1_subject_preds, y) # SAContrastiveAdversarialLoss.forward(z1s, z2s, z1_c_outs, z1_subject_labels)\n",
    "            # print(\"loss == \", loss)\n",
    "\n",
    "            # compute accuracy\n",
    "            # print(\"train_SA_model: \\t\\tcomputing accuracy\")\n",
    "            num_correct_train_preds += loss_fn.get_number_of_correct_reps(x1_rep, x2_rep, x1_subject_preds, y)\n",
    "            # print(\"train_SQ_model: \\t\\trecording accuracy\")\n",
    "            total_num_train_preds += len(x1_rep)\n",
    "\n",
    "            # update weights\n",
    "            # print(\"train_SA_model: \\t\\tperforming backprop\")\n",
    "            loss.backward()\n",
    "            # print(\"train_SA_model: \\t\\tupdating weights\")\n",
    "            optimizer.step()\n",
    "\n",
    "            # track loss\n",
    "            # print(\"train_SA_model: \\t\\trecording loss val\")\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # UPDATE MOMENTUM MODEL\n",
    "            # momentum_model.parameters = momentum*momentum_model.parameters + (1.-momentum)*model.parameters\n",
    "            momentum_model = momentum_model_parameter_update(momentum, momentum_model, model)\n",
    "\n",
    "            # free up cuda memory\n",
    "            # print(\"train_SA_model: \\t\\tclearing memory\")\n",
    "            del x_t1\n",
    "            del x_t2\n",
    "            del x1_rep\n",
    "            del x2_rep\n",
    "            del x1_embeds\n",
    "            del x1_subject_preds\n",
    "            del loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # if counter % 50 == 0:\n",
    "            #     print(\"train_SA_model: \\t\\tFinished batch \", counter)\n",
    "            counter += 1\n",
    "            # if counter == 5:\n",
    "            #     raise NotImplementedError()\n",
    "            # raise NotImplementedError()\n",
    "            # break # FOR DEBUGGING PURPOSES\n",
    "        \n",
    "        # iterate over validation batches\n",
    "        # print(\"train_SA_model: \\tNow performing validation\")\n",
    "        num_correct_val_preds = 0\n",
    "        total_num_val_preds = 0\n",
    "        num_correct_adversarial_val_preds = 0\n",
    "        total_num_adversarial_val_preds = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            momentum_model.eval()\n",
    "            adversary.eval()\n",
    "\n",
    "            for x_t1, x_t2, y in val_loader:\n",
    "                x_t1, x_t2, y = x_t1.to(device), x_t2.to(device), y.to(device)\n",
    "\n",
    "                # evaluate model and adversary\n",
    "                x1_rep = model(x_t1)\n",
    "                x2_rep = momentum_model(x_t2)\n",
    "                x1_embeds = model.embed_model(x_t1)\n",
    "                x1_subject_preds = adversary(x1_embeds)\n",
    "                # x1_subject_preds = adversary(x1_rep)\n",
    "\n",
    "                num_correct_val_preds += loss_fn.get_number_of_correct_reps(x1_rep, x2_rep, x1_subject_preds, y)\n",
    "                total_num_val_preds += len(x1_rep)\n",
    "\n",
    "                num_correct_adversarial_val_preds += adversarial_loss_fn.get_number_of_correct_preds(x1_subject_preds, y)\n",
    "                total_num_adversarial_val_preds += len(x1_subject_preds)\n",
    "\n",
    "                # free up cuda memory\n",
    "                del x_t1\n",
    "                del x_t2\n",
    "                del x1_rep\n",
    "                del x2_rep\n",
    "                del x1_embeds\n",
    "                del x1_subject_preds\n",
    "                torch.cuda.empty_cache()\n",
    "                # break # FOR DEBUGGING PURPOSES\n",
    "        \n",
    "        # record averages\n",
    "        avg_train_accs.append(num_correct_train_preds / total_num_train_preds)\n",
    "        avg_val_accs.append(num_correct_val_preds / total_num_val_preds)\n",
    "        avg_train_losses.append(running_train_loss / len(train_loader))\n",
    "        \n",
    "        avg_adversary_train_accs.append(num_adversary_correct_train_preds / total_num_adversary_train_preds)\n",
    "        avg_adversary_val_accs.append(num_correct_adversarial_val_preds / total_num_adversarial_val_preds)\n",
    "        avg_adversary_train_losses.append(running_adversary_train_loss / len(train_loader))\n",
    "        \n",
    "        # check stopping criterion / save model\n",
    "        incorrect_val_percentage = 1. - (num_correct_val_preds / total_num_val_preds)\n",
    "        if incorrect_val_percentage < min_val_inaccuracy:\n",
    "            num_evaluations_since_model_saved = 0\n",
    "            min_val_inaccuracy = incorrect_val_percentage\n",
    "            saved_model = model.state_dict()\n",
    "            saved_momentum_model = momentum_model.state_dict()\n",
    "            saved_adversary = adversary.state_dict()\n",
    "        else:\n",
    "            num_evaluations_since_model_saved += 1\n",
    "            if num_evaluations_since_model_saved >= max_evals_after_saving:\n",
    "                print(\"train_SA_model: EARLY STOPPING on epoch \", epoch)\n",
    "                break\n",
    "        \n",
    "        # save intermediate state_dicts just in case\n",
    "        if epoch % save_freq == 0:\n",
    "            temp_model_save_path = os.path.join(save_dir_for_model, \"temp_full_SA_model_epoch\"+str(epoch)+\".bin\")\n",
    "            torch.save(model.state_dict(), temp_model_save_path)\n",
    "            \n",
    "            temp_model_save_path = os.path.join(save_dir_for_model, \"temp_full_SA_momentum_model_epoch\"+str(epoch)+\".bin\")\n",
    "            torch.save(momentum_model.state_dict(), temp_model_save_path)\n",
    "            \n",
    "            temp_model_save_path = os.path.join(save_dir_for_model, \"temp_full_SA_adversary_epoch\"+str(epoch)+\".bin\")\n",
    "            torch.save(adversary.state_dict(), temp_model_save_path)\n",
    "\n",
    "            embedder_save_path = os.path.join(save_dir_for_model, \"temp_embedder_epoch\"+str(epoch)+\".bin\")\n",
    "            torch.save(model.embed_model.state_dict(), embedder_save_path)\n",
    "\n",
    "            plot_avgs(avg_train_losses, avg_train_accs, avg_val_accs, \"model_epoch\"+str(epoch), save_dir_for_model)\n",
    "            plot_avgs(avg_adversary_train_losses, avg_adversary_train_accs, avg_adversary_val_accs, \"adversary_epoch\"+str(epoch), save_dir_for_model)\n",
    "        # break # FOR DEBUGGING PURPOSES\n",
    "\n",
    "    print(\"train_SA_model: END OF TRAINING - now saving final model / other info\")\n",
    "\n",
    "    # save final model(s)\n",
    "    model.load_state_dict(saved_model)\n",
    "    model_save_path = os.path.join(save_dir_for_model, model_file_name)\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    momentum_model.load_state_dict(saved_momentum_model)\n",
    "    model_save_path = os.path.join(save_dir_for_model, \"momentum_model_\"+model_file_name)\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    adversary.load_state_dict(saved_adversary)\n",
    "    model_save_path = os.path.join(save_dir_for_model, \"adversary_\"+model_file_name)\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    embedder_save_path = os.path.join(save_dir_for_model, \"embedder_\"+model_file_name)\n",
    "    torch.save(model.embed_model.state_dict(), embedder_save_path)\n",
    "\n",
    "    meta_data_save_path = os.path.join(save_dir_for_model, \"meta_data_and_hyper_parameters.pkl\")\n",
    "    with open(meta_data_save_path, 'wb') as outfile:\n",
    "        pkl.dump({\n",
    "            \"avg_train_losses\": avg_train_losses, \n",
    "            \"avg_train_accs\": avg_train_accs, \n",
    "            \"avg_val_accs\": avg_val_accs, \n",
    "            \"avg_adversary_train_losses\": avg_adversary_train_losses, \n",
    "            \"avg_adversary_train_accs\": avg_adversary_train_accs, \n",
    "            \"avg_adversary_val_accs\": avg_adversary_val_accs, \n",
    "            \"save_dir_for_model\": save_dir_for_model, \n",
    "            \"model_file_name\": model_file_name, \n",
    "            \"batch_size\": batch_size, \n",
    "            \"shuffle\": shuffle, #\"num_workers\": num_workers, \n",
    "            \"max_epochs\": max_epochs, \n",
    "            \"learning_rate\": learning_rate, \n",
    "            \"beta_vals\": beta_vals, \n",
    "            \"weight_decay\": weight_decay, \n",
    "            \"max_evals_after_saving\": max_evals_after_saving, \n",
    "            \"save_freq\": save_freq, \n",
    "            \"former_state_dict_file\": former_state_dict_file, \n",
    "            \"ct_dim\": ct_dim, \n",
    "            \"h_dim\": h_dim, \n",
    "            \"channels\": channels, \n",
    "            \"temporal_len\": temporal_len, \n",
    "            \"dropout_rate\": dropout_rate, \n",
    "            \"embed_dim\": embed_dim,\n",
    "            \"encoder_type\": encoder_type, \n",
    "            \"bw\": bw, \n",
    "            \"randomized_augmentation\": randomized_augmentation, \n",
    "            \"num_upstream_decode_features\": num_upstream_decode_features, \n",
    "            \"temperature\": temperature, \n",
    "            \"NUM_AUGMENTATIONS\": NUM_AUGMENTATIONS, \n",
    "            \"perturb_orig_signal\": perturb_orig_signal, \n",
    "            \"former_adversary_state_dict_file\": former_adversary_state_dict_file, \n",
    "            \"adversarial_weighting_factor\": adversarial_weighting_factor, \n",
    "            \"momentum\": momentum, \n",
    "            \"cached_datasets_list_dir\": cached_datasets_list_dir, \n",
    "            \"total_points_val\": total_points_val, \n",
    "            \"tpos_val\": tpos_val, \n",
    "            \"tneg_val\": tneg_val, \n",
    "            \"window_size\": window_size,\n",
    "            \"sfreq\": sfreq, \n",
    "            \"Nc\": Nc, \n",
    "            \"Np\": Np, \n",
    "            \"Nb\": Nb,\n",
    "            \"max_Nb_iters\": max_Nb_iters, \n",
    "            \"total_points_factor\": total_points_factor, \n",
    "            \"windowed_data_name\": windowed_data_name,\n",
    "            \"windowed_start_time_name\": windowed_start_time_name,\n",
    "            \"data_folder_name\": data_folder_name, \n",
    "            \"data_root_name\": data_root_name, \n",
    "            \"file_names_list\": file_names_list, \n",
    "            \"train_portion\": train_portion, \n",
    "            \"val_portion\": val_portion, \n",
    "            \"test_portion\": test_portion, \n",
    "            \"random_seed\": random_seed, \n",
    "        }, outfile)\n",
    "\n",
    "    plot_avgs(avg_train_losses, avg_train_accs, avg_val_accs, \"Final_Model\", save_dir_for_model)\n",
    "    plot_avgs(avg_adversary_train_losses, avg_adversary_train_accs, avg_adversary_val_accs, \"Final_Adversary\", save_dir_for_model)\n",
    "    \n",
    "    print(\"train_SA_model: DONE!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
